{"title":"Geographically Weighted Statistics","markdown":{"yaml":{"title":"Geographically Weighted Statistics","execute":{"warning":false,"message":false}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\ninstalled <- installed.packages()[,1]\nrequired <- c(\"GWmodel\", \"proxy\", \"sf\", \"sp\", \"tidyverse\", \"tmap\", \"stars\", \"raster\", \"remotes\", \"cowplot\",\n              \"cartogram\", \"lpSolve\", \"colorspace\", \"cols4all\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE, repos = \"https://cloud.r-project.org\")\n```\n\n\nIn the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by 'pooling' the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in `GWmodel` -- [an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models](https://www.jstatsoft.org/article/view/v063i17){target=\"_blank\"}. These are geographically weighted statistics.\n\n## Geographical Weighted Statistics\n\nThe idea behind geographically weighted statistics is simple. Instead of calculating the (global) mean average, for example, for the all the data and the whole map, a series of (local) averages are calculated for various sub-spaces within the map.\n\nImagine a point location, $i$, at position $(u_i, v_i)$ on the map. To calculate the local statistic, first find either the $k$ nearest neighbours to $i$ or all of those within a fixed distance, $d$, from it. Second, to add the geographical weighting in the name of the statistics, apply a weighting scheme whereby the neighbours nearest to $i$ have most weight in the subsequent calculations and the weights decrease with increasing distance from $i$, becoming zero at the $k$th nearest neighbour or at the distance threshold, $d$. Third, calculate, for the point and its neighbours, the weighted mean value (or some other summary statistic) of a variable, using the inverse distance weighting in the calculation. Fourth, repeat the process for other points on the map. This means that if there are $n$ points of calculation then there will be $n$ geographically weighted mean values calculated across the map. These can be then be compared to look for spatial variation.\n\nLet's see this in action, beginning by ensuring the necessary packages are installed and required. As with previous sessions, you might start by opening the R Project that you created for these classes.\n\n```{r}\ninstalled <- installed.packages()[,1]\nrequired <- c(\"colorspace\", \"cols4all\", \"GWmodel\", \"proxy\", \"sf\", \"sp\",\n              \"tidyverse\", \"tmap\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE,\n                                     repos = \"https://cloud.r-project.org\")\n\nrequire(cols4all)\nrequire(GWmodel)\nrequire(sf)\nrequire(tidyverse)\nrequire(tmap)\n```\n\nWe will use the same data as in the previous session but confine the analysis to the Western Cape of South Africa. This is largely to reduce run times but there is another reason that I shall return to presently.\n\n```{r}\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nwards |>\n  filter(ProvinceNa == \"Western Cape\") ->\n  wcape_wards\n```\n\nThe calculated points for the geographically weighted statistics will be the ward centroids. A slight complication here is that `GWmodel` presently is built around the elder `sp` not `sf` formats for handling spatial data in R so the wards need to be converted from the one format to the other.\n\n```{r}\nwcape_wards_sp <- as_Spatial(wcape_wards)\n```\n\nWe can see that `wcape_wards` is of class `sf`,\n\n```{r}\nclass(wcape_wards)\n```\n\nwhereas `wcape_wards_sp` is now of class `SpatialPolygonsDataFrame`, which is what we want.\n\n```{r}\nclass(wcape_wards_sp)\n```\n\nHaving made the conversion we are ready to calculate some geographically weighted statistics, doing so for the `High_income` variable -- the percentage of the population with income R614401 or greater in 2011 -- in the example below.\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>Most warning messages are intended to be helpful and the one generated from the code above is not an exception. It is related to a [change in how coordinate reference systems (CRS) are represented](https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html){target=\"_blank\"} but does not affect the results of the analyses here.</font>\n\nThe results are contained in the spatial data frame (`$SDF`),\n\n```{r}\nhead(gwstats$SDF)\n```\n\nAs well as the local means, the local standard deviations, variances, skews and coefficients of variation are included (the coefficient of variation is the ratio of the standard deviation to the mean). The local medians, interquartile ranges and quantile imbalances could also be added by including the argument `quantile = TRUE` in `gwss()` -- see `?gwss`.\n\nThe results are dependent on the data (of course) but also\n\n-   the kernel (i.e. the shape of the weighting -- the distance decay -- around each point), and\n-   the bandwidth (i.e. the maximum number of neighbours to include, $k$ or the distance threshold, $d$).\n\nThe kernel matters...\n\n![](kernel.jpg)\n\n<font size = 2>Source: [GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models](https://www.jstatsoft.org/article/view/v063i17){target=\"_blank}</font>\n\n... but it (usually) matters much less than the bandwidth, which controls the amount of spatial smoothing: the larger it is, the more neighbours are being averaged over. The trade-off is between bias and precision. A smaller bandwidth is less likely to average-out geographical detail in the data and should create a geographically weighted average, for example, that is representative of the location at the centre of the kernel but it is also dependent on a small number of observations, some or more of which could be in error or unsual outliers for the vicinity.\n\nThe following maps compare a bandwidth of $bw = 10$ nearest neighbours to $bw = 100$. If you zoom into the areas around the north of Cape Town you will see some of the differences. Note that the argument `adaptive = TRUE` sets the bandwidth to be in terms of nearest neighbours, else it would indicate a fixed distance of 10 metres. The advantage of using nearest neighbours is it allows for varying population densities. Otherwise, using a fixed distance, more rural areas will tend to have fewer neighbours than urban ones because those rural areas are larger and more spaced apart.\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw100 <- gwstats$SDF$High_income_LM\n\nwards2 <- pivot_longer(wcape_wards, cols = starts_with(\"bw\"), names_to = \"bw\",\n             values_to = \"GWmean\")\n\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(wards2, names = \"wards\") +\n  tm_fill(\"GWmean\", palette = \"Reds\", title = \"%\",\n          alpha = 0.7,\n          id = \"District_1\",\n          popup.vars = c(\"GW mean:\" = \"GWmean\",\n                         \"Ward ID:\" = \"WardID\"),\n          popup.format = list(digits = 1)) +\n  tm_borders() +\n  tm_facets(by = \"bw\") +\n  tm_legend(title =\n        \"Geographically weighted % population with income R614401 or greater\")\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>A nice feature of the facetting (`tm_facets`) in `tmap` is it has created two dynamically linked maps.</font>\n\n### A slight tangent\n\nYou may note the use of the `pivot_longer` function from the `tidyverse` packages in the code above. To see what this does, take a look at,\n\n```{r}\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  arrange(WardID) |>\n  head(n = 3)\n```\n\nNow compare it with,\n\n```{r}\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  pivot_longer(cols = starts_with(\"bw\"), names_to = \"bw\",\n          values_to = \"GWmean\") |>\n  arrange(WardID) |>\n  head(n = 6)\n```\n\nWhat you can see is that the two columns, `bw10` and `bw100` from the first table have been stacked into rows in the second. Doing this allows us to create the two linked plots by faceting on the bandwidth variable, `bw`, using `tm_facets(by = \"bw\")`. The reverse operation to `pivot_longer()` is `pivot_wider()` -- see `?pivot_wider`. \n\n### 'Pre-calculating' the distance matrix\n\nIn the calculations above, the distances between the ward centroids that are used in the geographical weighting are calculated twice. First in `gwss(wcape_wards_sp, vars = \"High_income\", bw = 10, ...)` and then again in `gwss(wcape_wards_sp, vars = \"High_income\", bw = 100, ...)`. Since those distances don't actually change (the centroids are fixed so therefore are the distances between them) so we might have saved a little computational time by calculating the distance matrix in advance and then using it in the geographically weighted statistics. Curiously, though, it actually takes longer. I assume this is because the data set isn't large enough to justify the extra step of saving the distances in a matrix and then passing that matrix to the `gwss()` function.\n\n```{r}\n# Time to do the calculations without pre-calculating the distance matrix\nsystem.time({\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n# Time to do the calculations with the pre-calculated distance matrix\nsystem.time({\n  coords <- st_coordinates(st_centroid(wcape_wards))\n  dmatrix <- gw.dist(coords, longlat = T)\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n```\n\n## Selecting the bandwidth\n\nAs observed in the maps above, the geographically weighted statistics are a function of the geographical weighting that largely is controlled by the bandwidth. This raises the question of which is the correct bandwidth to use? Unfortunately, the most honest answer is that there is no correct answer, although an automatic bandwidth selection might be tried by calibrating the statistics around the local means.\n\nThe following uses a cross-validation approach,\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n# The selected number of nearest neighbours:\nbw\n```\n\nwhereas the following uses an AIC corrected approach.\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\")\nbw\n```\n\nBoth use a [golden-section search method](https://en.wikipedia.org/wiki/Golden-section_search){target=\"_blank\"} and, if you look at the output, both iterate to a very similar solution in a very similar set of steps. Bandwidths of 17 and 20 have been suggested but, in practice, there is probably little between them so we may as well use the larger.\n\nThat automatic bandwidth selection applies only for the Western Cape wards, however. Recall earlier that the data were filtered (`wards |> filter(ProvinceNa == \"Western Cape\") -> wcape_wards`) with the partial explanation for doing so being to reduce run times. Another explanation is that there is no reason to assume that the spatial autocorrelation that is quantified by the bandwidth selection will be the same everywhere across the map. In fact, it varies from province to province:\n\n```{r}\n#| results: false\nbandwidths <- sapply(unique(wards$ProvinceNa), \\(x) {\n  wards |>\n    filter(ProvinceNa == x) |>\n    as_Spatial() %>%\n    bw.gwr(High_income ~ 1, data = .,\n          adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n          approach = \"AIC\") %>%\n    paste0(\"Bandwidth = \", .)\n})\n```\n\n```{r}\nbandwidths\n```\n\nThis suggests that fitting geographically weighted statistics to too large a study region is not desirable because there is little reason to presume that the same bandwidth should apply throughout it. \n\n## Changing the interpolation (calculation) points\n\nSo far we have been using the ward centroids as the points for which the geographically weighted statistics are calculated. There is no requirement to do so as they could be interpolated at any point within this study region. To demonstrate this, let's reselect the wards in the Western Cape and convert them into a raster grid using the [stars](https://r-spatial.github.io/stars/){target=\"_blank\"} package. Stars is an abbreviation of spatial-temporal arrays and can be used for handling raster (gridded) data, as in the following example, which is taken from [this introduction to the package](https://r-spatial.github.io/stars/articles/stars1.html){target=\"_blank\"}.\n\n```{r}\nif(!(\"stars\" %in% installed)) install.packages(\"stars\", dependencies = TRUE)\nrequire(stars)\nsat_image <- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nsat_image <- read_stars(sat_image)\nplot(sat_image, axes = TRUE)\n```\n\nWe will use the `st_rasterize` function in `stars` to convert the geography of South Africa's Western Cape into a regular grid.\n\n```{r}\nwcape_wards |>\n  st_rasterize(nx = 100, ny = 100) |>\n  st_as_sf() ->\n  gridded\n\npar(mai=c(0,0,0,0))\ngridded |>\n  st_geometry() |>\n  plot()\n```\n\nWe can now calculate the geographically weighted mean for each raster cell, with a bandwidth of 20 nearest neighbours.\n\n```{r}\ngridded_sp <- as_Spatial(gridded)\n\ngwstats <- gwss(wcape_wards_sp, gridded_sp, vars = \"High_income\",\n                bw = 20, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\ngridded$GWmean <- gwstats$SDF$High_income_LM\n\nggplot(gridded, aes(fill = GWmean)) +\n  geom_sf(col = \"light grey\", size = 0) + \n                      # size is the width of the raster cell border\n  scale_fill_continuous_c4a_seq(palette = \"scico.lajolla\") +\n  theme_minimal()\n```\n\n### Using geography to interpolate missing values\n\nThis ability to interpolate at any point within the study region provides a means to deal with missing values in the data. Contained in the `wcape_wards` data is a variable giving the average age of the population in each ward in 2011 but it contains `r length(wcape_wards$age[is.na(wcape_wards$age)])` missing values:\n\n```{r}\nsummary(wcape_wards$age)\n```\n\nThe same observations also record NAs for the `No_schooling` variable.\n\n```{r}\nwhich(is.na(wcape_wards$age))\nwhich(is.na(wcape_wards$No_schooling))\n```\n\nMissing value are a problem when fitting a regression model, for example. Usually they are simply omitted, as in the following case, where the output reports \"(15 observations deleted due to missingness)\".\n\n```{r}\nols1 <- lm(No_schooling ~ age, data = wcape_wards)\nsummary(ols1)\n```\n\n</br>\nAn alternative approach is to replace the missing values with a 'safe' alternative such as the mean age for the values that are not missing and the same for the percentages of the populations without schooling. However, we could also replace each missing value with a locally interpolated mean which fits with the geographical context.\n\nHere are the wards with missing age and no schooling values. These are the points that need to be interpolated.\n\n```{r}\nwcape_wards |>\n  filter(is.na(age)) |>\n  as_Spatial() ->\n  missing\n```\n\nThese are the wards with the values that serve as the data points.\n\n```{r}\nwcape_wards |>\n  filter(!is.na(age)) |>\n  as_Spatial() ->\n  present\n```\n\nFortunately, the missing values seem to be fairly randomly distributed across the study region. It would be a problem if they were all geographically clustered together because interpolating their values from their neighbours would not be successful if their neighbours' values were also missing!\n\n```{r}\npar(mai=c(0, 0, 0, 0))\nplot(present, border = \"light grey\")\nplot(missing, col = \"red\", add = T)\n```\n\nWe can, then, interpolate the missing values from the present ones and match them into the data using base R's `match()` function, matching on their `WardID`. Note that I have done this twice, once for the `age` variable and once for `No_schooling`. This is to allow for the possibility of them having differently sized bandwidths from each other (which they are when using `approach = \"AIC\"`, although not, as it happens, with the default `approach = \"CV\"`).\n\n```{r}\n#| results: false\n\nbw <- bw.gwr(age ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n             approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"age\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch <- match(missing$WardID, wcape_wards$WardID)\nwcape_wards$age[mch] <- gwstats$SDF$age_LM\n\nbw <- bw.gwr(No_schooling ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"No_schooling\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = T)\n\nwcape_wards$No_schooling[mch] <- gwstats$SDF$No_schooling_LM\n```\n\nIt is useful to keep a note of which values are interpolated, so...\n\n```{r}\nwcape_wards$interpolated <- FALSE\nwcape_wards$interpolated[mch] <- TRUE\n```\n\nThere should be `r length(wcape_wards$interpolated[wcape_wards$interpolated])` of them.\n\n```{r}\ntable(wcape_wards$interpolated)\n```\n\nNow returning to our regression model, there are, of course, no longer any missing values and, reassuringly, no evidence that the interpolated values are significantly different from the rest in either their mean `No_schooling` value or in their effect of `age` upon `No_schooling`. \n\n```{r}\nols2 <- update(ols1, . ~ . + interpolated*age)\nsummary(ols2)\n```\n\n## The geographically weighted mean as a low pass filer\n\nThe geographically weighted mean acts as a [smoothing (low pass) filter](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-analyst-toolbox/filter.htm){target=\"_blank\"}. We can see this if we apply it to a part of the satellite image from earlier. (If you try it on the whole image, you will be waiting a long time for the bandwidth to be calculated). \n\nFirst we shall extract band 1 from the image, create a blank raster using the [raster package](https://cran.r-project.org/web/packages/raster/index.html){target=\"_blank\"} and assign it the same values as from the image.\n\n```{r}\nsat_image |>\n  slice(band, 1) |>\n  pull() -> vals\n\nif(!(\"raster\" %in% installed)) install.packages(\"raster\", dependencies = TRUE)\n\ndim(sat_image)\nst_bbox(sat_image)\nr <- raster::raster(nrows = 352, ncol = 349,\n                      xmn = 288776.3, xmx = 298722.8,\n                      ymn = 9110728.8, ymx = 9120760.8)\nraster::crs(r) <- \"EPSG:31985\"\n\n# The image is 349 (row) by 352 (col) whereas the raster is\n# 352 (col) by 349 (row) which is why the values are transposed, t(vals)\nr <- raster::setValues(r, t(vals))\n```\n\nNext we will crop out the top-left hand corner of the image, convert the coordinates and the attributes of those raster cells into a [SpatialPointsDataFrame](https://www.rdocumentation.org/packages/sp/versions/1.6-0/topics/SpatialPoints){target=\"_blank\"} for use with the `GWmodel` functions and calculate the geographically weighted statistics:\n\n```{r}\nr1 <- raster::crop(r, raster::extent(r, 1, 50, 1, 50))\npts <- as(r1, \"SpatialPointsDataFrame\")\n\nbw <- bw.gwr(layer ~ 1, data = pts,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F,\n             approach = \"AIC\")\ngwstats <- gwss(pts, vars = \"layer\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = F)\n```\n\nWe can now compare the original image with the smoothed image.\n\n```{r}\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LM)\nr <- st_as_stars(raster::addLayer(r1, r2))\n\nggplot() + \n  geom_stars(data = r) +\n  facet_wrap(~ band) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n```\n\nHowever, it isn't only the geographically weighted mean that is calculated. You could use the geographically weighted standard deviation, for example, as a high pass filter -- a form of edge detection.\n\n```{r}\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LSD)\nr <- st_as_stars(r2)\n\nggplot() + \n  geom_stars(data = r) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>You might spot that I have not loaded (required) the `raster` package in the code above but have accessed its functions via the `::` notation; for example `raster::setValues()`. That is because if I do load the package, its `select` function will then mask a different function but with the same name in tidyverse's `dplyr`. As I am only using the `raster` package very briefly, I didn't think it was worth the potential confusion.</font>\n\n## Geographically weighted correlation\n\nAccording to the regression model earlier, there is a negative correlation between the age of the population and the percentage without schooling. The correlation across the Western Cape is,\n\n```{r}\ncor(wcape_wards$No_schooling, wcape_wards$age)\n```\n\nThat is, however, the global correlation for what appears (below) to be a [heteroscedastic relationship](https://www.investopedia.com/terms/h/heteroskedasticity.asp){target=\"_blank\"}. \n\n```{r}\nggplot(data = wcape_wards, aes(x = age, y = No_schooling)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship so let's consider that by calculating and mapping the geographically weighted correlations between the two variables.\n\n```{r}\n#| eval: false\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\n```{r}\n#| include: false\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\nThe local correlations range `r round(min(wcape_wards$Corr_No_schooling.age), 3)` to `r round(max(wcape_wards$Corr_No_schooling.age), 3)`, with an interquartile range from `r round(quantile(wcape_wards$Corr_No_schooling.age, probs = 0.25), 3)` to `r round(quantile(wcape_wards$Corr_No_schooling.age, probs = 0.75), 3)`:\n\n```{r}\nsummary(wcape_wards$Corr_No_schooling.age)\n```\n\nIf we add a little 'cartographic know-how' from an earlier session, we can identify that the correlation is stronger in and around Parow than it is in and around Blue Downs, for example.\n\n```{r}\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\",\n                                                 dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |>\n  filter(place == \"city\" | place == \"town\") |>\n  st_join(wcape_wards) |>\n    # A spatial spatial join,\n    # giving the point places the attributes of the wards they fall in\n  mutate(population = as.integer(population)) |>\n  filter(!is.na(ProvinceNa) & population > 60000) ->\n  places\n\nlast_plot() +\n  geom_sf_label_repel(data = places, aes(label = name), alpha = 0.7,\n                      force = 5, size = 2, max.overlaps = 20) +\n  xlab(element_blank()) +\n  ylab(element_blank())\n```\n\nIn general, the correlation appears to be strongest in cities:\n\n```{r}\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  st_join(wcape_wards) %>%\n  st_drop_geometry %>%\n  group_by(place) %>%\n  summarise(meancorr = mean(Corr_No_schooling.age, na.rm = TRUE)) %>%\n  arrange(meancorr)\n```\n\n## Statistical inference and significance\n\nWe can use a randomisation procedure to determine whether any of the local summary statistics may be said to be significantly different from those obtained by chance. The randomisation procedure is found in the function, `gwss.montecarlo()` with a default number of `nsim = 99` simulations. This isn't very many but they are time-consuming to calculate and will be sufficient to demonstrate the process.\n\nThe following code chunk returns to the local mean percentages of high earners. It goes through the complete process of determining a bandwidth using the `bw.gwr()` function, then calculating the local and geographically weighted statistics using `gwss()`, determining the p-values under randomisation, using `gwsss.montecarlo`, then mapping the results. Very few of the results would be adjudged significant but a few are.\n\n![](hazard.gif){width=75}\n\n<font size = 3>Calculating the p-values under randomisation takes some time so please be patient.</font>\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = \"High_income\", bw = bw,\n                            kernel = \"bisquare\",\n                            adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$High_income_LM <- gwstats$SDF$High_income_LM\nwcape_wards$High_income_LM[p.values$High_income_LM > 0.025 &\n                     p.values$High_income_LM < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = High_income_LM)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", na.value = \"light grey\",\n                       direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Local mean percentage of higher earrners\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\nA second example considers the local correlations between the `No_schooling` and `age` variables. Again, most of the correlations are insignificant but with a few exceptions.\n\n```{r}\n#| results: false\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n```\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"),\n                bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nwcape_wards$Corr_No_schooling.age[p.values$Corr_No_schooling.age > 0.025 &\n                            p.values$Corr_No_schooling.age < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\",\n                       na.value = \"light grey\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>We have the  problem of repeating testing that we also had when looking for spatial hotspots in the session about spatial autocorrelation. The function `p.adjust()` could be used but we may wonder, as previously, where the methods are too conservative.</font>\n\n## Improving the leigibility of the map\n\n### Using a map insert\n\nBefore for completing this session there is value in addressing the problem that some parts of the map are so small that their contents are almost illegible (too small to be seen). The traditional and probably the most effective way to address this is to add one or more inserts to the map to magnify the small areas. One way to do this is by using the [cowplot](https://cran.r-project.org/web/packages/cowplot/index.html){target=\"_blank\"} package with `ggplot2`. The following example is based on [this tutorial](https://geocompr.github.io/post/2019/ggplot2-inset-maps/){target=\"_blank\"} and maps the percentages of the populations who are relatively high earners in each ward.\n\nFirst, we need to install and require the `cowplot` package.\n\n```{r}\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\", dependencies = TRUE)\nrequire(cowplot)\n```\n\nSecond, the part of the `wards` map to be included in the map insert is extracted, using `st_crop`.\n\n```{r}\nwards_extract <- st_crop(wcape_wards, xmin = 18.2, xmax = 19, ymin = -34.3,\n                         ymax = -33.5)\nplot(st_geometry(wards_extract))\n```\n\nThe bounding box for the extracted area is also obtained as it will be used as a feature in the final map (it will be drawn as a rectangle showing the geographical extent of the insert in the original map).\n\n```{r}\nbbox <- st_as_sfc(st_bbox(wards_extract))\n```\n\nNext, the main part of the map is created...\n\n```{r}\nmain_map <- ggplot(wcape_wards, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\nplot(main_map)\n```\n\n...as is the map insert:\n\n```{r}\ninsert <- ggplot(wards_extract, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_void() +\n  theme(legend.position = \"none\")\nplot(insert)\n```\n\nFinally, the maps are brought together using `cowplot`'s `ggdraw()` and `draw_plot()` functions. We know that some of the values it shows are not necessarily significant under randomisation but we will include them here to just to get \n\n```{r}\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(insert, x = 0, y = 0.25, scale = 0.22) +\n  draw_label(\"Cape Town\\nand\\nsurrounding areas\", x = 0.62, y = 0.78, size = 8)\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>The positioning of the map insert (`x = ...` & `y = ...`), the scaling of it, and the size and position of the label were all found by trial and error, using various values until I was happy with the results.</font>\n\n### Using a 'balanced carogtram'\n\nAnother approach is to use what has been described as a [visually balanced cartogram](https://journals.sagepub.com/doi/full/10.1177/0308518X17708439){target=\"_blank\"}. A [cartogram](https://gisgeography.com/cartogram-maps/){target=\"_blank\"} -- of which there are lots of interesting examples [on this website](https://www.viewsoftheworld.net/){target=\"_blank\"} -- usually works by distorting a map in a way that rescales the areas in proportion not to their physical size but by some other measured attribute such as their population count. Much of the motivation for this lies in political studies and mapping the results of an election wherein a traditional map can give a very distorted view of the outcome because of how much population density varies across a country. In the example below, rescaling the areas by population correctly shows that the 2020 US Presidential was not a Republican landslide, despite Trump's baseless accusations!\n\n![](USelection2020Cartogram.png)\n<font size = 2>Source: [www.viewsoftheworld.net](https://www.viewsoftheworld.net/?p=5777){target=\"_blank}</font>\n\nThe problem with cartograms is the distortion. Essentially, they are trading one visual problem (invisibility of small areas) for another (the amount of geographical distortion). The idea of a balanced cartogram comes from an awareness that sometimes it is sufficient just to make the small areas bigger and/or the big areas smaller, without causing too much geographical distortion. One way to achieve this is to scale the places by the square root of the original areas, as in the following example.\n\n```{r}\nif(!(\"cartogram\" %in% installed)) install.packages(\"cartogram\",\n                                                   dependencies = TRUE)\nrequire(cartogram)\n\n# Convert from longitude/latitude to a grid projection\nwcape_wards %>%\n  st_transform(22234) %>%\n  mutate(area = as.numeric(st_area(.)),\n         w = as.numeric(sqrt(area))) ->\n  wards_prj\n\n# Create the cartogram -- here a contiguous cartogram is used, see ?cartogram\nwards_carto <- cartogram_cont(wards_prj, weight = \"w\", maxSizeError = 1.4,\n                              prepare = \"none\")\n\nggplot(wards_carto, aes(fill = High_income)) +\n  geom_sf(col = \"white\", size = 0.1) +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title =\n      \"Cartogram of the percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\n```\n\nThe results are ok, drawing attention to the wards with higher percentages of higher earners but clearly there is geographical distortion in the map, too.\n\n### Using a hexogram\n\nThe final approach to be considered here is what has been described as [hexograms](https://www.tandfonline.com/doi/full/10.1080/17445647.2018.1478753){target=\"_blank\"} -- a combination of tile maps and cartograms that make small areas bigger on the map but try and limit the geographic distortion. The [original method](https://rpubs.com/profrichharris/hexograms){target=\"_blank\"} also preserved typology (i.e. contiguous neighbours remained so in the final map) but is dated and **very** slow to operationalise as it was really just a proof of concept. A faster method but one that cannot guarantee to preserve typology is presented below.\n\nFirst, we need some functions to create the hexogram.\n\n```{r}\nif(!(\"cartogram\" %in% installed.packages()[,1])) install.packages(\"cartogram\")\nif(!(\"sf\" %in% installed.packages()[,1])) install.packages(\"sf\")\nif(!(\"lpSolve\" %in% installed.packages()[,1])) install.packages(\"lpSolve\") \n\nrequire(sf)\n\ncreate_carto <- \\(x, w = NULL, k = 2, itermax = 25, maxSizeError = 1.4) {\n  if(class(x)[1] != \"sf\") stop(\"Object x is not of class sf\")\n  if(is.null(w)) x$w <- as.numeric(st_area(x))^(1/k)\n  cartogram::cartogram_cont(x, \"w\", itermax = itermax,\n                            maxSizeError = maxSizeError, prepare = \"none\")\n}\n\n\ncreate_grid <- \\(x, m = 6) {\n  bbox <- st_bbox(x)\n  ydiff <- bbox[4] - bbox[2]\n  xdiff <- bbox[3] - bbox[1]\n  \n  n <- m * nrow(x)\n  ny <- sqrt(n / (xdiff/ydiff))\n  nx <- n / ny\n  nx <- ceiling(nx)\n  ny <- ceiling(ny)\n  \n  grd <- st_sf(st_make_grid(x, n = c(nx, ny), square = FALSE))\n}\n\n\ngrid_data <- \\(x, grd, k = 2) {\n  x_pts <- st_centroid(st_geometry(x), of_largest_polygon = TRUE)\n  grd_pts <- st_centroid(grd)\n  \n  cost.mat <- st_distance(x_pts, grd_pts)^k\n  row.rhs <- rep(1, length(x_pts))\n  col.rhs <- rep(1, nrow(grd_pts))\n  row.signs <- rep(\"=\", length(x_pts))\n  col.signs <- rep(\"<=\", nrow(grd_pts))\n\n  optimisation <- lpSolve::lp.transport(cost.mat, \"min\", row.signs, row.rhs,\n                                        col.signs, col.rhs)$solution\n  \n  mch <- sapply(1:nrow(optimisation), \\(i) which(optimisation[i, ] == 1))\n  grd <- st_sf(grd[mch,])\n  cbind(grd, st_drop_geometry(x))\n}\n\n\ncreate_layers <- \\(x, grd) {\n  \n  area <- sapply(1: nrow(x), \\(i) {\n    y <- st_intersects(x[i, ], grd)[[1]]\n    if(i %in% y) {\n      if(length(y) == 1) return(st_area(x[i, ]))\n      if(length(y) > 1) {\n        area <- st_area(x[i, ])\n        overlaps <- y[-(y == i)]\n        area <- area - sum(st_area(st_intersection(st_geometry(x[i, ]),\n                                                st_geometry(grd[overlaps, ]))))\n        return(area) \n      }\n    } else {\n      return(0)\n    }\n  })\n\n  i <- which(area > 2*as.numeric(st_area(grd))[1])\n  list(x[i, ], grd[-i, ])\n}\n```\n\nNext, we run through the stages of its creation, which are to start with a balanced cartogram (`create_carto()`), then overlay that with a grid of hexagons (`create_grid()`), next assign places in the original map to a hexagon (`grid_data()`), then plot the results as two layers (`create_layers()`) to give a mash-up of the cartogram and the hexagons.\n\n```{r}\nwards_carto <- create_carto(wards_prj)\ngrd <- create_grid(wards_carto)\nwards_grid <- grid_data(wards_carto, grd)\nwards_layers <- create_layers(wards_carto, wards_grid)\n\nggplot() +\n  geom_sf(data = wards_layers[[1]], aes(fill = High_income),\n          col = \"white\", size = 0.4) +\n  geom_sf(data = wards_layers[[2]], aes(fill = High_income),\n          col = \"dark grey\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"Hexogram % of the population who are higher earners\",\n    subtitle = \"South African Western Cape (2011)\"\n  )\n```\n\nI find the result quite visually appealing and it isn't just me: [a separate study](https://journals.sagepub.com/doi/10.1177/2399808319873923){target=\"_blank\"} has provided empirical evidence for the value of balanced cartograms and hexograms as a visualisation tool mapping spatial distributions.\n\n## Summary\n\nThis session has introduced the concept of geographically weighted statistics to examine spatial heterogeneity in a measured variable and to allow for the possibility that its strength (and direction) of correlation with another variable varies from place-to-place. As such, these statistics are a form of local statistic, which is to say they can vary across the map. Sometimes, the parts of the map that we are interested in are small in relation to other parts, creating a problem of invisibility/legibility. Maps inserts, balanced cartograms and hexograms have been introduced as a means to address this visualisation problem.\n\n## Futher reading\n\nGollini I, Lu B, Charlton M, Brunsdon C & Harris P (2015). GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. *Journal of Statistical Software*, 63(17), 1–50. [https://doi.org/10.18637/jss.v063.i17](https://doi.org/10.18637/jss.v063.i17){target=\"_blank\"}\n\n![](martin.jpg){width=100}\n\nBrunsdon C, Comber A, Harris P, Rigby J & Large A (2021). In memoriam: Martin Charlton. *Geographical Analysis*, 54(4), 713–4. [https://doi.org/10.1111/gean.12309](https://doi.org/10.1111/gean.12309){target=\"_blank\"}\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| include: false\ninstalled <- installed.packages()[,1]\nrequired <- c(\"GWmodel\", \"proxy\", \"sf\", \"sp\", \"tidyverse\", \"tmap\", \"stars\", \"raster\", \"remotes\", \"cowplot\",\n              \"cartogram\", \"lpSolve\", \"colorspace\", \"cols4all\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE, repos = \"https://cloud.r-project.org\")\n```\n\n## Introduction\n\nIn the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by 'pooling' the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in `GWmodel` -- [an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models](https://www.jstatsoft.org/article/view/v063i17){target=\"_blank\"}. These are geographically weighted statistics.\n\n## Geographical Weighted Statistics\n\nThe idea behind geographically weighted statistics is simple. Instead of calculating the (global) mean average, for example, for the all the data and the whole map, a series of (local) averages are calculated for various sub-spaces within the map.\n\nImagine a point location, $i$, at position $(u_i, v_i)$ on the map. To calculate the local statistic, first find either the $k$ nearest neighbours to $i$ or all of those within a fixed distance, $d$, from it. Second, to add the geographical weighting in the name of the statistics, apply a weighting scheme whereby the neighbours nearest to $i$ have most weight in the subsequent calculations and the weights decrease with increasing distance from $i$, becoming zero at the $k$th nearest neighbour or at the distance threshold, $d$. Third, calculate, for the point and its neighbours, the weighted mean value (or some other summary statistic) of a variable, using the inverse distance weighting in the calculation. Fourth, repeat the process for other points on the map. This means that if there are $n$ points of calculation then there will be $n$ geographically weighted mean values calculated across the map. These can be then be compared to look for spatial variation.\n\nLet's see this in action, beginning by ensuring the necessary packages are installed and required. As with previous sessions, you might start by opening the R Project that you created for these classes.\n\n```{r}\ninstalled <- installed.packages()[,1]\nrequired <- c(\"colorspace\", \"cols4all\", \"GWmodel\", \"proxy\", \"sf\", \"sp\",\n              \"tidyverse\", \"tmap\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE,\n                                     repos = \"https://cloud.r-project.org\")\n\nrequire(cols4all)\nrequire(GWmodel)\nrequire(sf)\nrequire(tidyverse)\nrequire(tmap)\n```\n\nWe will use the same data as in the previous session but confine the analysis to the Western Cape of South Africa. This is largely to reduce run times but there is another reason that I shall return to presently.\n\n```{r}\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nwards |>\n  filter(ProvinceNa == \"Western Cape\") ->\n  wcape_wards\n```\n\nThe calculated points for the geographically weighted statistics will be the ward centroids. A slight complication here is that `GWmodel` presently is built around the elder `sp` not `sf` formats for handling spatial data in R so the wards need to be converted from the one format to the other.\n\n```{r}\nwcape_wards_sp <- as_Spatial(wcape_wards)\n```\n\nWe can see that `wcape_wards` is of class `sf`,\n\n```{r}\nclass(wcape_wards)\n```\n\nwhereas `wcape_wards_sp` is now of class `SpatialPolygonsDataFrame`, which is what we want.\n\n```{r}\nclass(wcape_wards_sp)\n```\n\nHaving made the conversion we are ready to calculate some geographically weighted statistics, doing so for the `High_income` variable -- the percentage of the population with income R614401 or greater in 2011 -- in the example below.\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>Most warning messages are intended to be helpful and the one generated from the code above is not an exception. It is related to a [change in how coordinate reference systems (CRS) are represented](https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html){target=\"_blank\"} but does not affect the results of the analyses here.</font>\n\nThe results are contained in the spatial data frame (`$SDF`),\n\n```{r}\nhead(gwstats$SDF)\n```\n\nAs well as the local means, the local standard deviations, variances, skews and coefficients of variation are included (the coefficient of variation is the ratio of the standard deviation to the mean). The local medians, interquartile ranges and quantile imbalances could also be added by including the argument `quantile = TRUE` in `gwss()` -- see `?gwss`.\n\nThe results are dependent on the data (of course) but also\n\n-   the kernel (i.e. the shape of the weighting -- the distance decay -- around each point), and\n-   the bandwidth (i.e. the maximum number of neighbours to include, $k$ or the distance threshold, $d$).\n\nThe kernel matters...\n\n![](kernel.jpg)\n\n<font size = 2>Source: [GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models](https://www.jstatsoft.org/article/view/v063i17){target=\"_blank}</font>\n\n... but it (usually) matters much less than the bandwidth, which controls the amount of spatial smoothing: the larger it is, the more neighbours are being averaged over. The trade-off is between bias and precision. A smaller bandwidth is less likely to average-out geographical detail in the data and should create a geographically weighted average, for example, that is representative of the location at the centre of the kernel but it is also dependent on a small number of observations, some or more of which could be in error or unsual outliers for the vicinity.\n\nThe following maps compare a bandwidth of $bw = 10$ nearest neighbours to $bw = 100$. If you zoom into the areas around the north of Cape Town you will see some of the differences. Note that the argument `adaptive = TRUE` sets the bandwidth to be in terms of nearest neighbours, else it would indicate a fixed distance of 10 metres. The advantage of using nearest neighbours is it allows for varying population densities. Otherwise, using a fixed distance, more rural areas will tend to have fewer neighbours than urban ones because those rural areas are larger and more spaced apart.\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw100 <- gwstats$SDF$High_income_LM\n\nwards2 <- pivot_longer(wcape_wards, cols = starts_with(\"bw\"), names_to = \"bw\",\n             values_to = \"GWmean\")\n\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(wards2, names = \"wards\") +\n  tm_fill(\"GWmean\", palette = \"Reds\", title = \"%\",\n          alpha = 0.7,\n          id = \"District_1\",\n          popup.vars = c(\"GW mean:\" = \"GWmean\",\n                         \"Ward ID:\" = \"WardID\"),\n          popup.format = list(digits = 1)) +\n  tm_borders() +\n  tm_facets(by = \"bw\") +\n  tm_legend(title =\n        \"Geographically weighted % population with income R614401 or greater\")\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>A nice feature of the facetting (`tm_facets`) in `tmap` is it has created two dynamically linked maps.</font>\n\n### A slight tangent\n\nYou may note the use of the `pivot_longer` function from the `tidyverse` packages in the code above. To see what this does, take a look at,\n\n```{r}\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  arrange(WardID) |>\n  head(n = 3)\n```\n\nNow compare it with,\n\n```{r}\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  pivot_longer(cols = starts_with(\"bw\"), names_to = \"bw\",\n          values_to = \"GWmean\") |>\n  arrange(WardID) |>\n  head(n = 6)\n```\n\nWhat you can see is that the two columns, `bw10` and `bw100` from the first table have been stacked into rows in the second. Doing this allows us to create the two linked plots by faceting on the bandwidth variable, `bw`, using `tm_facets(by = \"bw\")`. The reverse operation to `pivot_longer()` is `pivot_wider()` -- see `?pivot_wider`. \n\n### 'Pre-calculating' the distance matrix\n\nIn the calculations above, the distances between the ward centroids that are used in the geographical weighting are calculated twice. First in `gwss(wcape_wards_sp, vars = \"High_income\", bw = 10, ...)` and then again in `gwss(wcape_wards_sp, vars = \"High_income\", bw = 100, ...)`. Since those distances don't actually change (the centroids are fixed so therefore are the distances between them) so we might have saved a little computational time by calculating the distance matrix in advance and then using it in the geographically weighted statistics. Curiously, though, it actually takes longer. I assume this is because the data set isn't large enough to justify the extra step of saving the distances in a matrix and then passing that matrix to the `gwss()` function.\n\n```{r}\n# Time to do the calculations without pre-calculating the distance matrix\nsystem.time({\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n# Time to do the calculations with the pre-calculated distance matrix\nsystem.time({\n  coords <- st_coordinates(st_centroid(wcape_wards))\n  dmatrix <- gw.dist(coords, longlat = T)\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n```\n\n## Selecting the bandwidth\n\nAs observed in the maps above, the geographically weighted statistics are a function of the geographical weighting that largely is controlled by the bandwidth. This raises the question of which is the correct bandwidth to use? Unfortunately, the most honest answer is that there is no correct answer, although an automatic bandwidth selection might be tried by calibrating the statistics around the local means.\n\nThe following uses a cross-validation approach,\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n# The selected number of nearest neighbours:\nbw\n```\n\nwhereas the following uses an AIC corrected approach.\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\")\nbw\n```\n\nBoth use a [golden-section search method](https://en.wikipedia.org/wiki/Golden-section_search){target=\"_blank\"} and, if you look at the output, both iterate to a very similar solution in a very similar set of steps. Bandwidths of 17 and 20 have been suggested but, in practice, there is probably little between them so we may as well use the larger.\n\nThat automatic bandwidth selection applies only for the Western Cape wards, however. Recall earlier that the data were filtered (`wards |> filter(ProvinceNa == \"Western Cape\") -> wcape_wards`) with the partial explanation for doing so being to reduce run times. Another explanation is that there is no reason to assume that the spatial autocorrelation that is quantified by the bandwidth selection will be the same everywhere across the map. In fact, it varies from province to province:\n\n```{r}\n#| results: false\nbandwidths <- sapply(unique(wards$ProvinceNa), \\(x) {\n  wards |>\n    filter(ProvinceNa == x) |>\n    as_Spatial() %>%\n    bw.gwr(High_income ~ 1, data = .,\n          adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n          approach = \"AIC\") %>%\n    paste0(\"Bandwidth = \", .)\n})\n```\n\n```{r}\nbandwidths\n```\n\nThis suggests that fitting geographically weighted statistics to too large a study region is not desirable because there is little reason to presume that the same bandwidth should apply throughout it. \n\n## Changing the interpolation (calculation) points\n\nSo far we have been using the ward centroids as the points for which the geographically weighted statistics are calculated. There is no requirement to do so as they could be interpolated at any point within this study region. To demonstrate this, let's reselect the wards in the Western Cape and convert them into a raster grid using the [stars](https://r-spatial.github.io/stars/){target=\"_blank\"} package. Stars is an abbreviation of spatial-temporal arrays and can be used for handling raster (gridded) data, as in the following example, which is taken from [this introduction to the package](https://r-spatial.github.io/stars/articles/stars1.html){target=\"_blank\"}.\n\n```{r}\nif(!(\"stars\" %in% installed)) install.packages(\"stars\", dependencies = TRUE)\nrequire(stars)\nsat_image <- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nsat_image <- read_stars(sat_image)\nplot(sat_image, axes = TRUE)\n```\n\nWe will use the `st_rasterize` function in `stars` to convert the geography of South Africa's Western Cape into a regular grid.\n\n```{r}\nwcape_wards |>\n  st_rasterize(nx = 100, ny = 100) |>\n  st_as_sf() ->\n  gridded\n\npar(mai=c(0,0,0,0))\ngridded |>\n  st_geometry() |>\n  plot()\n```\n\nWe can now calculate the geographically weighted mean for each raster cell, with a bandwidth of 20 nearest neighbours.\n\n```{r}\ngridded_sp <- as_Spatial(gridded)\n\ngwstats <- gwss(wcape_wards_sp, gridded_sp, vars = \"High_income\",\n                bw = 20, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\ngridded$GWmean <- gwstats$SDF$High_income_LM\n\nggplot(gridded, aes(fill = GWmean)) +\n  geom_sf(col = \"light grey\", size = 0) + \n                      # size is the width of the raster cell border\n  scale_fill_continuous_c4a_seq(palette = \"scico.lajolla\") +\n  theme_minimal()\n```\n\n### Using geography to interpolate missing values\n\nThis ability to interpolate at any point within the study region provides a means to deal with missing values in the data. Contained in the `wcape_wards` data is a variable giving the average age of the population in each ward in 2011 but it contains `r length(wcape_wards$age[is.na(wcape_wards$age)])` missing values:\n\n```{r}\nsummary(wcape_wards$age)\n```\n\nThe same observations also record NAs for the `No_schooling` variable.\n\n```{r}\nwhich(is.na(wcape_wards$age))\nwhich(is.na(wcape_wards$No_schooling))\n```\n\nMissing value are a problem when fitting a regression model, for example. Usually they are simply omitted, as in the following case, where the output reports \"(15 observations deleted due to missingness)\".\n\n```{r}\nols1 <- lm(No_schooling ~ age, data = wcape_wards)\nsummary(ols1)\n```\n\n</br>\nAn alternative approach is to replace the missing values with a 'safe' alternative such as the mean age for the values that are not missing and the same for the percentages of the populations without schooling. However, we could also replace each missing value with a locally interpolated mean which fits with the geographical context.\n\nHere are the wards with missing age and no schooling values. These are the points that need to be interpolated.\n\n```{r}\nwcape_wards |>\n  filter(is.na(age)) |>\n  as_Spatial() ->\n  missing\n```\n\nThese are the wards with the values that serve as the data points.\n\n```{r}\nwcape_wards |>\n  filter(!is.na(age)) |>\n  as_Spatial() ->\n  present\n```\n\nFortunately, the missing values seem to be fairly randomly distributed across the study region. It would be a problem if they were all geographically clustered together because interpolating their values from their neighbours would not be successful if their neighbours' values were also missing!\n\n```{r}\npar(mai=c(0, 0, 0, 0))\nplot(present, border = \"light grey\")\nplot(missing, col = \"red\", add = T)\n```\n\nWe can, then, interpolate the missing values from the present ones and match them into the data using base R's `match()` function, matching on their `WardID`. Note that I have done this twice, once for the `age` variable and once for `No_schooling`. This is to allow for the possibility of them having differently sized bandwidths from each other (which they are when using `approach = \"AIC\"`, although not, as it happens, with the default `approach = \"CV\"`).\n\n```{r}\n#| results: false\n\nbw <- bw.gwr(age ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n             approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"age\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch <- match(missing$WardID, wcape_wards$WardID)\nwcape_wards$age[mch] <- gwstats$SDF$age_LM\n\nbw <- bw.gwr(No_schooling ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"No_schooling\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = T)\n\nwcape_wards$No_schooling[mch] <- gwstats$SDF$No_schooling_LM\n```\n\nIt is useful to keep a note of which values are interpolated, so...\n\n```{r}\nwcape_wards$interpolated <- FALSE\nwcape_wards$interpolated[mch] <- TRUE\n```\n\nThere should be `r length(wcape_wards$interpolated[wcape_wards$interpolated])` of them.\n\n```{r}\ntable(wcape_wards$interpolated)\n```\n\nNow returning to our regression model, there are, of course, no longer any missing values and, reassuringly, no evidence that the interpolated values are significantly different from the rest in either their mean `No_schooling` value or in their effect of `age` upon `No_schooling`. \n\n```{r}\nols2 <- update(ols1, . ~ . + interpolated*age)\nsummary(ols2)\n```\n\n## The geographically weighted mean as a low pass filer\n\nThe geographically weighted mean acts as a [smoothing (low pass) filter](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-analyst-toolbox/filter.htm){target=\"_blank\"}. We can see this if we apply it to a part of the satellite image from earlier. (If you try it on the whole image, you will be waiting a long time for the bandwidth to be calculated). \n\nFirst we shall extract band 1 from the image, create a blank raster using the [raster package](https://cran.r-project.org/web/packages/raster/index.html){target=\"_blank\"} and assign it the same values as from the image.\n\n```{r}\nsat_image |>\n  slice(band, 1) |>\n  pull() -> vals\n\nif(!(\"raster\" %in% installed)) install.packages(\"raster\", dependencies = TRUE)\n\ndim(sat_image)\nst_bbox(sat_image)\nr <- raster::raster(nrows = 352, ncol = 349,\n                      xmn = 288776.3, xmx = 298722.8,\n                      ymn = 9110728.8, ymx = 9120760.8)\nraster::crs(r) <- \"EPSG:31985\"\n\n# The image is 349 (row) by 352 (col) whereas the raster is\n# 352 (col) by 349 (row) which is why the values are transposed, t(vals)\nr <- raster::setValues(r, t(vals))\n```\n\nNext we will crop out the top-left hand corner of the image, convert the coordinates and the attributes of those raster cells into a [SpatialPointsDataFrame](https://www.rdocumentation.org/packages/sp/versions/1.6-0/topics/SpatialPoints){target=\"_blank\"} for use with the `GWmodel` functions and calculate the geographically weighted statistics:\n\n```{r}\nr1 <- raster::crop(r, raster::extent(r, 1, 50, 1, 50))\npts <- as(r1, \"SpatialPointsDataFrame\")\n\nbw <- bw.gwr(layer ~ 1, data = pts,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F,\n             approach = \"AIC\")\ngwstats <- gwss(pts, vars = \"layer\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = F)\n```\n\nWe can now compare the original image with the smoothed image.\n\n```{r}\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LM)\nr <- st_as_stars(raster::addLayer(r1, r2))\n\nggplot() + \n  geom_stars(data = r) +\n  facet_wrap(~ band) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n```\n\nHowever, it isn't only the geographically weighted mean that is calculated. You could use the geographically weighted standard deviation, for example, as a high pass filter -- a form of edge detection.\n\n```{r}\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LSD)\nr <- st_as_stars(r2)\n\nggplot() + \n  geom_stars(data = r) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>You might spot that I have not loaded (required) the `raster` package in the code above but have accessed its functions via the `::` notation; for example `raster::setValues()`. That is because if I do load the package, its `select` function will then mask a different function but with the same name in tidyverse's `dplyr`. As I am only using the `raster` package very briefly, I didn't think it was worth the potential confusion.</font>\n\n## Geographically weighted correlation\n\nAccording to the regression model earlier, there is a negative correlation between the age of the population and the percentage without schooling. The correlation across the Western Cape is,\n\n```{r}\ncor(wcape_wards$No_schooling, wcape_wards$age)\n```\n\nThat is, however, the global correlation for what appears (below) to be a [heteroscedastic relationship](https://www.investopedia.com/terms/h/heteroskedasticity.asp){target=\"_blank\"}. \n\n```{r}\nggplot(data = wcape_wards, aes(x = age, y = No_schooling)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship so let's consider that by calculating and mapping the geographically weighted correlations between the two variables.\n\n```{r}\n#| eval: false\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\n```{r}\n#| include: false\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\nThe local correlations range `r round(min(wcape_wards$Corr_No_schooling.age), 3)` to `r round(max(wcape_wards$Corr_No_schooling.age), 3)`, with an interquartile range from `r round(quantile(wcape_wards$Corr_No_schooling.age, probs = 0.25), 3)` to `r round(quantile(wcape_wards$Corr_No_schooling.age, probs = 0.75), 3)`:\n\n```{r}\nsummary(wcape_wards$Corr_No_schooling.age)\n```\n\nIf we add a little 'cartographic know-how' from an earlier session, we can identify that the correlation is stronger in and around Parow than it is in and around Blue Downs, for example.\n\n```{r}\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\",\n                                                 dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |>\n  filter(place == \"city\" | place == \"town\") |>\n  st_join(wcape_wards) |>\n    # A spatial spatial join,\n    # giving the point places the attributes of the wards they fall in\n  mutate(population = as.integer(population)) |>\n  filter(!is.na(ProvinceNa) & population > 60000) ->\n  places\n\nlast_plot() +\n  geom_sf_label_repel(data = places, aes(label = name), alpha = 0.7,\n                      force = 5, size = 2, max.overlaps = 20) +\n  xlab(element_blank()) +\n  ylab(element_blank())\n```\n\nIn general, the correlation appears to be strongest in cities:\n\n```{r}\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  st_join(wcape_wards) %>%\n  st_drop_geometry %>%\n  group_by(place) %>%\n  summarise(meancorr = mean(Corr_No_schooling.age, na.rm = TRUE)) %>%\n  arrange(meancorr)\n```\n\n## Statistical inference and significance\n\nWe can use a randomisation procedure to determine whether any of the local summary statistics may be said to be significantly different from those obtained by chance. The randomisation procedure is found in the function, `gwss.montecarlo()` with a default number of `nsim = 99` simulations. This isn't very many but they are time-consuming to calculate and will be sufficient to demonstrate the process.\n\nThe following code chunk returns to the local mean percentages of high earners. It goes through the complete process of determining a bandwidth using the `bw.gwr()` function, then calculating the local and geographically weighted statistics using `gwss()`, determining the p-values under randomisation, using `gwsss.montecarlo`, then mapping the results. Very few of the results would be adjudged significant but a few are.\n\n![](hazard.gif){width=75}\n\n<font size = 3>Calculating the p-values under randomisation takes some time so please be patient.</font>\n\n```{r}\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = \"High_income\", bw = bw,\n                            kernel = \"bisquare\",\n                            adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$High_income_LM <- gwstats$SDF$High_income_LM\nwcape_wards$High_income_LM[p.values$High_income_LM > 0.025 &\n                     p.values$High_income_LM < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = High_income_LM)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", na.value = \"light grey\",\n                       direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Local mean percentage of higher earrners\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\nA second example considers the local correlations between the `No_schooling` and `age` variables. Again, most of the correlations are insignificant but with a few exceptions.\n\n```{r}\n#| results: false\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n```\n\n```{r}\ngwstats <- gwss(wcape_wards_sp , vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"),\n                bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nwcape_wards$Corr_No_schooling.age[p.values$Corr_No_schooling.age > 0.025 &\n                            p.values$Corr_No_schooling.age < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\",\n                       na.value = \"light grey\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>We have the  problem of repeating testing that we also had when looking for spatial hotspots in the session about spatial autocorrelation. The function `p.adjust()` could be used but we may wonder, as previously, where the methods are too conservative.</font>\n\n## Improving the leigibility of the map\n\n### Using a map insert\n\nBefore for completing this session there is value in addressing the problem that some parts of the map are so small that their contents are almost illegible (too small to be seen). The traditional and probably the most effective way to address this is to add one or more inserts to the map to magnify the small areas. One way to do this is by using the [cowplot](https://cran.r-project.org/web/packages/cowplot/index.html){target=\"_blank\"} package with `ggplot2`. The following example is based on [this tutorial](https://geocompr.github.io/post/2019/ggplot2-inset-maps/){target=\"_blank\"} and maps the percentages of the populations who are relatively high earners in each ward.\n\nFirst, we need to install and require the `cowplot` package.\n\n```{r}\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\", dependencies = TRUE)\nrequire(cowplot)\n```\n\nSecond, the part of the `wards` map to be included in the map insert is extracted, using `st_crop`.\n\n```{r}\nwards_extract <- st_crop(wcape_wards, xmin = 18.2, xmax = 19, ymin = -34.3,\n                         ymax = -33.5)\nplot(st_geometry(wards_extract))\n```\n\nThe bounding box for the extracted area is also obtained as it will be used as a feature in the final map (it will be drawn as a rectangle showing the geographical extent of the insert in the original map).\n\n```{r}\nbbox <- st_as_sfc(st_bbox(wards_extract))\n```\n\nNext, the main part of the map is created...\n\n```{r}\nmain_map <- ggplot(wcape_wards, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\nplot(main_map)\n```\n\n...as is the map insert:\n\n```{r}\ninsert <- ggplot(wards_extract, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_void() +\n  theme(legend.position = \"none\")\nplot(insert)\n```\n\nFinally, the maps are brought together using `cowplot`'s `ggdraw()` and `draw_plot()` functions. We know that some of the values it shows are not necessarily significant under randomisation but we will include them here to just to get \n\n```{r}\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(insert, x = 0, y = 0.25, scale = 0.22) +\n  draw_label(\"Cape Town\\nand\\nsurrounding areas\", x = 0.62, y = 0.78, size = 8)\n```\n\n![](hazard.gif){width=75}\n\n<font size = 3>The positioning of the map insert (`x = ...` & `y = ...`), the scaling of it, and the size and position of the label were all found by trial and error, using various values until I was happy with the results.</font>\n\n### Using a 'balanced carogtram'\n\nAnother approach is to use what has been described as a [visually balanced cartogram](https://journals.sagepub.com/doi/full/10.1177/0308518X17708439){target=\"_blank\"}. A [cartogram](https://gisgeography.com/cartogram-maps/){target=\"_blank\"} -- of which there are lots of interesting examples [on this website](https://www.viewsoftheworld.net/){target=\"_blank\"} -- usually works by distorting a map in a way that rescales the areas in proportion not to their physical size but by some other measured attribute such as their population count. Much of the motivation for this lies in political studies and mapping the results of an election wherein a traditional map can give a very distorted view of the outcome because of how much population density varies across a country. In the example below, rescaling the areas by population correctly shows that the 2020 US Presidential was not a Republican landslide, despite Trump's baseless accusations!\n\n![](USelection2020Cartogram.png)\n<font size = 2>Source: [www.viewsoftheworld.net](https://www.viewsoftheworld.net/?p=5777){target=\"_blank}</font>\n\nThe problem with cartograms is the distortion. Essentially, they are trading one visual problem (invisibility of small areas) for another (the amount of geographical distortion). The idea of a balanced cartogram comes from an awareness that sometimes it is sufficient just to make the small areas bigger and/or the big areas smaller, without causing too much geographical distortion. One way to achieve this is to scale the places by the square root of the original areas, as in the following example.\n\n```{r}\nif(!(\"cartogram\" %in% installed)) install.packages(\"cartogram\",\n                                                   dependencies = TRUE)\nrequire(cartogram)\n\n# Convert from longitude/latitude to a grid projection\nwcape_wards %>%\n  st_transform(22234) %>%\n  mutate(area = as.numeric(st_area(.)),\n         w = as.numeric(sqrt(area))) ->\n  wards_prj\n\n# Create the cartogram -- here a contiguous cartogram is used, see ?cartogram\nwards_carto <- cartogram_cont(wards_prj, weight = \"w\", maxSizeError = 1.4,\n                              prepare = \"none\")\n\nggplot(wards_carto, aes(fill = High_income)) +\n  geom_sf(col = \"white\", size = 0.1) +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title =\n      \"Cartogram of the percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\n```\n\nThe results are ok, drawing attention to the wards with higher percentages of higher earners but clearly there is geographical distortion in the map, too.\n\n### Using a hexogram\n\nThe final approach to be considered here is what has been described as [hexograms](https://www.tandfonline.com/doi/full/10.1080/17445647.2018.1478753){target=\"_blank\"} -- a combination of tile maps and cartograms that make small areas bigger on the map but try and limit the geographic distortion. The [original method](https://rpubs.com/profrichharris/hexograms){target=\"_blank\"} also preserved typology (i.e. contiguous neighbours remained so in the final map) but is dated and **very** slow to operationalise as it was really just a proof of concept. A faster method but one that cannot guarantee to preserve typology is presented below.\n\nFirst, we need some functions to create the hexogram.\n\n```{r}\nif(!(\"cartogram\" %in% installed.packages()[,1])) install.packages(\"cartogram\")\nif(!(\"sf\" %in% installed.packages()[,1])) install.packages(\"sf\")\nif(!(\"lpSolve\" %in% installed.packages()[,1])) install.packages(\"lpSolve\") \n\nrequire(sf)\n\ncreate_carto <- \\(x, w = NULL, k = 2, itermax = 25, maxSizeError = 1.4) {\n  if(class(x)[1] != \"sf\") stop(\"Object x is not of class sf\")\n  if(is.null(w)) x$w <- as.numeric(st_area(x))^(1/k)\n  cartogram::cartogram_cont(x, \"w\", itermax = itermax,\n                            maxSizeError = maxSizeError, prepare = \"none\")\n}\n\n\ncreate_grid <- \\(x, m = 6) {\n  bbox <- st_bbox(x)\n  ydiff <- bbox[4] - bbox[2]\n  xdiff <- bbox[3] - bbox[1]\n  \n  n <- m * nrow(x)\n  ny <- sqrt(n / (xdiff/ydiff))\n  nx <- n / ny\n  nx <- ceiling(nx)\n  ny <- ceiling(ny)\n  \n  grd <- st_sf(st_make_grid(x, n = c(nx, ny), square = FALSE))\n}\n\n\ngrid_data <- \\(x, grd, k = 2) {\n  x_pts <- st_centroid(st_geometry(x), of_largest_polygon = TRUE)\n  grd_pts <- st_centroid(grd)\n  \n  cost.mat <- st_distance(x_pts, grd_pts)^k\n  row.rhs <- rep(1, length(x_pts))\n  col.rhs <- rep(1, nrow(grd_pts))\n  row.signs <- rep(\"=\", length(x_pts))\n  col.signs <- rep(\"<=\", nrow(grd_pts))\n\n  optimisation <- lpSolve::lp.transport(cost.mat, \"min\", row.signs, row.rhs,\n                                        col.signs, col.rhs)$solution\n  \n  mch <- sapply(1:nrow(optimisation), \\(i) which(optimisation[i, ] == 1))\n  grd <- st_sf(grd[mch,])\n  cbind(grd, st_drop_geometry(x))\n}\n\n\ncreate_layers <- \\(x, grd) {\n  \n  area <- sapply(1: nrow(x), \\(i) {\n    y <- st_intersects(x[i, ], grd)[[1]]\n    if(i %in% y) {\n      if(length(y) == 1) return(st_area(x[i, ]))\n      if(length(y) > 1) {\n        area <- st_area(x[i, ])\n        overlaps <- y[-(y == i)]\n        area <- area - sum(st_area(st_intersection(st_geometry(x[i, ]),\n                                                st_geometry(grd[overlaps, ]))))\n        return(area) \n      }\n    } else {\n      return(0)\n    }\n  })\n\n  i <- which(area > 2*as.numeric(st_area(grd))[1])\n  list(x[i, ], grd[-i, ])\n}\n```\n\nNext, we run through the stages of its creation, which are to start with a balanced cartogram (`create_carto()`), then overlay that with a grid of hexagons (`create_grid()`), next assign places in the original map to a hexagon (`grid_data()`), then plot the results as two layers (`create_layers()`) to give a mash-up of the cartogram and the hexagons.\n\n```{r}\nwards_carto <- create_carto(wards_prj)\ngrd <- create_grid(wards_carto)\nwards_grid <- grid_data(wards_carto, grd)\nwards_layers <- create_layers(wards_carto, wards_grid)\n\nggplot() +\n  geom_sf(data = wards_layers[[1]], aes(fill = High_income),\n          col = \"white\", size = 0.4) +\n  geom_sf(data = wards_layers[[2]], aes(fill = High_income),\n          col = \"dark grey\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"Hexogram % of the population who are higher earners\",\n    subtitle = \"South African Western Cape (2011)\"\n  )\n```\n\nI find the result quite visually appealing and it isn't just me: [a separate study](https://journals.sagepub.com/doi/10.1177/2399808319873923){target=\"_blank\"} has provided empirical evidence for the value of balanced cartograms and hexograms as a visualisation tool mapping spatial distributions.\n\n## Summary\n\nThis session has introduced the concept of geographically weighted statistics to examine spatial heterogeneity in a measured variable and to allow for the possibility that its strength (and direction) of correlation with another variable varies from place-to-place. As such, these statistics are a form of local statistic, which is to say they can vary across the map. Sometimes, the parts of the map that we are interested in are small in relation to other parts, creating a problem of invisibility/legibility. Maps inserts, balanced cartograms and hexograms have been introduced as a means to address this visualisation problem.\n\n## Futher reading\n\nGollini I, Lu B, Charlton M, Brunsdon C & Harris P (2015). GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. *Journal of Statistical Software*, 63(17), 1–50. [https://doi.org/10.18637/jss.v063.i17](https://doi.org/10.18637/jss.v063.i17){target=\"_blank\"}\n\n![](martin.jpg){width=100}\n\nBrunsdon C, Comber A, Harris P, Rigby J & Large A (2021). In memoriam: Martin Charlton. *Geographical Analysis*, 54(4), 713–4. [https://doi.org/10.1111/gean.12309](https://doi.org/10.1111/gean.12309){target=\"_blank\"}\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"gwstats.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","theme":"flatly","title":"Geographically Weighted Statistics"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}