[
  {
    "objectID": "answers-exercise1.html",
    "href": "answers-exercise1.html",
    "title": "Follow-up exercise 1",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n\nCode\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "answers-exercise1.html#including-plots",
    "href": "answers-exercise1.html#including-plots",
    "title": "Follow-up exercise 1",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "Measuring spatial autocorrelation",
    "section": "",
    "text": "We begin by recreating one of the maps from the previous session. You may wish to change your working directory to be the same as previously if it is not already (it should be if you are saving all your files in a Project and begin this session by opening that Project).\n\n\nCode\ninstalled <- installed.packages()[,1]\nrequired <- c(\"tidyverse\", \"sf\", \"RColorBrewer\", \"classInt\", \"ggplot2\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(sf)\nrequire(RColorBrewer)\nrequire(classInt)\nrequire(ggplot2)\n\nif(!file.exists(\"municipal.RData\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/municipal.RData?raw=true\", \"municipal.RData\", mode = \"wb\")\nload(\"municipal.RData\")\n\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\n\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population with no schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities, creating clusters of red and of blue values (and of yellow too). However, simply ‘eye-balling’ the map to look for patterns isn’t very scientific and it can be very deceptive. You can probably see patterns in the following map, too, but they arise from an entirely random permutation of the previous map’s data – all I have done to generate the map is randomly ‘shuffle’ the data around the locations on the map."
  },
  {
    "objectID": "autocorrelation.html#cols4all",
    "href": "autocorrelation.html#cols4all",
    "title": "Measuring spatial autocorrelation",
    "section": "cols4all",
    "text": "cols4all\nBefore we get to the main focus of today’s session, which is about assessing whether the patterns we think we see in the map might be random and whether there are clusters of ‘hot spots’ or ‘cold spots’ within it, it is worth introducing a new package which (at the time of writing: Feb 7, 2023) was published, on CRAN, just a few days ago. It is called cols4all and is introduced, with a very useful vignette, here. It is described as containing “a large collection of palettes (to be precise 436 at the time of writing), but with the central question: which palettes are good and why?”\n\n\nCode\nif(!(\"colorspace\" %in% installed)) install.packages(\"colorspace\",\n                                                  dependencies = TRUE)\nif(!(\"cols4all\" %in% installed)) install.packages(\"cols4all\",\n                                                  dependencies = TRUE)\nrequire(cols4all)\n\n\nIf we now activate its dashboard, we can learn a lot about different colour palettes and whether, for example, they are colour-blind friendly (try opening in browser if the dashboard does not load properly but that option appears).\n\n\nCode\nc4a_gui()\n\n\n\nWe can discover, for example, that the palette blue_red3 in the hcl series goes from blue to red and is colour-blind friendly. Let’s update the previous map, using cols4all to select the palette and keeping in mind that the fill colour is determined from the variable municipal$No_schooling_gp, which is a discrete (categorical) factor (use class(municipal$No_schooling_gp) to confirm this). You may need to close the dashboard, above, before you can proceed by hitting the esc key after clicking in the R Console.\n\n\nCode\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"hcl.blue_red3\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#morans-test",
    "href": "autocorrelation.html#morans-test",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran’s test",
    "text": "Moran’s test\nThe classic way of quantifying how similar places are to their neighbours is to calculate the Moran’s statistic, which is a measure of spatial autocorrelation – of how much the values of a variable exhibit spatial clustering of alike values (positive spatial autocorrelation) or of ‘opposite’ values (negative spatial autocorrelation). We can calculate this statistic in R using the spdep package. You may recall that that is a problem with the object (the map) municipal because of an invalid geometry (see previous session and try which(!st_is_valid(municipal)) to confirm). We were able to partially ‘fix’ this by changing its coordinate reference system.\n\n\nCode\nif(!(\"spdep\" %in% installed)) install.packages(\"spdep\", dependencies = TRUE)\nrequire(spdep)\n\nmunicipal <- st_transform(municipal, 3857)\n\n\n\nCreating a neighbours list\nIf the purpose of a Moran’s test is to quantify how similar places are to their neighbours, then the first step is to define neighbours. How to do so isn’t necessarily obvious. Think if I asked you to identify your neighbours. Would it just be people who lived next door to you? Or within a certain distance of your house? Or those you interact with most? Or…?\nIn the following example, neighbours are places that share a border (places that are contiguous). Presently it is sufficient for them to meet at a single point, so, if two places happened to be triangular in shape, it would be sufficient for the corners of those triangles to touch to count as neighbours. If the requirement is that they share an edge, not merely a corner, then change the default argument from queen = TRUE to queen = FALSE (see ?poly2nb for details).\n\n\nCode\nneighbours <- poly2nb(municipal)\n\n\n\nThe function poly2nb() includes the argument snap, which is the threshold distance boundary points can be apart to still be considered contiguous. It has a default value of sqrt(.Machine$double.eps) which, on my present computer, is equal to 1.4901161^{-8}and is tiny. The problem is it can sometimes be too tiny, not capturing neighbourhood relationships when, for example, there are ‘slivers’ or slight gaps between places. To avoid this, it can be helpful to replace the default value with a small but still larger distance, e.g.poly2nb(municipal, snap = 1)\\. \nThe summary of neighbours reveals that, on average, each South African municipality has 5.2 neighbours but it can range from 1 (the 183rd region in the municipal data) to 10 (region 69). The most frequent number is 6.\n\n\nCode\nsummary(neighbours)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1216 \nPercentage nonzero weights: 2.220761 \nAverage number of links: 5.196581 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 24 42 52 69 31  3  4  1 \n1 least connected region:\n183 with 1 link\n1 most connected region:\n69 with 10 links\n\n\n\nIt’s pretty common for the average number of contiguous neighbours to be about 5 and the most frequent number of contiguous neighbours to be 5 or 6. It suggests that the spatial configuration of the places is loosely approximating a hexagonal tessellation. But, of course, it isn’t guaranteed. A raster grid of square cells has 4 contiguous neighbours across the grid, except at the edges.\nThe neighbours of region 69 are,\n\n\nCode\nneighbours[[69]]\n\n\n [1]  60  64  68 139 153 154 157 158 164 165\n\n\nIt is instructive to write the list of neighbours to an external neighbours file,\n\n\nCode\nwrite.nb.gal(neighbours, \"neighbours.gal\")\n\n\n… which can then be viewed by using file.edit(\"neighbours.gal\"). The file will look like the below and has a very simple format. The top line say there are 234 regions. Going down, the first of these has 4 neighbours, which are regions 13, 14, 15 and 16. The second has 6 neighbours, which are 3, 4, 8, 18, 189, 234, and so forth. The same information could be encoded in a \\(234\\times234\\) matrix, where cell \\((i, j)\\) is given a value of one if \\(i\\) and \\(j\\) are considered neighbours, else zero. The problem with that approach is most of the matrix is sparse (contains nothing but zeros) because most regions are not neighbours. It is quicker to state which regions are neighbours. The rest, by definition, are not.\n\nThese neighbourhood relationships can be viewed as a graph by extracting the coordinate points (st_coordinates()), of the centroids (st_centroid()), of the polygons that represent each municipality, and by using the plot functions for sf (simple features) and nb (neighbours list) objects. The argument of_largest_polygon = TRUE returns the centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the whole MULTIPOLYGON, a multipolygon being when one place is represented by multiple polygons such as a mainland and an offshore island (so of_largest_polygon = TRUE would give the point at the centre of the mainland). Setting this argument to true should help the centroid to lie within the boundary of the place, rather than, say, in the sea between the mainland and an island, although it isn’t guaranteed.\n\nImagine an area that for some reason is roughly C shaped. Where would the centroid of that be and would it be within the area’s own boundary?\n\n\nCode\ncoords <- st_centroid(municipal, of_largest_polygon = TRUE)\npts <- st_coordinates(coords)\n\npar(mai = c(0, 0, 0, 0))  # Remove the margins and white space around the plot\nplot(st_geometry(municipal), border = \"grey\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\n\nCreating spatial weights\nThe neighbourhood list simply defines which places are neighbours. The spatial weights goes a step further and gives a weight to each neighbourhood link. One motivation for doing this is to stop any statistic that is based on a sum across neighbourhood links to be dominated by those neighbourhoods with most neighbours. Moran is one such statistic. Hence, if a region has six neighbours, each of those is given a weight of \\(1/6\\). If it has four, \\(1/4\\), and so forth. This is called row-standardisation and is the default style in the conversion of a neighbourhood to a spatial weights list with the function nb2listw(). See ?nb2listw for alternative specifications.\n\n\nCode\nspweight <- nb2listw(neighbours)\n# Here are the neighbours of and weights for the first region:\nspweight$neighbours[[1]]\n\n\n[1] 13 14 15 16\n\n\nCode\nspweight$weights[[1]]\n\n\n[1] 0.25 0.25 0.25 0.25\n\n\n\nDealing with places that don’t have any neighbours\nNot all places have neighbours. Islands, for example, can be separated from the mainland by the sea. If you attempt to create spatial weights using the nb2listw() function with a neighbours list that includes places without neighbours, then you will get an error message:\nError in nb2listw(neighbours) : Empty neighbour sets found.\nThere are four ways you can address this problem.\n\nChange the snap distance in the function poly2nb() to include, as neighbours, places that appear not to actually share a border but are within a certain threshold distance apart. This can be helpful to deal with digitisation errors – when, for example, there are small gaps between places that do really share a boundary.\nChange the way you are defining the neighbourhood relationships – for example, by identifying the \\(k\\) nearest neighbours of each location, regardless of whether they share a border or not (see below). Everywhere has somewhere that is closest to it.\nSave the .gal file using write.nb.gal() and then manually edit it to make any connections you wish to between places. You can read the revised version back into R using the function read.gal() – see ?read.gal().\nLeave it as it, with some places not having any neighbours, but specify the argument zero.policy = TRUE in nb2listw so it allows empty sets. For example, spweight <- nb2listw(neighbours, zero.policy = TRUE). Other related functions will also need zero.policy = TRUE to be included, such as moran.test(), moran.plot() and localmoran().\n\nPresently, however, this is not an issue because none of the South African municipalities are without a contiguous neighbour.\n\n\n\nCalculating the Moran’s value\nTo recap: we began with a definition of neighbours based on contiguity. We then row-standardised those weights to generate the spatial weights. With those we can now run a Moran’s test to measure the strength of spatial autocorrelation between neighbours in the municipal$No_schooling variable.\n\n\nCode\nmoran <- moran.test(municipal$No_schooling, spweight)\nmoran\n\n\n\n    Moran I test under randomisation\n\ndata:  municipal$No_schooling  \nweights: spweight    \n\nMoran I statistic standard deviate = 14.014, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.576702139      -0.004291845       0.001718747 \n\n\nThe Moran statistic is 0.577 and the 95% confidence interval is,\n\n\nCode\nz <- c(-1.96, 1.96)\nround(moran$estimate[1]  + z * sqrt(moran$estimate[3]), 3)\n\n\n[1] 0.495 0.658\n\n\nSince the confidence interval does not include the expected value of -0.004, we can conclude that there is statistically significant positive autocorrelation in the variables – municipalities with higher percentages of no schooling tend to be surrounded by other municipalities with the same. Similarly, low values tend to be surrounded by other ones that are low.\n\nThe expected value is very close to zero so what we are almost saying is that because the confidence interval does not span zero so there is evidence of positive spatial autocorrelation. Although this is quite close to being true, to actually be correct would require the expected value of the statistic to be zero with no spatial autocorrelation. It isn’t. It is presently -0.004 and approaches zero as the number of observations increases: \\(E(I) = -1 / (n - 1)\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "autocorrelation.html#moran-plot-and-local-moran-values",
    "href": "autocorrelation.html#moran-plot-and-local-moran-values",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran plot and local Moran values",
    "text": "Moran plot and local Moran values\nWhilst there is positive spatial autocorrelation in the values overall, not everywhere is surrounded by similar values. The following plot has 4 quadrants marked upon it. The top right indicates places where both they and their average neighbour have above average values of municipal$No_schooling. We can describe these as high-high clusters on the map. The bottom left indicates places where they and their average neighbour have below average values. These are low-low clusters. Both the high-high and the low-low contribute to positive spatial autocorrelation because, for these, the places and their neighbours display similar values. The two other quadrants do not. In the top left are low-high clusters. In the bottom right are high-low. There reveal clusters of dissimilar values (negative spatial autocorrelation). In the chart, the high-high and low-low places are more plentiful than the low-high and high-low ones, hence the upwards sloping line of best fit and the positive Moran statistic.\n\n\nCode\n# Return the plot margins to their default values in R:\npar(mai=c(1.02,0.82,0.82,0.42))\nmoran.plot(municipal$No_schooling, spweight)\n\n\n\n\n\nIt is straightforward to map which quadrant each place belongs to. First, we calculate the local Moran statistics proposed by Anselin (1995). A local statistic is one that applies to a subspace of the map, whereas a global statistic is a summary measure for the whole map. Anselin showed that the (global) Moran statistic can be decomposed into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours. There are 234 municipalities in the data so there will be 234 local Moran values too.\n\n\nCode\nlocalm <- localmoran(municipal$No_schooling, spweight)\n\n\nUsefully, if we look at the attributes of localm, we discover an attribute named quadr which contains what we want and which can be mapped. In fact, it includes three different versions of what we might want, the differences being due to which average “low” and “high” are defined by (see the text below the section Value in ?localmoran). Let’s now map one of them, still using the cols4all package to select a colour palette.\n\n\nCode\nnames(attributes(localm))\n\n\n[1] \"dim\"      \"dimnames\" \"call\"     \"class\"    \"quadr\"   \n\n\nCode\nquadr <- attr(localm, \"quadr\")\nhead(quadr)\n\n\n      mean   median    pysal\n1 Low-High Low-High Low-High\n2  Low-Low  Low-Low  Low-Low\n3  Low-Low  Low-Low  Low-Low\n4 High-Low High-Low High-Low\n5  Low-Low  Low-Low  Low-Low\n6  Low-Low  Low-Low  Low-Low\n\n\nCode\nggplot(data = municipal, aes(fill = quadr$median)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"parks.charmonix\") +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nUnfortunately, the resulting map is somewhat misleading because not all of the local Moran values are statistically significant and some of the various high-high, low-low, etc. pairings my be only trivially alike or dissimilar. Looking at the top of the local Moran data suggests a way of isolating those that are statistically significant and is adopted in the following map.\n\n\nCode\nhead(localm, n = 3)   # The p-values are in column 5\n\n\n          Ii          E.Ii     Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.1260406 -0.0055341954 0.31779548 -0.2137648      0.8307305\n2  0.1064341 -0.0020006564 0.07619128  0.3928401      0.6944376\n3  0.1402600 -0.0003160278 0.01205565  1.2803122      0.2004354\n\n\nCode\nquadr[localm[,5] > 0.05 | is.na(localm[,5]), ] <- NA\n\nggplot(data = municipal, aes(fill = quadr$median)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"parks.charmonix\",\n                              na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nArguably, this new map may still not apply a strict enough definition of statistical significance because the issue of repeat testing has not been tackled. Remember, there are 234 places, 234 local Moran values and therefore 234 tests of significance. Essentially the issue is if we test often enough, then it is hardly surprising if some values emerge as ‘significant’. It’s a bit like a fishing trip where we keep casting until we finally catch something.\nThe p-values can be adjusted for this using R’s p.adjust() function. The following example uses a false discovery rate method (method = fdr) but other alternatives include method = bonferroni. A question is whether this is now too strict given that there are not 234 independent tests. Rather, the data have overlapping geographies (places share neighbours) as well as spatial dependencies.\n\n\nCode\nquadr[p.adjust(localm[,5], method = \"fdr\") > 0.05 | is.na(localm[,5]), ] <- NA\n\n# There is now a problem in that only 2 of the 4 categories remain,\n# which are Low-Low and High-High:\nunique(quadr)\n\n\n        mean    median     pysal\n1       <NA>      <NA>      <NA>\n68   Low-Low   Low-Low   Low-Low\n92 High-High High-High High-High\n\n\nCode\n# For compatibility with the previous map, I extract its palette\npal <- c4a(\"parks.charmonix\", 4)\npal\n\n\n[1] \"#008FF8\" \"#B6AA0D\" \"#E2C2A2\" \"#E23B0E\"\n\n\nCode\n# and then only use the first and fourth of those colours in the new map:\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(name = \"%\", type = pal[c(1,4)], na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant (p-adjusted) local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#issues-with-the-moran-statistic",
    "href": "autocorrelation.html#issues-with-the-moran-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Issues with the Moran statistic",
    "text": "Issues with the Moran statistic\n\nHow to define neighbours?\n\nn-order contiguity\nAny statistic that includes spatial weights is dependent upon how those weights are defined: how, then, to decide which places are neighbours and also how much weight each neighbourhood connection should be given in the calculation? The calculations above use first order contiguity (places that share a boundary) but we could extend that definition to include neighbours of neighbours (or more):\n\n\nCode\nneighbours <- nblag(neighbours, maxlag = 2)\n\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))   # Plot graphics in a 1 row by 2 column grid\nplot(st_geometry(municipal), border = \"grey\", main = \"First order contiguity\")\nplot(neighbours[[1]], pts, add = T)\nplot(st_geometry(municipal), border = \"grey\", main = \"Second order contiguity\")\nplot(neighbours[[2]], pts, add = T)\n\n\n\n\n\nChanging the definition of neighbours does, of course, change the value of the Moran statistic and it will change the local Moran values too.\n\n\nCode\nlapply(neighbours, \\(x) {\n  nb2listw(x) %>%\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\n\n[[1]]\nMoran I statistic \n        0.5767021 \n\n[[2]]\nMoran I statistic \n        0.3890747 \n\n\nIn passing, if you remember the discussion around piping and the differences between %>% and |> back when we were looking at ‘Flavours of R’ and `Tidyverse’ then you may recall how the following code is equivalent to the above but more ‘clunky’:\n\n\nCode\nlapply(neighbours, \\(x) {\n  nb2listw(x) |>\n    (\\(y) moran.test(municipal$No_schooling, y))() ->\n    moran\n  moran$estimate[1]\n})\n\n\n[[1]]\nMoran I statistic \n        0.5767021 \n\n[[2]]\nMoran I statistic \n        0.3890747 \n\n\n\n\nk nearest neighbours\nThere is no particular reason to stick with a contiguity-based definition. We could, for example, look for the \\(k\\) nearest neighbours (or, more precisely, the \\(k\\) nearest centroids to the centroid of each region), as in the two examples below.\n\n\nCode\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))\n\nneighbours <- knn2nb(knearneigh(coords, k = 5))\nplot(st_geometry(municipal), border = \"grey\", main = \"Five nearest neighbours\")\nplot(neighbours, pts, add = T)\n\nneighbours <- knn2nb(knearneigh(coords, k = 10))\nplot(st_geometry(municipal), border = \"grey\", main = \"Ten nearest neighbours\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\nThe function knearneigh() contains the default argument, longlat = NULL. It should be ok not to change this here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. If you suspect it isn’t or if coords is simply a matrix of point coordinates, not an explicitly spatial object, change the default argument to longlat = TRUE.\nIn fact, we can run through all the possible values of \\(k\\), from \\(1\\) to \\(k_{max} = (n - 1)\\) where \\(n\\) is the number of municipalities, and consider the Moran statistics that they generate. The resulting chart shows that the statistic is highly dependent on the scale of the analysis: as \\(k\\) increases, neighbourhood relationships extend over an increasing portion of the map, and the statistic tends to decline. It declines because nearby places tend to have similar values whereas those that are further away introduce more variation because of the spatial heterogeneity across the map.\n\nThe code will generate a lot of warning messages. You can ignore them. They are warning you that \\(k\\) has become a large subset of all \\(n\\).\n\n\nCode\nn <- nrow(municipal)\nI <- sapply(1: (n-1), \\(k) {\n  knearneigh(coords, k) |>\n    knn2nb() |>       # I am mixing my\n    nb2listw() %>%    # pipes here (deliberately!)\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\nggplot(data.frame(k = 1:(n-1), I = I), aes(x = k, y = I)) +\n  geom_line() +\n  ylab(\"Moran statistic\") -> g\ng\n\n\n\n\n\nIn principle the chart could be used to identify an ‘optimal’ value of \\(k\\). Unfortunately, the present case offers few clues as to what that optimal value should be. We could use the inflection point of the curve, based on this tutorial:\n\n\nCode\nif(!(\"inflection\" %in% installed)) install.packages(\"inflection\",\n                                                    dependencies = TRUE)\nrequire(inflection)\nk <- 1: (n-1)\ncheck_curve(k, I)\n\n\n$ctype\n[1] \"convex_concave\"\n\n$index\n[1] 0\n\n\nCode\ninfl_pt <- bese(k, I, index = 0)$iplast\ncat(\"\\nThe inflection point is at k = \", infl_pt, \"\\n\")\n\n\n\nThe inflection point is at k =  142 \n\n\nCode\ng + geom_vline(xintercept = infl_pt, linetype = \"dotted\")\n\n\n\n\n\nThe result has correctly identified where the curve turns and you could reasonably argue that this is a good value of \\(k\\) to avoid given the Moran’s statistic is higher for values of \\(k\\) either side of it. However, eliminating one value still leaves a lot of others to choose from!\nWe could also try selecting based on seeking to avoid a sharp decrease in the Moran statistic for an increase in \\(k\\) by one. The sharpest fall comes by changing from \\(k = 3\\) to \\(k = 4\\) (the function diff() in the code chunk below calculates the difference between the value of one number in a vector and the next number to its right in that same vector):\n\n\nCode\nslope <- diff(I) # It's really diff(I) / diff(k) but diff(k) = 1 in all cases here\nmax_fall <- k[which.min(slope)]\ncat(\"\\nThe greatest fall in the Moran value is between k = \",\n    max_fall, \"and k = \", max_fall + 1, \"\\n\")\n\n\n\nThe greatest fall in the Moran value is between k =  3 and k =  4 \n\n\nCode\ng + geom_vline(xintercept = max_fall, linetype = \"dashed\")\n\n\n\n\n\nThis shows that the ‘cost’ in increasing from \\(k = 3\\) to \\(k = 4\\) is a relatively big drop in the similarity of the neighbours to each other (it’s still not actually that large though; it’s a drop of 0.032.\nWe could look at the drop in the Moran statistic over a larger increase in \\(k\\) than one. In the following code chunk the differences are calculated over a span of ten.\n\n\nCode\nslope <- diff(I, lag = 10) # It's really diff(y) / diff(x) but diff(x) = 10 in all cases here\nmax_fall <- k[which.min(slope)]\ncat(\"\\nNow the greatest fall in the Moran value is between k = \",\n    max_fall, \"and k = \", max_fall + 10, \"\\n\")\n\n\n\nNow the greatest fall in the Moran value is between k =  1 and k =  11 \n\n\nCode\ng + geom_vline(xintercept = c(max_fall, max_fall + 10), linetype = \"dotdash\")\n\n\n\n\n\nIn either case, what it favours is a small value of \\(k\\). This is hardly surprising because near neighbours do tend to be more similar – that’s Tobler’s first ‘law’ of geography, which, although not a law at all and with plenty of exceptions, still has a ring of truth to it. Similarities tend to drop away most quickly at fairly short distances and then ‘flatten out’ at greater distances, which is what the curve in the charts above is showing.\nSo, then, the ‘optimal’ value of \\(k\\) is a small number of nearest neighbours? Not necessarily! When calculating the Moran statistic it is based on the correlation of each location with their neighbours. Imagine there is error in the data; some rogue results. Alternatively, imagine some places are just very unusual. Those errors or outliers will have more influence in the calculation if they are pooled with only a small number of other neighbours than if more neighbours are included so that their unusualness has more chance of being ‘averaged out’. But, if we do choose to include lots of neighbours then we are including ones that are further away and probably less like the location we are correlating them against. We will return to this trade-off when we look at geographically weighted statistics.\nHow else could we decide on the value of \\(k\\) to select? The p-value for the Moran statistics is not shown in the charts but is least when \\(k = 200\\). That might be a justification, of sorts, for choosing \\(k = 200\\) but it is splitting hairs somewhat when all but one of the p-values is tiny and well below \\(p = 0.001\\).\nPerhaps the best way is simply to decide on a threshold value for the Moran statistic and use that but quite how you decide on that threshold…? Here is the max value of \\(k\\) for a purely arbitrary threshold of \\(I > 0.4\\).\n\n\nCode\nthreshold_k <- max(which(I > 0.4))\ncat(\"\\nThe maximum value of k for a Moran value greater than 0.4 is \",\n    threshold_k, \"\\n\")\n\n\n\nThe maximum value of k for a Moran value greater than 0.4 is  21 \n\n\nCode\ng + geom_vline(xintercept = threshold_k, linetype = \"dashed\")\n\n\n\n\n\n\n\nDistance bounds\nOther ways of defining neighbours include whether they lie within a lower or upper distance bound of each other: see ?dnearneigh and this vignette on Creating Neighbours.\n\n\n\nHow to interpet the I statistic\nConceptually, the Moran statistic is easy to understand: it is a (global) measure of the correlation between the values of a variable at a set of locations, and the value of the same variable for those locations’ neighbours.\nMore simply, it could be thought of as the correlation between locations and their average neighbour. Except it isn’t. Or, rather, it is a measure of correlation, just not the more commonly used Pearson correlation. The difference is evident in the following comparison, with \\(k = 21\\), where the Moran value is less than two thirds of the Pearson correlation between locations and their average neighbour, where the value for the average neighbour is calculated using last.listw().\n\n\nCode\nspweight <- nb2listw(knn2nb(knearneigh(coords, 21)))\npearson <- cor(lag.listw(spweight, municipal$No_schooling),\n               municipal$No_schooling)\nmoran <- moran.test(municipal$No_schooling, spweight)$estimate[1]\ndata.frame(person = pearson, moran = moran, row.names = \"correlation\")\n\n\n               person     moran\ncorrelation 0.6643967 0.4013773\n\n\nAs Brunsdon and Comber note, the value of Moran’s I is not constrained to be in the range from -1 to +1 but changes with the spatial weights matrix. Following, Jong et al. (1984, p. 20), the extremes for the current spatial weights are,\n\n\nCode\nlistw2mat(spweight) %>%\n  range(eigen((. + t(.)) / 2))\n\n\n[1] -0.420394  1.031139\n\n\nIf this is correct then not only is this not in the range -1 to 1, it is not symmetric around the expected (null) value of -0.004.\nGiven this, it might, perhaps, be regarded as preferable to use the more readily interpreted Pearson correlation between locations and their average neighbour. That correlation, with \\(k = 21\\) nearest neighbours, was calculated above and is 0.664. Whether that figure arose by chance can be assessed using a permutation approach. Given the geography of the South African municipalities, their neighbourhood relations and given the data values, then permuting those values randomly 1000 times allows us to look whether the value of 0.664 is unusual compared to what could arise by chance. Specifically, we look at whether the value is outside of the ‘middle 95%’ of values that have been generated through randomisation. If it is, then it suggests a less than 5-in-100 probability that it arose by chance. (A permutation approach is also available for the global and local Moran statistics: see ?moran.mc and ?localmoran_perm.)\n\n\nCode\nn <- length(municipal$No_schooling)\nsapply(1:1000, \\(x) {\n  sample(municipal$No_schooling, n) %>% \n    cor(lag.listw(spweight, .))\n}) %>%\n  quantile(prob = c(0.025, 0.975))\n\n\n      2.5%      97.5% \n-0.2046279  0.1360938 \n\n\nNote, however, that there is a subtle but important difference between Moran’s I and the Pearson correlation of the values and their average neighbour. Moran’s I can be used to determine whether, for example, high values of a measurement tend to be surrounded by average neighbours that also have a high value, where the measurement scale is the same for the locations and their average neighbour. What the Pearson correlation is assessing is whether values that are high, relatively speaking, for all the locations are surrounded by average neighbours that have a high value relatively speaking, for all the average neighbours. What they are measuring the correlation of is therefore not the same. The difference can be seen in the formulae below:\n\\(I_{xy} = \\dfrac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\)\n\\(r_{xy} = \\dfrac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\)\nwhere \\(I_{xy}\\) is the Moran value, \\(r_{xy}\\) is the Pearson value, \\(x\\) is the measured value at each location and \\(y\\) is the (spatially lagged) value for the average neighbour."
  },
  {
    "objectID": "autocorrelation.html#getis-and-ord-g-statistic",
    "href": "autocorrelation.html#getis-and-ord-g-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Getis and Ord G-Statistic",
    "text": "Getis and Ord G-Statistic\nA different method for identifying ‘hot’ or ‘cold spots’ of a variable is provided by the G-statistic. As Brunsdon and Comber observe, the statistic – which is a local statistic, calculated for each location in turn – is based on the proportion of the total sum of standardised attribute values that are within a threshold distance, \\(d\\), of each area centroid. Looking at the map below, which is of South African wards (simplified slightly from this source), we may suspect there are clusters of places with greater percentages of their populations having relatively high incomes and that these are spatial anomalies. Because the percentages are extremely skewed, they have been plotted with a square root transformation applied to the scale of the map (trans = \"sqrt\") – notice how the values 0 to 4% (which are the original values, not the square roots) occupy more of the legend than the values 4 to 8 do, and so forth.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nggplot(data = wards, aes(fill = High_income)) +\n  geom_sf(colour = \"transparent\") +\n  scale_fill_continuous_c4a_div(name = \"%\", palette = \"carto.yel_or\",\n                                trans = \"sqrt\", mid = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Percentage of Population with income R614401 or greater\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )\n\n\n\n\n\nThe G-statistic requires a binary spatial weights (style = \"B\", below), whereby locations either are or are not within the threshold distance of each other. The following has a threshold distance of 20km. Not all of the ward centroids will be within 20km of another which creates situations of zero neighbours, which is tolerated by setting zero.policy = TRUE in the conversion from a neighbours to spatial list.\n\nAs with the function knearneigh(), dnearneigh() has the default argument, longlat = NULL, which should not need changing here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. However, don’t take this for granted.\n\n\nCode\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nneighbours <- dnearneigh(coords, 0, 20)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\n\nHaving calculated the local G values, we can map them, using a manually generated fill palette. The argument na.translate = F removes the NA values from the legend but not from the map, wherein they are shaded white (na.value = \"white\"). The G values are also standardised z-values, hence values of 1.96 or greater are relatively rate if the assumption that the statistic is Normally distributed is warranted. (If it isn’t, consider using localG_perm.)\n\n\nCode\nbrks <- c(min(wards$localG, na.rm = TRUE), -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 20km threshold\"\n  )\n\n\n\n\n\nLet’s now add a refinement to the map and, based on what we learned from the previous session, label the cities that appear to contain a hot spot of higher percentages of higher earners. Note the use of st_join() which is a spatial join: it identifies, from geography, which ward the cities are located in and appends the ward data, including the G statistics, to the cities, retaining only those that are in the most significant hot spots. Note that this definition of ‘most significant’ doesn’t address the problem of repeat testing which we also saw with the local Moran statistics.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n  \nread_sf(\"hotosm_zaf_populated_places_points.shp\") |>\n  filter(place == \"city\") |>\n  st_join(wards) |>\n  filter(localG > 3.29) ->\n  cities\n\nlast_plot() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)\n\n\n\n\n\nThe results are dependent on the distance threshold defining which places are or are not neighbours. Here are the results with a distance threshold of 100km.\n\n\nCode\nneighbours <- dnearneigh(coords, 0, 100)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\nbrks <- c(min(wards$localG, na.rm = TRUE),\n          -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 100km threshold\"\n  ) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)"
  },
  {
    "objectID": "autocorrelation.html#summary",
    "href": "autocorrelation.html#summary",
    "title": "Measuring spatial autocorrelation",
    "section": "Summary",
    "text": "Summary\nThis session has been about identifying and quantifying spatial clustering in data, using a combination of ‘global’ (whole map) and local statistics; specifically, Moran’s I, its local equivalent, and the Getis-Ord G statistic. These statistics are dependent on the spatial weights matrix used in their calculation, which does, unfortunately, create something of a vicious circle: ideally that matrix would be calibrated to the spatial patterns evident in the data but those patterns are only measured and quantified once the weights matrix has been specified. For example, we could, in principle, use Moran’s I to measure the spatial autocorrelation and select the weights matrix accordingly. However, we need the spatial weights matrix to calculate Moran’s I. One option is to take the view that looking at the patterns at a range of scales is itself revealing so choose lots of bandwidths, not just one, and treat it as a multiscale analysis, as in the following.\n\n\nCode\n# Sets the threshold distance from 20 to 180km in intervals of 20\n# I chose 9 distances because they can be shown in a 3 x 3 grid of maps\nd <- seq(from = 20, by = 20, length.out = 9)\n\n# Calculate the G statistics for each distance and save them\n# as a list of sf features (maps)\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nmaps <- lapply(d, \\(x) {\n  neighbours <- dnearneigh(coords, 0, x)\n  spweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\n  wards$localG <- localG(wards$High_income, spweight)\n  wards$distance <- x\n  return(wards)\n})\n\n# Bind the maps together into a single object to be used\n# for the faceting in ggplot2\nmaps <- do.call(rbind, maps)\n\nbrks <- c(min(maps$localG, na.rm = TRUE),\n          -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(maps$localG, na.rm = TRUE))\nmaps$localG_gp <- cut(maps$localG, brks, include.lowest = TRUE)\n\n# This function is used with the labeller argument in the\n# facet_wrap function to customise the title of each sub-plot\nfacet_labels <- \\(x) paste0(x, \"km radius\")\n\nggplot(data = maps, aes(fill = localG_gp)) +\n  geom_sf(colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  facet_wrap(~ distance, labeller = labeller(distance = facet_labels)) +\n  theme_light() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank(),\n        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),\n        axis.text.x = element_blank(), axis.text.y = element_blank(),\n        legend.position = \"bottom\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "autocorrelation.html#further-reading",
    "href": "autocorrelation.html#further-reading",
    "title": "Measuring spatial autocorrelation",
    "section": "Further Reading",
    "text": "Further Reading\n\nChapter 2 of Spatial Regression Models for the Social Sciences by Guangqing Chi and Jun Zhu is recommended. For University of Bristol students, it is available to view as an eBook here.\n\nChapter 8 on Localised Spatial Analysis in An Introduction to R for Spatial Analysis & Mapping by Chris Brunsdon and Lex Comber."
  },
  {
    "objectID": "base.html#introduction",
    "href": "base.html#introduction",
    "title": "Base R",
    "section": "Introduction",
    "text": "Introduction\nBase R is what you download from CRAN. You might think of it as classic R. A short introduction of ‘the basics’ is provided below. For a fuller introduction, see the software manual, An Introduction to R. It is worth reading even if you end-up regularly using the Tidyverse variant of R described in the next session as there are some tasks that are (in my opinion) easier to do using Base R or by mixing it up a little."
  },
  {
    "objectID": "base.html#functions",
    "href": "base.html#functions",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\nR is a functional programming language where functions ‘do things’ to objects. What they do is dependent upon the class/type and attributes of the objects that go into the function, and also on the arguments of the function.\nFor example, try typing the following into the R Console, which is the bottom left panel of R Studio. Type it alongside the prompt symbol, > then hit Enter/Return.\n\n\nCode\nround(10.32, digits = 0)\n\n\n[1] 10\n\n\nThis calls the function round(), which is operating on the numeric object, 10.32. The argument digits specifies the number of digits to round to. It is set to zero in the example above.\nBecause digits = 0 is the default value for the function, we could just write\n\n\nCode\nround(10.32)\n\n\n[1] 10\n\n\nand obtain the same answer as before. I know that digits = 0 is the default value because, as I type the name of the function into the R Console, I see the arguments of the function and any default values appear.\n\nWe can also find out more about the function, including some examples of its use, by opening its help file.\n\n\nCode\n?round\n\n\n Should we wish to round 10.32 to one digit then we are no longer rounding to the default of zero decimal places and must therefore specify the argument explicitly (the default is no longer what we want).\n\n\nCode\nround(10.32, digits = 1)\n\n\n[1] 10.3\n\n\nThe following also works because it preserves the order of the arguments in the function.\n\n\nCode\nround(10.32, 1)\n\n\n[1] 10.3\n\n\nIn other words, if we do not specifically state that x = 10.32 (where x is a numeric vector; here, 10.32) and digits = 1 then they will be taken as the first and second arguments of the function. This requires care to make sure they genuinely are in the right order. If you aren’t certain, then define the arguments explicitly because they will then work out of order.\n\n\nCode\nround(digits = 1, x = 10.32)\n\n\n[1] 10.3\n\n\n In the examples above, both the input to and output from the function are a numeric vector of type double. The input is:\n\n\nCode\nclass(10.32)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(10.32)\n\n\n[1] \"double\"\n\n\nThe output is:\n\n\nCode\nclass(round(10.32, digits = 1))\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(round(10.32, digits = 1))\n\n\n[1] \"double\"\n\n\nNote how a function can be wrapped within a function, as in the example above: class(round(...)).\nAt the momment we are using x = 10.32, which is a numeric vector of length 1,\n\n\nCode\nlength(10.32)\n\n\n[1] 1\n\n\nHowever, the round() function can operate on numeric vectors of other lengths too.\n\n\nCode\nround(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7))\n\n\n[1] 1 2 3 4 6 7 8\n\n\nHere the combine function, c is used to create a vector of length 7, which is the input into round(). The output is of length 7 too.\n\n\nCode\nlength(round(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7)))\n\n\n[1] 7\n\n\n\nThere are lots of functions for R and I often forget what I need. Fortunately, there is a large user community too and so a quick web search often helps me quickly find what I need. Don’t be afraid to do a Google search for what you need.\n\nWriting a new function\nWe can write our own functions. The following will take a number and report whether it is a prime number or not.\n\n\nCode\nis.prime <- function(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}\n\n\nLet’s try it.\n\n\nCode\nis.prime(2)\n\n\n[1] TRUE\n\n\nCode\nis.prime(10)\n\n\n[1] FALSE\n\n\nCode\nis.prime(13)\n\n\n[1] TRUE\n\n\nCode\nis.prime(3.3)\n\n\n[1] NA\n\n\nThere is quite a lot to unpack about the function. It is not all immediately relevant but it is instructive to have an overview of what it is doing. First of all the function takes the form\nf <- function(x) {\n  ...\n}\nwhere x is the input into the function in much the same way that x is the number to be rounded in round(x = ...). It is a ‘place holder’ for the input into the function.\nStatements such as if(x == 2) are logical statements: if(...) is true then do whatever follows. Where what is to be done spans over multiple lines, they are enclosed by ‘curly brackets’, {...}.\nThe statement if(x < 2 | x %% floor(x) != 0) in the function is also a logical statement with the inclusion of an or statement, denoted by |. What it is checking is whether x < 2 or if x is a fraction. Had we needed to have both conditions to be met, then an and statement would be used, denoted by & instead of |. Note that ! means not, so != tests for not equal to and is the opposite of ==, which tests for equality.\nWhere it says, 2:(x-1), this is equivalent to the function, seq(from = 2, to = (x-1), by = 1). It generates a sequence of integer numbers from \\(2\\) to \\((x-1)\\).\n\n\nCode\nx <- 10\n2 : (x - 1)\n\n\n[1] 2 3 4 5 6 7 8 9\n\n\nCode\nseq(from = 2, to = (x-1), by = 1)\n\n\n[1] 2 3 4 5 6 7 8 9\n\n\nifelse() is another logical statement. It takes the form, ifelse(condition, a, b): if the condition is met then do a, else do b. In the prime number function it is checking whether dividing \\(x\\) by any of the numbers from \\(2\\) to \\(x-1\\) generates a whole number.\nFinally, the function return() returns an output from the function; specifically, a logical vector of length 1 that is TRUE, FALSE or NA dependent upon whether \\(x\\) is or is not a prime number, or if it is not a whole number above \\(1\\).\nNote that in newer versions of R, functions can also take the form,\nf <- \\(x) {\n  ...\n}\nTherefore the following is exactly equivalent to before.\n\n\nCode\nis.prime <- \\(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "base.html#objects-and-classes",
    "href": "base.html#objects-and-classes",
    "title": "Base R",
    "section": "Objects and Classes",
    "text": "Objects and Classes\nOur function that checks for a prime number is stored in the object is.prime.\n\n\nCode\nclass(is.prime)\n\n\n[1] \"function\"\n\n\nThere are other classes of object in R. Some of the most common are listed below.\n\nLogical\nThe output from the is.prime() function is an example of an object of class logical because the answer is TRUE or FALSE (or NA, not applicable).\n\n\nCode\nx <- is.prime(10)\nprint(x)\n\n\n[1] FALSE\n\n\nCode\nclass(x)\n\n\n[1] \"logical\"\n\n\nSome other examples:\n\n\nCode\ny <- 10 > 5\nprint(y)\n\n\n[1] TRUE\n\n\nCode\nclass(y)\n\n\n[1] \"logical\"\n\n\nCode\nz <- 2 == 5   # is 2 equal to 5?\nprint(z)\n\n\n[1] FALSE\n\n\n\n\nNumeric\nWe have already seen that some objects are numeric.\n\n\nCode\nx <- mean(0:100)\nprint(x)\n\n\n[1] 50\n\n\nCode\nclass(x)\n\n\n[1] \"numeric\"\n\n\nThis presently is of type double; i.e. it allows for decimal places even where they are not required.\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nbut it could be converted to class integer.\n\n\nCode\nx <- as.integer(x)\nclass(x)\n\n\n[1] \"integer\"\n\n\n\n\nCharacter\nOther classes include character. Note the difference between the length() of a character vector and the number of characters, nchar(), that any element of that vector contains.\n\n\nCode\nx <- \"Mapping and Modelling in R\"\nprint(x)\n\n\n[1] \"Mapping and Modelling in R\"\n\n\nCode\nlength(x)   # There is only one element in this vector\n\n\n[1] 1\n\n\nCode\nnchar(x)    # And that element contains 26 letters\n\n\n[1] 26\n\n\nCode\nclass(x)\n\n\n[1] \"character\"\n\n\nCode\ny <- paste(x, \"with Richard Harris\")\nprint(y)\n\n\n[1] \"Mapping and Modelling in R with Richard Harris\"\n\n\nCode\nlength(y)   # There is still only one element\n\n\n[1] 1\n\n\nCode\nnchar(y)    # But now it contains more letters\n\n\n[1] 46\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nCode\nz <- unlist(strsplit(x, \" \"))\nprint(z)\n\n\n[1] \"Mapping\"   \"and\"       \"Modelling\" \"in\"        \"R\"        \n\n\nCode\nlength(z)   # The initial vectors has been split into 5 parts\n\n\n[1] 5\n\n\nCode\nnchar(z)\n\n\n[1] 7 3 9 2 1\n\n\nCode\nclass(z)\n\n\n[1] \"character\"\n\n\n\nAs the name suggests, print is a function that prints its contents to screen. Often it can be omitted in favour of referencing the object directly. For instance, in the example above, rather than typing print(z) it would be sufficient just to type z. Just occasionally though you will find that an object does not print as you intended when the function is omitted. If this happens, try putting print back in.\n\n\nMatrix\nAn example of a matrix is\n\n\nCode\nx <- matrix(1:9, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nCode\nncol(x)   # Number of columns\n\n\n[1] 3\n\n\nCode\nnrow(x)   # Number of rows\n\n\n[1] 3\n\n\nCode\nclass(x)\n\n\n[1] \"matrix\" \"array\" \n\n\nHere the argument byrow is changed from its default value of FALSE to be TRUE:\n\n\nCode\ny <- matrix(1:9, ncol = 3, byrow = TRUE)\n\n\nThis result is equivalent to the transpose of the original matrix.\n\n\nCode\ny\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nCode\nt(x)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\nData frame\nA data.frame is a table of data, such as,\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nncol(df)    # Number of columns\n\n\n[1] 4\n\n\nCode\nnrow(df)    # Number of rows\n\n\n[1] 7\n\n\nCode\nlength(df)  # The length is also the number of columns\n\n\n[1] 4\n\n\nCode\nnames(df)   # The names of the variables in the data frame\n\n\n[1] \"Day\"   \"Date\"  \"Month\" \"Year\" \n\n\nNote that the length of each column should be equal in the specification of the data frame. The following will generate an error because the Date column is now too short. You might wonder why the Month and Year columns were fine previously when, in fact, they were give only one value, whereas there are 7 days and 7 dates. It is because R recycled them the requisite number of times (i.e. it gave all the rows the same value for Month and Year – it recycled June and 2022 seven times). That options isn’t available for the example below where there are 7 days but 6 dates.\n# This will generate an error\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:25,\n                 Month = \"June\",\n                 Year = 2022)\n\n\nFactors\nEarlier versions of R would, by default, convert character fields in a data frame into factors. The equivalent operation now is,\n\n\nCode\ndf2 <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022, stringsAsFactors = TRUE)\n\n\nTreating character fields as factors was clever but frustrating if you didn’t realise it was happening and wanted the characters to remains as characters. The difference is not immediately obvious,\n\n\nCode\nhead(df, n= 2)    # with stringsAsFactors = FALSE (the current default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nCode\nhead(df2, n = 2)  # with stringsAsFactors = TRUE  (the historic default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nbut begins to be apparent in the following:\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf2$Day\n\n\n[1] Mon   Tues  Wed   Thurs Fri   Sat   Sun  \nLevels: Fri Mon Sat Sun Thurs Tues Wed\n\n\nCode\ndf$Month\n\n\n[1] \"June\" \"June\" \"June\" \"June\" \"June\" \"June\" \"June\"\n\n\nCode\ndf2$Month\n\n\n[1] June June June June June June June\nLevels: June\n\n\nBasically, a factor is a categorical variable: it encodes which groups or categories (which levels) are to be found in the variable. Knowing this, it is possible to count the number of each group, as in,\n\n\nCode\nsummary(df2)\n\n\n    Day         Date       Month        Year     \n Fri  :1   Min.   :20.0   June:7   Min.   :2022  \n Mon  :1   1st Qu.:21.5            1st Qu.:2022  \n Sat  :1   Median :23.0            Median :2022  \n Sun  :1   Mean   :23.0            Mean   :2022  \n Thurs:1   3rd Qu.:24.5            3rd Qu.:2022  \n Tues :1   Max.   :26.0            Max.   :2022  \n Wed  :1                                         \n\n\nbut not\n\n\nCode\nsummary(df)\n\n\n     Day                 Date         Month                Year     \n Length:7           Min.   :20.0   Length:7           Min.   :2022  \n Class :character   1st Qu.:21.5   Class :character   1st Qu.:2022  \n Mode  :character   Median :23.0   Mode  :character   Median :2022  \n                    Mean   :23.0                      Mean   :2022  \n                    3rd Qu.:24.5                      3rd Qu.:2022  \n                    Max.   :26.0                      Max.   :2022  \n\n\nFactors can be useful but do not always behave as you might anticipate. For example,\n\n\nCode\nx <- c(\"2021\", \"2022\")\nas.numeric(x)\n\n\n[1] 2021 2022\n\n\nis different from,\n\n\nCode\nx <- factor(c(\"2021\", \"2022\"))\nas.numeric(x)\n\n\n[1] 1 2\n\n\nThese days the defult is stringsAsFactors = FALSE, which is better when using functions such as read.csv() to read a .csv file into a data.frame in R.\n\n\nLists\nA list is a more flexible class that can hold together other types of object. Without a list, the following only works because the 1:3 are coerced from numbers in x to characters in y – note the \" \" that appear around them, which shows they are now text.\n\n\nCode\nx <- as.integer(1:3)\nclass(x)\n\n\n[1] \"integer\"\n\n\nCode\ny <- c(\"a\", x)\ny\n\n\n[1] \"a\" \"1\" \"2\" \"3\"\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nOn the other hand,\n\n\nCode\ny <- list(\"a\", x)\n\n\ncreates a ragged list of two parts:\n\n\nCode\nclass(y)\n\n\n[1] \"list\"\n\n\nCode\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n\nThe first part has the character \"a\" in it.\n\n\nCode\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\nclass(y[[1]])\n\n\n[1] \"character\"\n\n\nThe second has the numbers 1 to 3 in it.\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\nclass(y[[2]])\n\n\n[1] \"integer\"\n\n\nNote that the length of the list is the length of its parts. Presently it is 2 but the following example has a length of three.\n\n\nCode\ny <- list(\"a\", x, df)\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nlength(y)\n\n\n[1] 3\n\n\nThis should not be confused with the length of any one part.\n\n\nCode\nlength(y[[1]])\n\n\n[1] 1\n\n\nCode\nlength(y[[2]])\n\n\n[1] 3\n\n\nCode\nlength(y[[3]])\n\n\n[1] 4"
  },
  {
    "objectID": "base.html#assignments",
    "href": "base.html#assignments",
    "title": "Base R",
    "section": "Assignments",
    "text": "Assignments\nThroughout this document I have used the assignment term <- to store the output of a function, as in x <- as.integer(1:3) and y <- list(\"a\", x, df), and so forth. The <- is used to assign the result of a function to an object. You can, if you prefer use =. For example, all the following make the same assignment, which is to give x the value of 1.\n\n\nCode\nx <- 1\nx = 1\n1 -> x\n\n\nPersonally, I avoid using = as an assignment so, (1) not to confuse assignments with arguments,\n\n\nCode\nx <- round(10.32, digits = 1)   # I think this is a bit clearer\nx = round(10.32, digits = 1)    # and this a bit less so\n\n\n\nto not confuse assignments with logical statements,\n\n\n\nCode\nx <- 1\ny <- 2\nz <- x == y   # Again, this is a bit clearer\nz = x == y    # and this not so much\n\n\nand, (3) – but this is pedantic – to avoid the following sort of situation which makes no sense mathematically…\n\n\nCode\nx = 1\ny = 2\nx = y\n\n\n… but does in terms of what it really means:\n\n\nCode\nx <- 1\ny <- 2\nx <- y # Assign the value of y to x, overwriting its previous value\n\n\nWhich you use is a matter of personal preference and, of course, = has one less character than <- to worry about. However, this course is written with,\n<- (or ->) is as assignment, as in x <- 1;\n= is the value of an argument, as in round(x, digits = 1); and\n== is a logical test for equality, as in x == y.\n\nIt is important to remember that R is case sensitive. An object called x is different from one called X; y is not the same as Y and so forth."
  },
  {
    "objectID": "base.html#manipulating-objects",
    "href": "base.html#manipulating-objects",
    "title": "Base R",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nIn addition to passing objects to functions such as…\n\n\nCode\nx <- 0:100\nmean(x)\n\n\n[1] 50\n\n\nCode\nsum(x)\n\n\n[1] 5050\n\n\nCode\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      25      50      50      75     100 \n\n\nCode\nmedian(x)\n\n\n[1] 50\n\n\nCode\nquantile(x, probs = c(0, 0.25, 0.5, 0.75, 1))\n\n\n  0%  25%  50%  75% 100% \n   0   25   50   75  100 \n\n\nCode\nhead(sqrt(x)) # The square roots of the first of x\n\n\n[1] 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nCode\ntail(x^2)     # The square roots of the last of x\n\n\n[1]  9025  9216  9409  9604  9801 10000\n\n\nCode\nsd(x)         # The standard deviation of x\n\n\n[1] 29.30017\n\n\n…there are other ways we may wish to interact with objects.\n\nMathematical operations\nMathematical operations generally operate on a pairwise basis between corresponding elements in a vector. For example,\n\n\nCode\nx <- 1\ny <- 3\nx + y\n\n\n[1] 4\n\n\nCode\nx <- 1:5\ny <- 6:10\nx + y\n\n\n[1]  7  9 11 13 15\n\n\nCode\nx * y   # Multiplication\n\n\n[1]  6 14 24 36 50\n\n\nCode\nx / y   # Divisions\n\n\n[1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000\n\n\nIf one vector is shorter that the other, values will be recycled. In the following example the results are \\(1\\times6\\), \\(2\\times7\\), \\(3\\times8\\), \\(4\\times9\\) and then \\(5\\times6\\) as y is recycled.\n\n\nCode\nx <- 1:5  # This is a vector of length 5\ny <- 6:9  # This is a vector of length 4\nx * y     # A vector of length 5 but some of y is recycled\n\n\n[1]  6 14 24 36 30\n\n\n\n\nSubsets of objects\n\nVectors\nIf x is a vector then x[n] is the nth element in the vector (the nth position, the nth item). To illustrate,\n\n\nCode\nx <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nx[1]\n\n\n[1] \"a\"\n\n\nCode\nx[3]\n\n\n[1] \"c\"\n\n\nCode\nx[c(1, 3, 5)]\n\n\n[1] \"a\" \"c\" \"e\"\n\n\nCode\nx[length(x)]\n\n\n[1] \"f\"\n\n\nThe notation -n can be used to exclude elements.\n\n\nCode\nx[-3]   # All of x except the 3rd element\n\n\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n\nCode\nx[c(-1, -3, -5)]    # x without the 1st, 3rd and 5th elements\n\n\n[1] \"b\" \"d\" \"f\"\n\n\n\n\nMatrices\nIf x is a matrix then x[i, j] is the value of the ith row of the jth column:\n\n\nCode\nx <- matrix(1:10, ncol = 2)\nx\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[1, 1]     # row 1, column 1\n\n\n[1] 1\n\n\nCode\nx[2, 1]     # row 2, column 1\n\n\n[1] 2\n\n\nCode\nx[c(3, 5), 2]   # rows 3 and 5 of column 2\n\n\n[1]  8 10\n\n\nCode\nx[nrow(x), ncol(x)]   # the final entry in the matrix\n\n\n[1] 10\n\n\nAll of the values in the ith row can be selected using the form x[i, ]\n\n\nCode\nx[1, ]    # row 1\n\n\n[1] 1 6\n\n\nCode\nx[3, ]    # row 3\n\n\n[1] 3 8\n\n\nCode\nx[c(1, 5), ]  # rows 1 and 5\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    5   10\n\n\nCode\nx[c(-1, -3), ]  # All except the 1st and 3rd rows\n\n\n     [,1] [,2]\n[1,]    2    7\n[2,]    4    9\n[3,]    5   10\n\n\nSimilarly, all of the values in the jth column can be selected using the form x[, j]\n\n\nCode\nx[ ,1]    # column 1\n\n\n[1] 1 2 3 4 5\n\n\nCode\nx[ ,2]    # column 2\n\n\n[1]  6  7  8  9 10\n\n\nCode\nx[ , 1:2]   # columns 1 and 2\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[-3 , 1:2]   # columns 1 and 2 except row 3\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    4    9\n[4,]    5   10\n\n\n\n\nData frames\nData frames are not unlike a matrix.\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf[, 1]   # The first column\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, 1]  # The first row of the first column (Day)\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, 2]  # The second row of the second column (Date)\n\n\n[1] 21\n\n\nHowever, you can also use reference the variable name directly, through the x$variable style notation,\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ndf$Date[2]\n\n\n[1] 21\n\n\nAlternatively, if you wish, with the square brackets, using the [, \"variable\"] format.\n\n\nCode\ndf[, \"Day\"]\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, \"Day\"]\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, \"Date\"]\n\n\n[1] 21\n\n\n\n\nLists\nWe have already seen the use of double square brackets, [[...]] to refer to a part of a list:\n\n\nCode\nx <- 1:3\ny <- list(\"a\", x, df)\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\ny[[3]]\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nThe extension to this is to be able to refer to a specific element within a part of the list by combining it with the other notation. Some examples are:\n\n\nCode\ny[[1]][1]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]][3]\n\n\n[1] 3\n\n\nCode\ny[[3]]$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ny[[3]]$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ny[[3]][2, \"Date\"]\n\n\n[1] 21\n\n\n\nThe way to remember the difference between [[...]] and [...] is that the double square brackets reference a specific part of a list, for example [[3]], the third part; the single square brackets reference a position or element in a vector, such as [4], the fourth. Combining them, [[3]][4] is the 4th element of a vector where that vector forms the 3rd part of a list."
  },
  {
    "objectID": "base.html#deleting-objects-and-saving-the-workspace",
    "href": "base.html#deleting-objects-and-saving-the-workspace",
    "title": "Base R",
    "section": "Deleting objects and saving the workspace",
    "text": "Deleting objects and saving the workspace\nMy current working directory is,\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/profr/Dropbox/github/MandM\"\n\n\nand it contains the following objects:\n\n\nCode\nls()\n\n\n[1] \"df\"       \"df2\"      \"is.prime\" \"x\"        \"y\"        \"z\"       \n\n\nYours will be different. Remember, it can be useful to create a new project for a new collection of work that you are doing in R and then opening that project each time you start R will ensure that the working directory is that of the project.\nTo delete a specific object, use rm(),\n\n\nCode\nrm(z)\n\n\nOr, more than one,\n\n\nCode\nrm(df, df2, is.prime)\n\n\nTo save the workspace and all the objects it now contains use the save.image() function.\n\n\nCode\nsave.image(\"workspace1.RData\")\n\n\nTo delete all the objects created in your workspace, use\n\n\nCode\nrm(list=ls())\n\n\n\nIt is a good idea to save a workspace with a new filename before deleting too much from your workspace to allow you to recover it if necessary. Be especially careful if you use rm(list=ls()) as there is no undo function. The best you can do is load the workspace as it was the last time that you saved it.\nTo (re)load a workspace, use load().\n\n\nCode\nload(\"workspace1.RData\")"
  },
  {
    "objectID": "base.html#further-reading",
    "href": "base.html#further-reading",
    "title": "Base R",
    "section": "Further reading",
    "text": "Further reading\nThis short introduction to base R has really only scratched the surface. There are many books about R that provide a lot more detail but, to remind you, the manual that comes with the software is worth reading and probably the best place to start – An Introduction to R. It is thorough but also relatively short.\nDon’t worry if not everything makes sense at this stage. The best way to learn R is to put it into practice and that is what we shall be doing in later sessions."
  },
  {
    "objectID": "exercise1.html",
    "href": "exercise1.html",
    "title": "Follow-up exercise",
    "section": "",
    "text": "For this short follow-up exercise, create a simple R markdown file (I suggest in HTML output format) that, when knitted, includes the code and output for the following:\n\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/diversity2.csv. The file is in .csv format. It contains headers (i.e. the top row of the data is the variable names).\nTo draw a scatterplot using the following variables in the data: E.01 as the x-axis and E.21 as the y-axis. In the plot, name the x-axis, “Diversity (2001)” and the y-axis “Diversity (2021)”. For your information, E.01 and E.21 measure the ethnic diversity of various places in England and Wales in 2001 and 2021, respectively. The measure ranges from 0 (no diversity) to 1 (‘maximum diversity’).\nTo add a reference line to the plot, with a gradient of 1 and a y-intercept of 0, showing where E.01 = E.21 (no change in diversity),\nTo add a rug plot to the plot on both axes.\n\nRun through the stages above twice in your markdown file, once using base R and the base R graphics, the other using tidyverse and ggplot. Then add a very brief textual comment beneath your code and output interpreting the graphs in terms of whether places are becoming more ethnically diverse or not. This only needs to be a sentence or two of text, nothing longer."
  },
  {
    "objectID": "exercise1.html#to-help",
    "href": "exercise1.html#to-help",
    "title": "Follow-up exercise",
    "section": "To help",
    "text": "To help\nMost of the code you need to complete this task is included in the ‘Flavours of R’ section of this course. However, you may also find it useful to look at the help files for ?read.csv, ?read_csv, ?plot,?abline and ?rug, the example ggplot2 code available here, here and here, and perhaps also the ggplot2 cheatsheet. You might also look at this introduction to R Markdown but it covers more about R Markdown than you need at this stage. Simply creating a new document using File -> New File -> R Markdown… should be sufficient to get you started."
  },
  {
    "objectID": "exercise2.html",
    "href": "exercise2.html",
    "title": "Follow-up exercise",
    "section": "",
    "text": "For this follow-up exercise, create an R markdown file that, when knitted, includes the code and output for the following, ideally as a pdf document:\n\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/Local_Authority.geojson. The file is in .geojson format. The data are sourced from here and give Index of Multiple Deprivation (IMD) data for English local authorities.\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/gb.csv. This gives the names, population size and a centroid location for 2,680 prominent cities in United Kingdom. The file is in .csv format (with a header). Read it into an object called cities. The data were sourced from here.\nTo run the following code, which will convert the cities into spatial points:\n\n\n\nCode\ncities <- st_as_sf(cities, coords = c(\"lng\", \"lat\"), crs = 4326)\n\n\n\nTo run the following code, which will ‘clip out’ the cities that are not in England:\n\n\n\nCode\ncities <- st_filter(cities, imd)\n\n\n\nTo draw a map of the variable RAvgScor, which is the rank of the average 2019 multiple deprivation score for the authorities. Aim for the map to be of a publishable quality – suitable for a journal publication. Include cities above a chosen population threshold on your map to provide some geographical context for the reader.\n\nThen add a very brief textual comment beneath your code and map(s) to provide a brief description of the geography of deprivation shown – do you see any geographical pattern(s)?"
  },
  {
    "objectID": "exercise2.html#to-help",
    "href": "exercise2.html#to-help",
    "title": "Follow-up exercise",
    "section": "To help",
    "text": "To help\nAll that you need is in the session ‘Thematic maps in R’. However, you do need to decide whether to produce your map in ggplot2 or tmap. You could always impress me and use both!"
  },
  {
    "objectID": "exercise3.html",
    "href": "exercise3.html",
    "title": "Follow-up exercise",
    "section": "",
    "text": "For this follow-up exercise, create an R markdown file that, when knitted, includes the code and output for the following, ideally as a pdf document:\n\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/Local_Authority.geojson. The file is in .geojson format. The data are sourced from here and give Index of Multiple Deprivation (IMD) data for English local authorities.\nTo create a neighbourhood list for the local authorities and then spatial weights. How you define neighbours is up to you.\nTo use the spatial weights to map clusters of high or low values of the variable RAvgScor. This is the rank of the average 2019 multiple deprivation score for each authority. (It isn’t therefore, really a continuous variable1 but we will treat it as though it is).\n\nConclude with a brief reflection on what, if anything, the map reveals about a possible geography of deprivation in England."
  },
  {
    "objectID": "exercise3.html#to-help",
    "href": "exercise3.html#to-help",
    "title": "Follow-up exercise",
    "section": "To help",
    "text": "To help\nAll that you need is in the session ‘Measuring spatial autocorrelation’. Keep in mind that a rank with a low numeric value is high for deprivation (because 1 = most) so, rather confusingly perhaps, a cluster of high values is a cluster of low deprivation, and vice versa. Keep in mind, also, that some local authorities may not have any neighbours, depending upon how you define them, so you may need to change the zero.policy in some functions."
  },
  {
    "objectID": "gwstats.html",
    "href": "gwstats.html",
    "title": "Geographically Weighted Statistics",
    "section": "",
    "text": "In the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by ‘pooling’ the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in GWmodel – an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. These are geographically weighted statistics."
  },
  {
    "objectID": "gwstats.html#geographical-weighted-statistics",
    "href": "gwstats.html#geographical-weighted-statistics",
    "title": "Geographically Weighted Statistics",
    "section": "Geographical Weighted Statistics",
    "text": "Geographical Weighted Statistics\nThe idea behind geographically weighted statistics is simple. Instead of calculating the (global) mean average, for example, for the all the data and the whole map, a series of (local) averages are calculated for various sub-spaces within the map.\nImagine a point location, \\(i\\), at position \\((u_i, v_i)\\) on the map. To calculate the local statistic, first find either the \\(k\\) nearest neighbours to \\(i\\) or all of those within a fixed distance, \\(d\\), from it. Second, to add the geographical weighting in the name of the statistics, apply a weighting scheme whereby the neighbours nearest to \\(i\\) have most weight in the subsequent calculations and the weights decrease with increasing distance from \\(i\\), becoming zero at the \\(k\\)th nearest neighbour or at the distance threshold, \\(d\\). Third, calculate, for the point and its neighbours, the weighted mean value (or some other summary statistic) of a variable, using the inverse distance weighting in the calculation. Fourth, repeat the process for other points on the map. This means that if there are \\(n\\) points of calculation then there will be \\(n\\) geographically weighted mean values calculated across the map. These can be then be compared to look for spatial variation.\nLet’s see this in action, beginning by ensuring the necessary packages are installed and required. As with previous sessions, you might start by opening the R Project that you created for these classes.\n\n\nCode\ninstalled <- installed.packages()[,1]\nrequired <- c(\"colorspace\", \"cols4all\", \"GWmodel\", \"proxy\", \"sf\", \"sp\",\n              \"tidyverse\", \"tmap\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE,\n                                     repos = \"https://cloud.r-project.org\")\n\nrequire(cols4all)\nrequire(GWmodel)\nrequire(sf)\nrequire(tidyverse)\nrequire(tmap)\n\n\nWe will use the same data as in the previous session but confine the analysis to the Western Cape of South Africa. This is largely to reduce run times but there is another reason that I shall return to presently.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nwards |>\n  filter(ProvinceNa == \"Western Cape\") ->\n  wcape_wards\n\n\nThe calculated points for the geographically weighted statistics will be the ward centroids. A slight complication here is that GWmodel presently is built around the elder sp not sf formats for handling spatial data in R so the wards need to be converted from the one format to the other.\n\n\nCode\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\n\nWe can see that wcape_wards is of class sf,\n\n\nCode\nclass(wcape_wards)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nwhereas wcape_wards_sp is now of class SpatialPolygonsDataFrame, which is what we want.\n\n\nCode\nclass(wcape_wards_sp)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nHaving made the conversion we are ready to calculate some geographically weighted statistics, doing so for the High_income variable – the percentage of the population with income R614401 or greater in 2011 – in the example below.\n\n\nCode\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\n\n\nMost warning messages are intended to be helpful and the one generated from the code above is not an exception. It is related to a change in how coordinate reference systems (CRS) are represented but does not affect the results of the analyses here.\nThe results are contained in the spatial data frame ($SDF),\n\n\nCode\nhead(gwstats$SDF)\n\n\n  High_income_LM High_income_LSD High_income_LVar High_income_LSKe\n1       5.887743      2.92991675        8.5844122       -0.2838245\n2       3.280802      2.53680091        6.4353588        0.1765590\n3       1.109567      1.61167650        2.5975011        1.7142999\n4       2.377564      2.80047422        7.8426558        1.3837936\n5       5.781072      2.59128151        6.7147399        0.3118916\n6       0.101829      0.05610169        0.0031474        0.1862646\n  High_income_LCV\n1       0.4976298\n2       0.7732258\n3       1.4525268\n4       1.1778756\n5       0.4482354\n6       0.5509400\n\n\nAs well as the local means, the local standard deviations, variances, skews and coefficients of variation are included (the coefficient of variation is the ratio of the standard deviation to the mean). The local medians, interquartile ranges and quantile imbalances could also be added by including the argument quantile = TRUE in gwss() – see ?gwss.\nThe results are dependent on the data (of course) but also\n\nthe kernel (i.e. the shape of the weighting – the distance decay – around each point), and\nthe bandwidth (i.e. the maximum number of neighbours to include, \\(k\\) or the distance threshold, \\(d\\)).\n\nThe kernel matters…\n\nSource: GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models\n… but it (usually) matters much less than the bandwidth, which controls the amount of spatial smoothing: the larger it is, the more neighbours are being averaged over. The trade-off is between bias and precision. A smaller bandwidth is less likely to average-out geographical detail in the data and should create a geographically weighted average, for example, that is representative of the location at the centre of the kernel but it is also dependent on a small number of observations, some or more of which could be in error or unsual outliers for the vicinity.\nThe following maps compare a bandwidth of \\(bw = 10\\) nearest neighbours to \\(bw = 100\\). If you zoom into the areas around the north of Cape Town you will see some of the differences. Note that the argument adaptive = TRUE sets the bandwidth to be in terms of nearest neighbours, else it would indicate a fixed distance of 10 metres. The advantage of using nearest neighbours is it allows for varying population densities. Otherwise, using a fixed distance, more rural areas will tend to have fewer neighbours than urban ones because those rural areas are larger and more spaced apart.\n\n\nCode\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw100 <- gwstats$SDF$High_income_LM\n\nwards2 <- pivot_longer(wcape_wards, cols = starts_with(\"bw\"), names_to = \"bw\",\n             values_to = \"GWmean\")\n\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(wards2, names = \"wards\") +\n  tm_fill(\"GWmean\", palette = \"Reds\", title = \"%\",\n          alpha = 0.7,\n          id = \"District_1\",\n          popup.vars = c(\"GW mean:\" = \"GWmean\",\n                         \"Ward ID:\" = \"WardID\"),\n          popup.format = list(digits = 1)) +\n  tm_borders() +\n  tm_facets(by = \"bw\") +\n  tm_legend(title =\n        \"Geographically weighted % population with income R614401 or greater\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA nice feature of the facetting (tm_facets) in tmap is it has created two dynamically linked maps.\n\nA slight tangent\nYou may note the use of the pivot_longer function from the tidyverse packages in the code above. To see what this does, take a look at,\n\n\nCode\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  arrange(WardID) |>\n  head(n = 3)\n\n\n    WardID      bw10     bw100\n1 10101001 0.3868500 0.4349990\n2 10101002 0.4108465 0.4435761\n3 10101003 0.3742776 0.4254601\n\n\nNow compare it with,\n\n\nCode\nwcape_wards |>\n  st_drop_geometry() |>\n  select(WardID, bw10, bw100) |>\n  pivot_longer(cols = starts_with(\"bw\"), names_to = \"bw\",\n          values_to = \"GWmean\") |>\n  arrange(WardID) |>\n  head(n = 6)\n\n\n# A tibble: 6 × 3\n  WardID   bw    GWmean\n  <chr>    <chr>  <dbl>\n1 10101001 bw10   0.387\n2 10101001 bw100  0.435\n3 10101002 bw10   0.411\n4 10101002 bw100  0.444\n5 10101003 bw10   0.374\n6 10101003 bw100  0.425\n\n\nWhat you can see is that the two columns, bw10 and bw100 from the first table have been stacked into rows in the second. Doing this allows us to create the two linked plots by faceting on the bandwidth variable, bw, using tm_facets(by = \"bw\"). The reverse operation to pivot_longer() is pivot_wider() – see ?pivot_wider.\n\n\n‘Pre-calculating’ the distance matrix\nIn the calculations above, the distances between the ward centroids that are used in the geographical weighting are calculated twice. First in gwss(wcape_wards_sp, vars = \"High_income\", bw = 10, ...) and then again in gwss(wcape_wards_sp, vars = \"High_income\", bw = 100, ...). Since those distances don’t actually change (the centroids are fixed so therefore are the distances between them) so we might have saved a little computational time by calculating the distance matrix in advance and then using it in the geographically weighted statistics. Curiously, though, it actually takes longer. I assume this is because the data set isn’t large enough to justify the extra step of saving the distances in a matrix and then passing that matrix to the gwss() function.\n\n\nCode\n# Time to do the calculations without pre-calculating the distance matrix\nsystem.time({\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n   0.14    0.00    0.14 \n\n\nCode\n# Time to do the calculations with the pre-calculated distance matrix\nsystem.time({\n  coords <- st_coordinates(st_centroid(wcape_wards))\n  dmatrix <- gw.dist(coords, longlat = T)\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n   1.14    0.00    1.12"
  },
  {
    "objectID": "gwstats.html#selecting-the-bandwidth",
    "href": "gwstats.html#selecting-the-bandwidth",
    "title": "Geographically Weighted Statistics",
    "section": "Selecting the bandwidth",
    "text": "Selecting the bandwidth\nAs observed in the maps above, the geographically weighted statistics are a function of the geographical weighting that largely is controlled by the bandwidth. This raises the question of which is the correct bandwidth to use? Unfortunately, the most honest answer is that there is no correct answer, although an automatic bandwidth selection might be tried by calibrating the statistics around the local means.\nThe following uses a cross-validation approach,\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\n# The selected number of nearest neighbours:\nbw\n\n\n[1] 17\n\n\nwhereas the following uses an AIC corrected approach.\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\")\n\n\nAdaptive bandwidth (number of nearest neighbours): 256 AICc value: 1480.449 \nAdaptive bandwidth (number of nearest neighbours): 166 AICc value: 1473.901 \nAdaptive bandwidth (number of nearest neighbours): 110 AICc value: 1442.05 \nAdaptive bandwidth (number of nearest neighbours): 75 AICc value: 1398.784 \nAdaptive bandwidth (number of nearest neighbours): 54 AICc value: 1370.417 \nAdaptive bandwidth (number of nearest neighbours): 40 AICc value: 1345.457 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 1335.979 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1320.554 \nAdaptive bandwidth (number of nearest neighbours): 23 AICc value: 1314.985 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 1306.969 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \n\n\nCode\nbw\n\n\n[1] 20\n\n\nBoth use a golden-section search method and, if you look at the output, both iterate to a very similar solution in a very similar set of steps. Bandwidths of 17 and 20 have been suggested but, in practice, there is probably little between them so we may as well use the larger.\nThat automatic bandwidth selection applies only for the Western Cape wards, however. Recall earlier that the data were filtered (wards |> filter(ProvinceNa == \"Western Cape\") -> wcape_wards) with the partial explanation for doing so being to reduce run times. Another explanation is that there is no reason to assume that the spatial autocorrelation that is quantified by the bandwidth selection will be the same everywhere across the map. In fact, it varies from province to province:\n\n\nCode\nbandwidths <- sapply(unique(wards$ProvinceNa), \\(x) {\n  wards |>\n    filter(ProvinceNa == x) |>\n    as_Spatial() %>%\n    bw.gwr(High_income ~ 1, data = .,\n          adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n          approach = \"AIC\") %>%\n    paste0(\"Bandwidth = \", .)\n})\n\n\n\n\nCode\nbandwidths\n\n\n       Free State           Limpopo        Mpumalanga      Western Cape \n\"Bandwidth = 126\"  \"Bandwidth = 25\"  \"Bandwidth = 95\"  \"Bandwidth = 20\" \n    KwaZulu-Natal      Eastern Cape        North West           Gauteng \n \"Bandwidth = 21\"  \"Bandwidth = 24\"  \"Bandwidth = 19\"  \"Bandwidth = 19\" \n    Northern Cape \n\"Bandwidth = 201\" \n\n\nThis suggests that fitting geographically weighted statistics to too large a study region is not desirable because there is little reason to presume that the same bandwidth should apply throughout it."
  },
  {
    "objectID": "gwstats.html#changing-the-interpolation-calculation-points",
    "href": "gwstats.html#changing-the-interpolation-calculation-points",
    "title": "Geographically Weighted Statistics",
    "section": "Changing the interpolation (calculation) points",
    "text": "Changing the interpolation (calculation) points\nSo far we have been using the ward centroids as the points for which the geographically weighted statistics are calculated. There is no requirement to do so as they could be interpolated at any point within this study region. To demonstrate this, let’s reselect the wards in the Western Cape and convert them into a raster grid using the stars package. Stars is an abbreviation of spatial-temporal arrays and can be used for handling raster (gridded) data, as in the following example, which is taken from this introduction to the package.\n\n\nCode\nif(!(\"stars\" %in% installed)) install.packages(\"stars\", dependencies = TRUE)\nrequire(stars)\nsat_image <- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nsat_image <- read_stars(sat_image)\nplot(sat_image, axes = TRUE)\n\n\n\n\n\nWe will use the st_rasterize function in stars to convert the geography of South Africa’s Western Cape into a regular grid.\n\n\nCode\nwcape_wards |>\n  st_rasterize(nx = 100, ny = 100) |>\n  st_as_sf() ->\n  gridded\n\npar(mai=c(0,0,0,0))\ngridded |>\n  st_geometry() |>\n  plot()\n\n\n\n\n\nWe can now calculate the geographically weighted mean for each raster cell, with a bandwidth of 20 nearest neighbours.\n\n\nCode\ngridded_sp <- as_Spatial(gridded)\n\ngwstats <- gwss(wcape_wards_sp, gridded_sp, vars = \"High_income\",\n                bw = 20, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\ngridded$GWmean <- gwstats$SDF$High_income_LM\n\nggplot(gridded, aes(fill = GWmean)) +\n  geom_sf(col = \"light grey\", size = 0) + \n                      # size is the width of the raster cell border\n  scale_fill_continuous_c4a_seq(palette = \"scico.lajolla\") +\n  theme_minimal()\n\n\n\n\n\n\nUsing geography to interpolate missing values\nThis ability to interpolate at any point within the study region provides a means to deal with missing values in the data. Contained in the wcape_wards data is a variable giving the average age of the population in each ward in 2011 but it contains 15 missing values:\n\n\nCode\nsummary(wcape_wards$age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.90   27.95   29.80   30.91   33.00   51.30      15 \n\n\nThe same observations also record NAs for the No_schooling variable.\n\n\nCode\nwhich(is.na(wcape_wards$age))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nCode\nwhich(is.na(wcape_wards$No_schooling))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nMissing value are a problem when fitting a regression model, for example. Usually they are simply omitted, as in the following case, where the output reports “(15 observations deleted due to missingness)”.\n\n\nCode\nols1 <- lm(No_schooling ~ age, data = wcape_wards)\nsummary(ols1)\n\n\n\nCall:\nlm(formula = No_schooling ~ age, data = wcape_wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1998 -0.5384  1.7687 13.2843 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 19.69367    1.14121   17.26   <2e-16 ***\nage         -0.38835    0.03657  -10.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.064 on 385 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.2265,    Adjusted R-squared:  0.2245 \nF-statistic: 112.8 on 1 and 385 DF,  p-value: < 2.2e-16\n\n\n An alternative approach is to replace the missing values with a ‘safe’ alternative such as the mean age for the values that are not missing and the same for the percentages of the populations without schooling. However, we could also replace each missing value with a locally interpolated mean which fits with the geographical context.\nHere are the wards with missing age and no schooling values. These are the points that need to be interpolated.\n\n\nCode\nwcape_wards |>\n  filter(is.na(age)) |>\n  as_Spatial() ->\n  missing\n\n\nThese are the wards with the values that serve as the data points.\n\n\nCode\nwcape_wards |>\n  filter(!is.na(age)) |>\n  as_Spatial() ->\n  present\n\n\nFortunately, the missing values seem to be fairly randomly distributed across the study region. It would be a problem if they were all geographically clustered together because interpolating their values from their neighbours would not be successful if their neighbours’ values were also missing!\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nplot(present, border = \"light grey\")\nplot(missing, col = \"red\", add = T)\n\n\n\n\n\nWe can, then, interpolate the missing values from the present ones and match them into the data using base R’s match() function, matching on their WardID. Note that I have done this twice, once for the age variable and once for No_schooling. This is to allow for the possibility of them having differently sized bandwidths from each other (which they are when using approach = \"AIC\", although not, as it happens, with the default approach = \"CV\").\n\n\nCode\nbw <- bw.gwr(age ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n             approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"age\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch <- match(missing$WardID, wcape_wards$WardID)\nwcape_wards$age[mch] <- gwstats$SDF$age_LM\n\nbw <- bw.gwr(No_schooling ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\ngwstats <- gwss(present, missing, vars = \"No_schooling\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = T)\n\nwcape_wards$No_schooling[mch] <- gwstats$SDF$No_schooling_LM\n\n\nIt is useful to keep a note of which values are interpolated, so…\n\n\nCode\nwcape_wards$interpolated <- FALSE\nwcape_wards$interpolated[mch] <- TRUE\n\n\nThere should be 15 of them.\n\n\nCode\ntable(wcape_wards$interpolated)\n\n\n\nFALSE  TRUE \n  387    15 \n\n\nNow returning to our regression model, there are, of course, no longer any missing values and, reassuringly, no evidence that the interpolated values are significantly different from the rest in either their mean No_schooling value or in their effect of age upon No_schooling.\n\n\nCode\nols2 <- update(ols1, . ~ . + interpolated*age)\nsummary(ols2)\n\n\n\nCall:\nlm(formula = No_schooling ~ age + interpolated + age:interpolated, \n    data = wcape_wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1709 -0.5446  1.7484 13.2843 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          19.69367    1.12855  17.450   <2e-16 ***\nage                  -0.38835    0.03617 -10.738   <2e-16 ***\ninterpolatedTRUE     -1.72125   10.95102  -0.157    0.875    \nage:interpolatedTRUE  0.02817    0.34961   0.081    0.936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.03 on 398 degrees of freedom\nMultiple R-squared:  0.2285,    Adjusted R-squared:  0.2226 \nF-statistic: 39.28 on 3 and 398 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "gwstats.html#the-geographically-weighted-mean-as-a-low-pass-filer",
    "href": "gwstats.html#the-geographically-weighted-mean-as-a-low-pass-filer",
    "title": "Geographically Weighted Statistics",
    "section": "The geographically weighted mean as a low pass filer",
    "text": "The geographically weighted mean as a low pass filer\nThe geographically weighted mean acts as a smoothing (low pass) filter. We can see this if we apply it to a part of the satellite image from earlier. (If you try it on the whole image, you will be waiting a long time for the bandwidth to be calculated).\nFirst we shall extract band 1 from the image, create a blank raster using the raster package and assign it the same values as from the image.\n\n\nCode\nsat_image |>\n  slice(band, 1) |>\n  pull() -> vals\n\nif(!(\"raster\" %in% installed)) install.packages(\"raster\", dependencies = TRUE)\n\ndim(sat_image)\n\n\n   x    y band \n 349  352    6 \n\n\nCode\nst_bbox(sat_image)\n\n\n     xmin      ymin      xmax      ymax \n 288776.3 9110728.8  298722.8 9120760.8 \n\n\nCode\nr <- raster::raster(nrows = 352, ncol = 349,\n                      xmn = 288776.3, xmx = 298722.8,\n                      ymn = 9110728.8, ymx = 9120760.8)\nraster::crs(r) <- \"EPSG:31985\"\n\n# The image is 349 (row) by 352 (col) whereas the raster is\n# 352 (col) by 349 (row) which is why the values are transposed, t(vals)\nr <- raster::setValues(r, t(vals))\n\n\nNext we will crop out the top-left hand corner of the image, convert the coordinates and the attributes of those raster cells into a SpatialPointsDataFrame for use with the GWmodel functions and calculate the geographically weighted statistics:\n\n\nCode\nr1 <- raster::crop(r, raster::extent(r, 1, 50, 1, 50))\npts <- as(r1, \"SpatialPointsDataFrame\")\n\nbw <- bw.gwr(layer ~ 1, data = pts,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F,\n             approach = \"AIC\")\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth (number of nearest neighbours): 1552 AICc value: 17665.9 \nAdaptive bandwidth (number of nearest neighbours): 967 AICc value: 17492.74 \nAdaptive bandwidth (number of nearest neighbours): 604 AICc value: 17293.36 \nAdaptive bandwidth (number of nearest neighbours): 381 AICc value: 17083.12 \nAdaptive bandwidth (number of nearest neighbours): 242 AICc value: 16875.38 \nAdaptive bandwidth (number of nearest neighbours): 157 AICc value: 16641.81 \nAdaptive bandwidth (number of nearest neighbours): 103 AICc value: 16366.74 \nAdaptive bandwidth (number of nearest neighbours): 71 AICc value: 16052.66 \nAdaptive bandwidth (number of nearest neighbours): 50 AICc value: 15616.22 \nAdaptive bandwidth (number of nearest neighbours): 38 AICc value: 15240.6 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 14739.11 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 14546.43 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 14003.78 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 13950.83 \nAdaptive bandwidth (number of nearest neighbours): 18 AICc value: 13851.75 \nAdaptive bandwidth (number of nearest neighbours): 18 AICc value: 13851.75 \n\n\nCode\ngwstats <- gwss(pts, vars = \"layer\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = F)\n\n\nWe can now compare the original image with the smoothed image.\n\n\nCode\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LM)\nr <- st_as_stars(raster::addLayer(r1, r2))\n\nggplot() + \n  geom_stars(data = r) +\n  facet_wrap(~ band) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n\n\n\n\n\nHowever, it isn’t only the geographically weighted mean that is calculated. You could use the geographically weighted standard deviation, for example, as a high pass filter – a form of edge detection.\n\n\nCode\nr2 <- raster::setValues(r1, gwstats$SDF$layer_LSD)\nr <- st_as_stars(r2)\n\nggplot() + \n  geom_stars(data = r) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n\n\n\n\n\n\nYou might spot that I have not loaded (required) the raster package in the code above but have accessed its functions via the :: notation; for example raster::setValues(). That is because if I do load the package, its select function will then mask a different function but with the same name in tidyverse’s dplyr. As I am only using the raster package very briefly, I didn’t think it was worth the potential confusion."
  },
  {
    "objectID": "gwstats.html#geographically-weighted-correlation",
    "href": "gwstats.html#geographically-weighted-correlation",
    "title": "Geographically Weighted Statistics",
    "section": "Geographically weighted correlation",
    "text": "Geographically weighted correlation\nAccording to the regression model earlier, there is a negative correlation between the age of the population and the percentage without schooling. The correlation across the Western Cape is,\n\n\nCode\ncor(wcape_wards$No_schooling, wcape_wards$age)\n\n\n[1] -0.4756981\n\n\nThat is, however, the global correlation for what appears (below) to be a heteroscedastic relationship.\n\n\nCode\nggplot(data = wcape_wards, aes(x = age, y = No_schooling)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship so let’s consider that by calculating and mapping the geographically weighted correlations between the two variables.\n\n\nCode\nwcape_wards_sp <- as_Spatial(wcape_wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\nThe local correlations range -0.868 to -0.105, with an interquartile range from -0.714 to -0.451:\n\n\nCode\nsummary(wcape_wards$Corr_No_schooling.age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.8679 -0.7136 -0.6294 -0.5801 -0.4506 -0.1054 \n\n\nIf we add a little ‘cartographic know-how’ from an earlier session, we can identify that the correlation is stronger in and around Parow than it is in and around Blue Downs, for example.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\",\n                                                 dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |>\n  filter(place == \"city\" | place == \"town\") |>\n  st_join(wcape_wards) |>\n    # A spatial spatial join,\n    # giving the point places the attributes of the wards they fall in\n  mutate(population = as.integer(population)) |>\n  filter(!is.na(ProvinceNa) & population > 60000) ->\n  places\n\nlast_plot() +\n  geom_sf_label_repel(data = places, aes(label = name), alpha = 0.7,\n                      force = 5, size = 2, max.overlaps = 20) +\n  xlab(element_blank()) +\n  ylab(element_blank())\n\n\n\n\n\nIn general, the correlation appears to be strongest in cities:\n\n\nCode\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  st_join(wcape_wards) %>%\n  st_drop_geometry %>%\n  group_by(place) %>%\n  summarise(meancorr = mean(Corr_No_schooling.age, na.rm = TRUE)) %>%\n  arrange(meancorr)\n\n\n# A tibble: 5 × 2\n  place             meancorr\n  <chr>                <dbl>\n1 city                -0.855\n2 hamlet              -0.620\n3 isolated_dwelling   -0.572\n4 village             -0.555\n5 town                -0.514"
  },
  {
    "objectID": "gwstats.html#statistical-inference-and-significance",
    "href": "gwstats.html#statistical-inference-and-significance",
    "title": "Geographically Weighted Statistics",
    "section": "Statistical inference and significance",
    "text": "Statistical inference and significance\nWe can use a randomisation procedure to determine whether any of the local summary statistics may be said to be significantly different from those obtained by chance. The randomisation procedure is found in the function, gwss.montecarlo() with a default number of nsim = 99 simulations. This isn’t very many but they are time-consuming to calculate and will be sufficient to demonstrate the process.\nThe following code chunk returns to the local mean percentages of high earners. It goes through the complete process of determining a bandwidth using the bw.gwr() function, then calculating the local and geographically weighted statistics using gwss(), determining the p-values under randomisation, using gwsss.montecarlo, then mapping the results. Very few of the results would be adjudged significant but a few are.\n\nCalculating the p-values under randomisation takes some time so please be patient.\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\ngwstats <- gwss(wcape_wards_sp , vars = \"High_income\", bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = \"High_income\", bw = bw,\n                            kernel = \"bisquare\",\n                            adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$High_income_LM <- gwstats$SDF$High_income_LM\nwcape_wards$High_income_LM[p.values$High_income_LM > 0.025 &\n                     p.values$High_income_LM < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = High_income_LM)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", na.value = \"light grey\",\n                       direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Local mean percentage of higher earrners\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\nA second example considers the local correlations between the No_schooling and age variables. Again, most of the correlations are insignificant but with a few exceptions.\n\n\nCode\nbw <- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\n\n\nCode\ngwstats <- gwss(wcape_wards_sp , vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values <- gwss.montecarlo(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"),\n                bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwcape_wards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nwcape_wards$Corr_No_schooling.age[p.values$Corr_No_schooling.age > 0.025 &\n                            p.values$Corr_No_schooling.age < 0.975] <- NA\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\",\n                       na.value = \"light grey\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\n\nWe have the problem of repeating testing that we also had when looking for spatial hotspots in the session about spatial autocorrelation. The function p.adjust() could be used but we may wonder, as previously, where the methods are too conservative."
  },
  {
    "objectID": "gwstats.html#improving-the-leigibility-of-the-map",
    "href": "gwstats.html#improving-the-leigibility-of-the-map",
    "title": "Geographically Weighted Statistics",
    "section": "Improving the leigibility of the map",
    "text": "Improving the leigibility of the map\n\nUsing a map insert\nBefore for completing this session there is value in addressing the problem that some parts of the map are so small that their contents are almost illegible (too small to be seen). The traditional and probably the most effective way to address this is to add one or more inserts to the map to magnify the small areas. One way to do this is by using the cowplot package with ggplot2. The following example is based on this tutorial and maps the percentages of the populations who are relatively high earners in each ward.\nFirst, we need to install and require the cowplot package.\n\n\nCode\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\", dependencies = TRUE)\nrequire(cowplot)\n\n\nSecond, the part of the wards map to be included in the map insert is extracted, using st_crop.\n\n\nCode\nwards_extract <- st_crop(wcape_wards, xmin = 18.2, xmax = 19, ymin = -34.3,\n                         ymax = -33.5)\nplot(st_geometry(wards_extract))\n\n\n\n\n\nThe bounding box for the extracted area is also obtained as it will be used as a feature in the final map (it will be drawn as a rectangle showing the geographical extent of the insert in the original map).\n\n\nCode\nbbox <- st_as_sfc(st_bbox(wards_extract))\n\n\nNext, the main part of the map is created…\n\n\nCode\nmain_map <- ggplot(wcape_wards, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\nplot(main_map)\n\n\n\n\n\n…as is the map insert:\n\n\nCode\ninsert <- ggplot(wards_extract, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_void() +\n  theme(legend.position = \"none\")\nplot(insert)\n\n\n\n\n\nFinally, the maps are brought together using cowplot’s ggdraw() and draw_plot() functions. We know that some of the values it shows are not necessarily significant under randomisation but we will include them here to just to get\n\n\nCode\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(insert, x = 0, y = 0.25, scale = 0.22) +\n  draw_label(\"Cape Town\\nand\\nsurrounding areas\", x = 0.62, y = 0.78, size = 8)\n\n\n\n\n\n\nThe positioning of the map insert (x = ... & y = ...), the scaling of it, and the size and position of the label were all found by trial and error, using various values until I was happy with the results.\n\n\nUsing a ‘balanced carogtram’\nAnother approach is to use what has been described as a visually balanced cartogram. A cartogram – of which there are lots of interesting examples on this website – usually works by distorting a map in a way that rescales the areas in proportion not to their physical size but by some other measured attribute such as their population count. Much of the motivation for this lies in political studies and mapping the results of an election wherein a traditional map can give a very distorted view of the outcome because of how much population density varies across a country. In the example below, rescaling the areas by population correctly shows that the 2020 US Presidential was not a Republican landslide, despite Trump’s baseless accusations!\n Source: www.viewsoftheworld.net\nThe problem with cartograms is the distortion. Essentially, they are trading one visual problem (invisibility of small areas) for another (the amount of geographical distortion). The idea of a balanced cartogram comes from an awareness that sometimes it is sufficient just to make the small areas bigger and/or the big areas smaller, without causing too much geographical distortion. One way to achieve this is to scale the places by the square root of the original areas, as in the following example.\n\n\nCode\nif(!(\"cartogram\" %in% installed)) install.packages(\"cartogram\",\n                                                   dependencies = TRUE)\nrequire(cartogram)\n\n# Convert from longitude/latitude to a grid projection\nwcape_wards %>%\n  st_transform(22234) %>%\n  mutate(area = as.numeric(st_area(.)),\n         w = as.numeric(sqrt(area))) ->\n  wards_prj\n\n# Create the cartogram -- here a contiguous cartogram is used, see ?cartogram\nwards_carto <- cartogram_cont(wards_prj, weight = \"w\", maxSizeError = 1.4,\n                              prepare = \"none\")\n\nggplot(wards_carto, aes(fill = High_income)) +\n  geom_sf(col = \"white\", size = 0.1) +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title =\n      \"Cartogram of the percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\n\n\n\n\n\nThe results are ok, drawing attention to the wards with higher percentages of higher earners but clearly there is geographical distortion in the map, too.\n\n\nUsing a hexogram\nThe final approach to be considered here is what has been described as hexograms – a combination of tile maps and cartograms that make small areas bigger on the map but try and limit the geographic distortion. The original method also preserved typology (i.e. contiguous neighbours remained so in the final map) but is dated and very slow to operationalise as it was really just a proof of concept. A faster method but one that cannot guarantee to preserve typology is presented below.\nFirst, we need some functions to create the hexogram.\n\n\nCode\nif(!(\"cartogram\" %in% installed.packages()[,1])) install.packages(\"cartogram\")\nif(!(\"sf\" %in% installed.packages()[,1])) install.packages(\"sf\")\nif(!(\"lpSolve\" %in% installed.packages()[,1])) install.packages(\"lpSolve\") \n\nrequire(sf)\n\ncreate_carto <- \\(x, w = NULL, k = 2, itermax = 25, maxSizeError = 1.4) {\n  if(class(x)[1] != \"sf\") stop(\"Object x is not of class sf\")\n  if(is.null(w)) x$w <- as.numeric(st_area(x))^(1/k)\n  cartogram::cartogram_cont(x, \"w\", itermax = itermax,\n                            maxSizeError = maxSizeError, prepare = \"none\")\n}\n\n\ncreate_grid <- \\(x, m = 6) {\n  bbox <- st_bbox(x)\n  ydiff <- bbox[4] - bbox[2]\n  xdiff <- bbox[3] - bbox[1]\n  \n  n <- m * nrow(x)\n  ny <- sqrt(n / (xdiff/ydiff))\n  nx <- n / ny\n  nx <- ceiling(nx)\n  ny <- ceiling(ny)\n  \n  grd <- st_sf(st_make_grid(x, n = c(nx, ny), square = FALSE))\n}\n\n\ngrid_data <- \\(x, grd, k = 2) {\n  x_pts <- st_centroid(st_geometry(x), of_largest_polygon = TRUE)\n  grd_pts <- st_centroid(grd)\n  \n  cost.mat <- st_distance(x_pts, grd_pts)^k\n  row.rhs <- rep(1, length(x_pts))\n  col.rhs <- rep(1, nrow(grd_pts))\n  row.signs <- rep(\"=\", length(x_pts))\n  col.signs <- rep(\"<=\", nrow(grd_pts))\n\n  optimisation <- lpSolve::lp.transport(cost.mat, \"min\", row.signs, row.rhs,\n                                        col.signs, col.rhs)$solution\n  \n  mch <- sapply(1:nrow(optimisation), \\(i) which(optimisation[i, ] == 1))\n  grd <- st_sf(grd[mch,])\n  cbind(grd, st_drop_geometry(x))\n}\n\n\ncreate_layers <- \\(x, grd) {\n  \n  area <- sapply(1: nrow(x), \\(i) {\n    y <- st_intersects(x[i, ], grd)[[1]]\n    if(i %in% y) {\n      if(length(y) == 1) return(st_area(x[i, ]))\n      if(length(y) > 1) {\n        area <- st_area(x[i, ])\n        overlaps <- y[-(y == i)]\n        area <- area - sum(st_area(st_intersection(st_geometry(x[i, ]),\n                                                st_geometry(grd[overlaps, ]))))\n        return(area) \n      }\n    } else {\n      return(0)\n    }\n  })\n\n  i <- which(area > 2*as.numeric(st_area(grd))[1])\n  list(x[i, ], grd[-i, ])\n}\n\n\nNext, we run through the stages of its creation, which are to start with a balanced cartogram (create_carto()), then overlay that with a grid of hexagons (create_grid()), next assign places in the original map to a hexagon (grid_data()), then plot the results as two layers (create_layers()) to give a mash-up of the cartogram and the hexagons.\n\n\nCode\nwards_carto <- create_carto(wards_prj)\ngrd <- create_grid(wards_carto)\nwards_grid <- grid_data(wards_carto, grd)\nwards_layers <- create_layers(wards_carto, wards_grid)\n\nggplot() +\n  geom_sf(data = wards_layers[[1]], aes(fill = High_income),\n          col = \"white\", size = 0.4) +\n  geom_sf(data = wards_layers[[2]], aes(fill = High_income),\n          col = \"dark grey\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"Hexogram % of the population who are higher earners\",\n    subtitle = \"South African Western Cape (2011)\"\n  )\n\n\n\n\n\nI find the result quite visually appealing and it isn’t just me: a separate study has provided empirical evidence for the value of balanced cartograms and hexograms as a visualisation tool mapping spatial distributions."
  },
  {
    "objectID": "gwstats.html#summary",
    "href": "gwstats.html#summary",
    "title": "Geographically Weighted Statistics",
    "section": "Summary",
    "text": "Summary\nThis session has introduced the concept of geographically weighted statistics to examine spatial heterogeneity in a measured variable and to allow for the possibility that its strength (and direction) of correlation with another variable varies from place-to-place. As such, these statistics are a form of local statistic, which is to say they can vary across the map. Sometimes, the parts of the map that we are interested in are small in relation to other parts, creating a problem of invisibility/legibility. Maps inserts, balanced cartograms and hexograms have been introduced as a means to address this visualisation problem."
  },
  {
    "objectID": "gwstats.html#futher-reading",
    "href": "gwstats.html#futher-reading",
    "title": "Geographically Weighted Statistics",
    "section": "Futher reading",
    "text": "Futher reading\nGollini I, Lu B, Charlton M, Brunsdon C & Harris P (2015). GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. Journal of Statistical Software, 63(17), 1–50. https://doi.org/10.18637/jss.v063.i17\n\nBrunsdon C, Comber A, Harris P, Rigby J & Large A (2021). In memoriam: Martin Charlton. Geographical Analysis, 54(4), 713–4. https://doi.org/10.1111/gean.12309"
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "Welcome",
    "section": "About the course",
    "text": "About the course\nThe contents of this course were first developed for a short course at the University of Cape Town (UCT) in August 2022. It also forms part of the MSc Geographic Data Science and Spatial Analytics in the School of Geographical Sciences, University of Bristol.\nThe aims of this course are to teach an introduction to mapping and spatial modelling in R. It is a course in geographic data science with a particular focus on mapping, measuring and quantifying spatial patterns in data. The present parts of the course are:\n\nWhy use R for mapping and spatial modelling?\nThe basics of mapping in R\nThe Spatial Variable: from maps towards models\nSpatial clustering and spatial heterogeneity: measuring patterns in data\nHarnessing spatial autocorrelation with geographically weighted statistics\nSpatial regression models\n\n\nThis is a work in progress\nChanges will be made and additional content added over time so check back here for the latest updates."
  },
  {
    "objectID": "index.html#pre-reading",
    "href": "index.html#pre-reading",
    "title": "Welcome",
    "section": "Pre-reading",
    "text": "Pre-reading\nThe following short pre-reading is recommended for the course:\nHarris RJ (2019). Not just nuisance: spatialising social statistics. In A Whitworth (ed.) Towards a Spatial Social Policy: Bridging the Gap Between Geography and Social Policy. Chapter 8. Bristol: Policy Press. Available here (or, if that doesn’t work try here)."
  },
  {
    "objectID": "index.html#other-useful-resources",
    "href": "index.html#other-useful-resources",
    "title": "Welcome",
    "section": "Other useful resources",
    "text": "Other useful resources\nSpatial Regression Models for the Social Sciences covers similar statistical ground to this course, For University of Bristol students, it is available to view as an eBook here.\n\nIn addition, Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow offers an extremely useful reference to have to hand if you are stuck when undertaking geocomputation with R. There is a free online version available."
  },
  {
    "objectID": "index.html#provisional-masters-programme",
    "href": "index.html#provisional-masters-programme",
    "title": "Welcome",
    "section": "Provisional Masters programme",
    "text": "Provisional Masters programme\nFor the 2022-3 iteration of the Masters unit, the draft teaching schedule is:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\nMon Jan 23, 3 - 6pm\nWhy R & Flavours of R\n\n\n2\nMon Jan 30, 3 - 6pm\nMapping the spatial variable\n(Thematic maps in R)\n\n\n3\nMon Feb 13, 3 - 6pm\nMeasuring spatial autocorrelation\n\n\n4\nMon Feb 20, 3 - 6pm\nFrom Maps to models (1):\nGeographically Weighted Statistics\n\n\n5\nMon Mar 6, 3 - 6pm\nFrom Maps to models (2):\nSpatial Regression\n\n\n6\nMon Mar 13, 3 - 6pm\nTBC\n\n\n\n\nAdditional virtual office hours\nInformal ‘drop-in’ sessions when you can chat to me online. These won’t appear in your formal timetables.\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\n\n\n\n\nMonday Feb 6\n3 - 4pm\nIn person – in my office\n\n\nMonday Feb 27\n3 - 4pm*\n*Likely to be affected by UCU industrial action\n\n\nMonday Apr 17\n3 - 4pm\nOnline – details to follow\n\n\nMonday Apr 24\n3 - 4pm\nOnline – details to follow\n\n\n\nFor more about the reasons for the industrial action, please see here."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Welcome",
    "section": "About the author",
    "text": "About the author\nThis course is authored by Richard Harris, Professor of Quantitative Social Geography at the University of Bristol. You can find out more about me, my research and other interests at https://profrichharris.github.io/. It is co-taught at the University with Dr. Richard Timmerman.\n @profrichharris"
  },
  {
    "objectID": "index.html#copyright-notice",
    "href": "index.html#copyright-notice",
    "title": "Welcome",
    "section": "Copyright notice",
    "text": "Copyright notice\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n \n@GeogBristol #justsaying!"
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "",
    "text": "(Draft version)"
  },
  {
    "objectID": "multilevel.html#introduction",
    "href": "multilevel.html#introduction",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Introduction",
    "text": "Introduction\nGeographically Weighted Regression, which we looked at in the previous session, treats geographic space as a continuous spatial variable in the sense that regression relationships can vary from one location to another across a geographic study region. That continuous view of space is clear if we fit a simple ‘null model’ of the COVID-19 rates in London in the week before Christmas 2021 and map the results. It is a null model because it includes no predictor variables other than the local estimate of the intercept (i.e. the local mean rate of COVID-19). What is clear is how that local estimate is allowed to vary across geographic space.\n(Download and extract the data for London and fit the GWR model)\n\n\nCode\nrequire(sf)\nrequire(ggplot2)\nrequire(tidyverse)\nrequire(GWmodel)\n\nif(!file.exists(\"covid_xmas_2021.geojson\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_xmas_2021.geojson\", \"covid_xmas_2021.geojson\", mode = \"wb\", quiet = TRUE)\nxmas_covid <- read_sf(\"covid_xmas_2021.geojson\")\n\nxmas_covid %>% \n  filter(regionName == \"London\") ->\n  ldn_covid\n\nldn_covid_sp <- as_Spatial(ldn_covid)\nbw <- bw.gwr(rate ~ 1, data = ldn_covid_sp, adaptive = TRUE)\ngwrmod <- gwr.basic(rate ~ 1, data = ldn_covid_sp, bw = bw, adaptive = TRUE)\n\n\n(Map the predicted COVID-19 rates across London based on the spatially varying local mean estimate)\n\n\nCode\nldn_covid$yhat <- gwrmod$SDF$yhat\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nMultilevel models, by contrast, have more of a discrete view of space because observations at some base level are treated as members of a group at a second, more aggregate level. In a geographic context, the base level could be neighbourhoods and the groups could be the local authorities to which the neighbourhoods belong. This more discrete view of space becomes clear if we fit a very simple (too simple, really) null model of the COVID-19 rates in London, using the local authority estimates from this multilevel model as the nearest equivalent to the local means of the GWR model.\n\n\nCode\nrequire(lme4)\nmlm <- lmer(rate ~ (1|LtlaName), data = ldn_covid)\nldn_covid$yhat <- predict(mlm, re.form = ~ (1|LtlaName))\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nIf we compare the map above with that produced from the GWR estimates then we find that there are commonalities in the two maps but the multilevel model clearly has the more bounded view of geographic space."
  },
  {
    "objectID": "multilevel.html#multilevel-models",
    "href": "multilevel.html#multilevel-models",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Multilevel models",
    "text": "Multilevel models\nThe package that we are using to fit the models here is lme4. The formula rate ~ (1|LtlaName) means that we are modelling the COVID-19 rate against a random intercept that varies by local authority (by local authority name, LtlaName). It is not the only way of fitting multilevel models in R (an alternative, for example, is brms) but it is sufficient for the purposes of this exercise.\nThe more discrete and bounded view of geographic space that the multilevel model (in its simplest form, at least) employs seems quite restricting – in this case study it is suggesting that the causes of COVID-19 are somehow related to ‘things’ that happen or have consequences at a local authority scale. The implication is that the boundaries of local authorities are relevant to the geographical causes and rates of COVID-19. This is not an assumption that GWR makes so why use multilevel models instead?\nOne answer is that a multilevel model is often faster to fit. Consider the following example, which extends the study region to London and the South East.\n(Fit the GWR model and the multilevel model)\n\n\nCode\nxmas_covid %>%\n  filter(regionName == \"London\" | regionName == \"South East\") ->\n  ldn_se_covid\nldn_se_covid_sp <- as_Spatial(ldn_se_covid)\nt1 <- Sys.time()\nbw <- bw.gwr(rate ~ 1, data = ldn_se_covid_sp, adaptive = TRUE)\ngwrmod <- gwr.basic(rate ~ 1, data = ldn_se_covid_sp, bw = bw, adaptive = TRUE)\ngwrt <- Sys.time() - t1\nt2 <- Sys.time()\nmlm <- lmer(rate ~ (1|LtlaName), data = ldn_se_covid)\nmlmt <- Sys.time() - t2\n\n\nThe computation times are,\n\n\nCode\ngwrt\n\n\nTime difference of 4.767647 secs\n\n\nCode\nmlmt\n\n\nTime difference of 0.03726602 secs\n\n\nAdmittedly, neither took very long to fit, with the GWR taking 0.08 minutes and the multilevel model (MLM) taking 0.04 seconds on my laptop. However, these differences become more noticeable with more complex models or larger study regions. The multilevel model is estimating far fewer model parameters than the GWR. This is because the GWR is really lots of models that are fitted separately (one for each location) and then compared, whereas the multilevel model is one model but one which allows for variance at the geographic level(s) of the model, so better local authorities, for example. Put another way, the GWR is a series of local models that are compared, whereas the multilevel model is a global model that allows for local variations around it: at the moment, all it is doing is estimating the average COVID rate for the whole study region but also recognising that different local authorities have rates that vary around the overall average.\nSecond, multilevel models can be used – as their name suggests! – to fit multi-level models to examine the scale of the geographic pattern of a variable. Consider the following example, which fits a multilevel model to the COVID-19 rates of all English neighbourhoods in the week before Christmas 2021. There are now three levels in the model, which are the base neighbourhoods level, then the local authorities in which the neighourhoods are located, and then the English regions to which the local authorities belong. Hence, three levels and three simultaneous scales of analysis: neighbourhoods, local authorities and regions. Although the resulting map suffers a little from the ‘invisibility problem’ of some small areas within it, the higher COVID-19 rate in parts of London and the South East is evident (but not solely confined to these regions, as there is also a high rate evident in the North West).\n\n\nCode\nmlm <- lmer(rate ~ (1|LtlaName) + (1|regionName), data = xmas_covid)\nxmas_covid$yhat <- predict(mlm, re.form = ~ (1|LtlaName) + (1|regionName))\nggplot(data = xmas_covid, aes(fill = yhat)) +\n  geom_sf(col = NA) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1) +\n  theme_minimal()\n\n\n\n\n\nHaving fitted a model with three geographic levels, the geographical question that we can now ask is which of the levels of the model contributes most to the geographical patterning of the disease. Do we get most variation between neighbourhoods (suggesting the geographic pattern is mostly at the neighbourhood level), between local authorities (so a local authority patterning) or between regions (evidence of strongest regional differences)? We can answer the question by looking at how much of the variance belongs to each level, which we can see in the summary under Random effects.\n\n\nCode\nsummary(mlm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rate ~ (1 | LtlaName) + (1 | regionName)\n   Data: xmas_covid\n\nREML criterion at convergence: 2858.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3127 -0.6222 -0.0334  0.5832  6.8771 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n LtlaName   (Intercept) 0.08043  0.2836  \n regionName (Intercept) 0.14923  0.3863  \n Residual               0.07714  0.2777  \nNumber of obs: 6789, groups:  LtlaName, 315; regionName, 9\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)     1.26       0.13   9.687\n\n\nFrom these summary values we can work out the percentage of the variance which is at each level, calculating what is known as the intraclass correlation (ICC). For this particular week of the pandemic it is,\n\n\nCode\nsummary(mlm)$varcor %>%\n  as_tibble %>%\n  select(grp, vcov) %>%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  <chr>       <dbl> <dbl>\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThe ICC appears to suggest the presence of regional effects, in particular (because the ICC for regionName is greater than the ICC for any other level). These regional effects, which are treated as random effects in the model, can be examined using the code below, from which they are found to be strongest for London. This means that the COVID-19 rate for this week is higher for London that it is for any other region.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  arrange(desc(condval))\n\n\n# A tibble: 9 × 5\n  grpvar     term        grp                      condval condsd\n  <chr>      <fct>       <fct>                      <dbl>  <dbl>\n1 regionName (Intercept) London                    0.814  0.0505\n2 regionName (Intercept) East of England           0.236  0.0433\n3 regionName (Intercept) North West                0.181  0.0463\n4 regionName (Intercept) South East                0.0644 0.0356\n5 regionName (Intercept) East Midlands            -0.0688 0.0463\n6 regionName (Intercept) West Midlands            -0.189  0.0527\n7 regionName (Intercept) Yorkshire and The Humber -0.241  0.0626\n8 regionName (Intercept) North East               -0.389  0.0816\n9 regionName (Intercept) South West               -0.407  0.0536\n\n\nOf course, somewhere has to be highest so the fact the it is highest for London does not mean the difference is necessarily statistically significant. However, as it turns out, it is significant (significantly different from zero), which we can see if we plot what is called a caterpillar plot in the multilevel literature, drawn here with a 95% confidence interval around each of the regional effects.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nNot only is London’s regional ‘uplift’ in the COVID rates significantly greater than zero, London’s regional effect is greater than for any other regions. The following chart suggests that London is different from the rest because its confidence interval does not overlap with any others. Note that if you are testing to see if two places differ from each other as opposed to from zero then you need to shorten the confidence interval before looking to see if they overlap. As a rule of thumb, the 95% confidence interval for a test of difference in two values is 1.39 \\(\\times\\) the standard error of the parameter estimate instead of the more usual 1.96.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.39 * condsd,\n         upr = condval + 1.39 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nDespite the clear evidence of regional differences that are driven by London’s much higher COVID-19 rate in the week before Christmas 2021, keep in mind that not all the pattern in the COVID-19 rates is at the regional level. To recall,\n\n\nCode\nsummary(mlm)$varcor %>%\n  as_tibble %>%\n  select(grp, vcov) %>%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  <chr>       <dbl> <dbl>\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThere are, for example, differences between the local authorities, too, at the next level down in the model. These are evident from a caterpillar plot for the authorities although because there are more of these authorities than there are regions (315 authorities Vs rlength(unique(xmas_covid$regionName))` regions, individual ones are harder to discern.\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_blank()) +\n           xlab(\"Local authorities\")\n\n\n\n\n\nLooking at the ‘top ten’ with COVID-19 rates higher than the national and their regional averages would predict are,\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  arrange(desc(condval)) %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  slice_max(condval, n = 10)\n\n\n# A tibble: 10 × 7\n   grpvar   term        grp                  condval condsd   lwr   upr\n   <chr>    <fct>       <fct>                  <dbl>  <dbl> <dbl> <dbl>\n 1 LtlaName (Intercept) Thurrock               0.849 0.0746 0.702 0.995\n 2 LtlaName (Intercept) Dartford               0.688 0.0814 0.528 0.847\n 3 LtlaName (Intercept) Bristol, City of       0.679 0.0645 0.553 0.805\n 4 LtlaName (Intercept) Lambeth                0.674 0.0675 0.541 0.806\n 5 LtlaName (Intercept) Manchester             0.666 0.0584 0.551 0.780\n 6 LtlaName (Intercept) Elmbridge              0.643 0.0722 0.502 0.785\n 7 LtlaName (Intercept) Reigate and Banstead   0.587 0.0722 0.446 0.729\n 8 LtlaName (Intercept) Epsom and Ewell        0.582 0.0937 0.398 0.765\n 9 LtlaName (Intercept) Salford                0.567 0.0671 0.435 0.698\n10 LtlaName (Intercept) Gedling                0.554 0.0820 0.393 0.714\n\n\nThe key point here is that the multilevel model is useful to help unpack at what scale(s) geographic outcomes are arising and to indentify some of the key places driving those outcomes."
  },
  {
    "objectID": "multilevel.html#spatial-varying-coefficient-effects",
    "href": "multilevel.html#spatial-varying-coefficient-effects",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Spatial varying coefficient effects",
    "text": "Spatial varying coefficient effects\nThe mutilevel models fitted above are random intercepts models – only the intercept term is permitted to vary from place to place. Under these models, some places are expected to have a higher or lower average COVID-19 rate than others but that rate is not affected by any predictor variables that can also very in their effects from place-to-place.\nImagine that the percentage of the population of secondary school age is a factor in the COVID-19 rates in London ahead of Christmas 2021. It is a very simple model but it appears to have some (limited) explanatory power. As it happens, more children of school age appears to be less associated with COVID, perhaps because they and the families had already caught it?\n\n\nCode\nols <- lm(rate ~ age12.17, data = ldn_covid_sp)\nsummary(ols)\n\n\n\nCall:\nlm(formula = rate ~ age12.17, data = ldn_covid_sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11561 -0.29813 -0.00225  0.27856  1.20720 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.629006   0.058593  44.869   <2e-16 ***\nage12.17    -0.076615   0.008245  -9.292   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4111 on 980 degrees of freedom\nMultiple R-squared:  0.08097,   Adjusted R-squared:  0.08003 \nF-statistic: 86.34 on 1 and 980 DF,  p-value: < 2.2e-16\n\n\nA geographically weighted regression suggests that there might be spatial variation in the relationship between age12.17 and the COVID rate. In truth, not all of these local estimates are necessarily significant but let us ignore that for the moment.\n(Fit the GWR model allowing for spatial variation in the effects of age12.17 on rate)\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE)\n\n\nAdaptive bandwidth: 614 CV score: 124.8849 \nAdaptive bandwidth: 387 CV score: 113.7512 \nAdaptive bandwidth: 246 CV score: 103.5928 \nAdaptive bandwidth: 159 CV score: 94.75184 \nAdaptive bandwidth: 105 CV score: 88.35261 \nAdaptive bandwidth: 72 CV score: 84.00381 \nAdaptive bandwidth: 51 CV score: 81.04989 \nAdaptive bandwidth: 38 CV score: 79.4773 \nAdaptive bandwidth: 30 CV score: 79.26434 \nAdaptive bandwidth: 25 CV score: 79.3698 \nAdaptive bandwidth: 33 CV score: 79.28656 \nAdaptive bandwidth: 28 CV score: 79.23342 \nAdaptive bandwidth: 27 CV score: 79.22796 \nAdaptive bandwidth: 26 CV score: 79.35989 \nAdaptive bandwidth: 27 CV score: 79.22796 \n\n\nCode\ngwrmod <- gwr.basic(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE, bw = bw)\n\n\n(Map the results)\n\n\nCode\nldn_covid$age12.17GWR <- gwrmod$SDF$age12.17\nggplot(data = ldn_covid, aes(fill = age12.17GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n  theme_minimal() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nThe closest multilevel model to the above GWR model (given the current data) is one that allows the effects of age12.17 to vary by local authority (LtlaName).\n(fit the model)\n\n\nCode\nmlm <- lmer(rate ~ (age12.17|LtlaName), data = ldn_covid)\n\n\n(plot the results)\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  filter(term == \"age12.17\") %>%\n  rename(LtlaName = grp,\n         age12.17LM = condval) %>%\n  select(LtlaName, age12.17LM) %>%\n  inner_join(ldn_covid, ., by = \"LtlaName\") %>%\n  ggplot(aes(fill = age12.17LM)) +\n    geom_sf(size = 0.25) +\n    scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n    theme_minimal() +\n    guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nAgain, there are commonalities in the GWR and MLM estimates but the differing conceptions of the structure of geographic space and the spatial relationships it contains makes a difference to the results."
  },
  {
    "objectID": "multilevel.html#more-to-be-added",
    "href": "multilevel.html#more-to-be-added",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "More to be added!",
    "text": "More to be added!\nThis session is incomplete. For now, the key point is that GWR and multilevel models conceptualise space in different ways. I am not meaning to imply that multilevel models are worse, as they can be very useful and flexible. It useful to note that there are now methods that intgrate spatial regression models with multilevel models (see, for example, here and GWR with hierarchical group structures (for example, this and this but these are pretty advanced methods of analysis!"
  },
  {
    "objectID": "multilevel.html#further-reading",
    "href": "multilevel.html#further-reading",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Further Reading",
    "text": "Further Reading\nMultilevel models – an introduction to multilevel models from the Handbook of Spatial Analysis in the Social Sciences."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "program.html#scripts",
    "href": "program.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is based on the opening parts of Geographic Data in ipumsr, where ipumsr provides “an easy way to import census, survey and geographic data provided by ‘IPUMS’ into R”, and IPUMS “provides census and survey data from around the world integrated across time and space.” I have modified their code for greater consistency with other parts of this course but what it does is largely the same:\n\nIt loads some example data and manipulates it to calculate the percentages of people using solid fuels for cooking in regions of Colombia at three time periods. These are the attribute data\nIt loads a geographic boundary file of those regions. This is the map.\nIt simplifies (reduces the detail of) the map to make it faster for plotting.\nIt joins the attribute data to the map by means of their common geography.\nIt produces a choropleth map (thematic map) of the geographic variation in the percentages using solid fuel.\nIt saves the spatially joined data as a shapefile.\n\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the following maps.\nBe patient whilst the code takes a few moments to run.\n\n\n\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety.\n\nIf the script doesn’t run to completion then it may be because it is not very sensibly coded on my part and risks introducing an infinite loop that will never finish! This can happen because I have tried (unwisely) to be clever: the script tries to load (to require()), for example, the remotes library and, if that fails, tries to install it instead, before repeating the attempt to require it. The problem is that if it doesn’t install for whatever reason, then the library will never successfully load and the process will just go around and around in circles.\n\nTo Do\nAs suggested earlier, an advantage of using a script is that it is easy to make changes to and then re-run it either in part or in full. In the script, find the type = \"cartolight\" argument in the line that says annotation_map_tile(type = \"cartolight\", progress = \"none\") + and change the type to any other of the following, such as cartodark.\n\n\nCode\nrosm::osm.types()\n\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"hikebike\"              \n [7] \"hillshade\"              \"osmgrayscale\"           \"stamenbw\"              \n[10] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[13] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\nIf you wish to view what the different types look like, you can do so here.\nOnce you have made the change, select the parts of the script you wish to rerun (everything under Map the data should be sufficient) and press the Run button or hit command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) to execute the selected code.\n\n\nA note about the :: notation\nThe use of the :: notation in the code rosm::osm.types() allows a function to be run from a package that has not been loaded. In the example, osm.types is a function in the rosm package. That package provides access to Open Steet Map and other maps tiles. We could also require(rosm) and then use the function directly as we have in other cases, i.e. using osm.types() instead of rosm::osm.types(). However, sometimes, if a function only is to be used once then there is no need to require the whole package. Also, sometimes we load multiple packages that have functions within them that share the same name and could conflict with each other. In such circumstances, the package::function format may be required to make sure the correct function (from the correct package) is being called."
  },
  {
    "objectID": "program.html#r-markdown",
    "href": "program.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs, then to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -> New File -> R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet."
  },
  {
    "objectID": "program.html#the-source-code-for-this-document",
    "href": "program.html#the-source-code-for-this-document",
    "title": "Programming",
    "section": "The source code for this document",
    "text": "The source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "program.html#summary",
    "href": "program.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "program.html#further-reading",
    "href": "program.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/. It is a bit advanced for this stage of the course but it is worth noting that there is a cheatsheet available."
  },
  {
    "objectID": "spregress.html",
    "href": "spregress.html",
    "title": "Spatial Regression",
    "section": "",
    "text": "In the previous session we looked at geographically weighted statistics, including geographically weighted correlation, which examines whether the correlation between two variables varies across a map. In this session we extend from correlation to looking at regression modelling with a spatial component (spatial regression). The data we shall use are a map of the rate of COVID-19 infections per thousand population in neighbourhoods of the North West of England over the period from the week commencing 2020-03-07 to the week commencing 2022-04-16. That rate is calculated as the total number of reported infections per neighbourhood, divided by the mid-2020 ONS estimated population. There are three problems with these data, which originate (prior to adjustment) from https://coronavirus.data.gov.uk/details/download:\n\nThe rate, as calculated, does not allow for re-infection (i.e. the same person can catch COVID-19 more than once).\nNot everyone who had COVID was tested for a positive diagnosis. Limitations in the testing regime are described in this paper and are most severe earlier in the pandemic and again towards the end of the data period when testing was scaled-back and then largely withdrawn.\nFor data protection reasons, where a neighbourhood had less than three cases in a week, that number was reported as zero even though it could really be one or two. This undercount adds up, although, unsurprisingly it affects the largest cities most (because they have more neighbourhoods to be undercounted) in weeks when COVID is not especially prevalent (because in other weeks there are more often more than two cases per neighbourhoods). An adjustment has been made to the data to allow for this censoring of the data but not for the undercount caused by not testing positive.\n\nDespite their shortcomings, the data are sufficient to illustrate the methods, below."
  },
  {
    "objectID": "spregress.html#getting-started",
    "href": "spregress.html#getting-started",
    "title": "Spatial Regression",
    "section": "Getting Started",
    "text": "Getting Started\nThe data are downloaded as follows. The required packages should be installed already, from previous sessions. As before, you may wish to begin by opening the R Project that you created for these classes.\n\n\nCode\nif(!(\"remotes\" %in% installed.packages()[,1])) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed.packages()[,1])) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(sf)\nrequire(tidyverse)\nrequire(ggplot2)\nrequire(ggsflabel)\n\nif(!file.exists(\"covid.geojson\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid.geojson\",\n\"covid.geojson\", mode = \"wb\", quiet = TRUE)\n\nread_sf(\"covid.geojson\") |>\n  filter(regionName == \"North West\") ->\n  covid\n\n\nLooking at a map of the COVID-19 rates, it appears that there are patches of higher rates that tend to cluster in cities such as Preston, Manchester and Liverpool, although not exclusively so.\n\nDon’t forget that you can, if you wish, use the cols4all and it’s functions, e.g. scale_fill_continuous_c4a_seq() instead of scale_fill_distiller() in the code chunk below.\n\n\nCode\ncovid |>\n  filter(Rate > 400) |>\n  select(LtlaName) |>   # LtlaName is the name of the local authority\n  filter(!duplicated(LtlaName)) ->\n  high_rate\n\nggplot() +\n  geom_sf(data = covid, aes(fill = Rate), size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  geom_sf_label_repel(data = high_rate, aes(label = LtlaName), size = 3,\n                      alpha = 0.5) +\n  theme_void()\n\n\n\n\n\n… and not without variation within the cities such as Manchester:\n\n\nCode\nggplot(data = covid |> filter(LtlaName == \"Manchester\")) +\n  geom_sf(aes(fill = Rate), size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  theme_void()\n\n\n\n\n\nAcross the map there appears to be a pattern of positive spatial autocorrelation in the COVID-19 rates of contiguous neighbours, whether this is measured using a Moran test,\n\n\nCode\nrequire(spdep)\nspweight <- nb2listw(poly2nb(covid, snap = 1))\nmoran.test(covid$Rate, spweight)\n\n\n\n    Moran I test under randomisation\n\ndata:  covid$Rate  \nweights: spweight    \n\nMoran I statistic standard deviate = 24.422, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4919206758     -0.0010834236      0.0004075223 \n\n\nor using the Pearson correlation,\n\n\nCode\ncor(lag.listw(spweight, covid$Rate), covid$Rate)\n\n\n[1] 0.6726405\n\n\nSome of the ‘hot spots’ of infection are suggested using the geographically-weighted means,\n\n\nCode\nrequire(GWmodel)\ncovid_sp <- as_Spatial(covid)\nbw <- bw.gwr(Rate ~ 1, data = covid_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F)\ngwstats <- gwss(covid_sp, vars = \"Rate\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = F)\n\n\nPlotting these,\n\n\nCode\ncovid$Rate_LM <- gwstats$SDF$Rate_LM\nggplot(covid, aes(fill = Rate_LM)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  theme_void()\n\n\n\n\n\nThey also are suggested using the G-statistic with a somewhat arbitrary bandwidth of 5km.\n\n\nCode\ncoords <- st_centroid(covid, of_largest_polygon = TRUE)\nneighbours <- dnearneigh(coords, 0, 5000)\nspweightB <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\ncovid$localG <- localG(covid$Rate, spweightB)\nbrks <- c(min(covid$localG, na.rm = TRUE),\n          -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$localG, na.rm = TRUE))\ncovid$localG_gp <- cut(covid$localG, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\",\n                 \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = localG_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "spregress.html#an-initial-model-to-explain-the-spatial-patterns-in-the-covid-19-rates",
    "href": "spregress.html#an-initial-model-to-explain-the-spatial-patterns-in-the-covid-19-rates",
    "title": "Spatial Regression",
    "section": "An initial model to explain the spatial patterns in the COVID-19 rates",
    "text": "An initial model to explain the spatial patterns in the COVID-19 rates\nIt is likely that the spatial variation in the COVID-19 rates is due to differences in the physical attributes of the different neighbourhoods and/or of the populations who live in them. For example, the rates may be related to the relative level of deprivation in the neighbourhoods (the Index of Multiple Deprivation, IMD), the age composition of their populations (e.g. percentage aged 0 to 11, age0.11), the population density (density), the number of carehome beds (carebeds, because particularly early on in the pandemic, carehome residents were at very high risk), and whether they contain an Accident and Emergency hospital (AandE, coded 1 if they do and 0 if they don’t). Incorporating this into a standard regression model gives,\n\n\nCode\nols1 <- lm(Rate ~ IMD + age0.11 + age12.17 + age18.24 + age25.34 + age35.39 + \n             age50.59 + age60.69 + age70plus + density +\n             carebeds + AandE, data = covid)\nsummary(ols1)\n\n\n\nCall:\nlm(formula = Rate ~ IMD + age0.11 + age12.17 + age18.24 + age25.34 + \n    age35.39 + age50.59 + age60.69 + age70plus + density + carebeds + \n    AandE, data = covid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-134.816  -17.761    1.592   20.124  139.444 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 182.26850  113.79175   1.602 0.109553    \nIMD          -0.29381    0.10610  -2.769 0.005736 ** \nage0.11       2.21553    1.36681   1.621 0.105376    \nage12.17     -2.36944    1.94032  -1.221 0.222341    \nage18.24     -0.16683    1.16041  -0.144 0.885719    \nage25.34      2.31424    1.19221   1.941 0.052550 .  \nage35.39      6.65665    2.65716   2.505 0.012413 *  \nage50.59      6.31033    1.74161   3.623 0.000307 ***\nage60.69      0.34605    1.46550   0.236 0.813386    \nage70plus    -0.71097    1.23595  -0.575 0.565269    \ndensity     164.39302  636.21170   0.258 0.796162    \ncarebeds      0.03583    0.01489   2.407 0.016292 *  \nAandE         1.13369    4.03313   0.281 0.778703    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.75 on 911 degrees of freedom\nMultiple R-squared:  0.2039,    Adjusted R-squared:  0.1934 \nF-statistic: 19.44 on 12 and 911 DF,  p-value: < 2.2e-16\n\n\n which, as its R-squared value of 0.204 suggests, goes some way to explaining the variation in the rates. Note that you can tidier output for some models using tidyvere’s broom package and functions.\n\n\nCode\nrequire(broom)\ntidy(ols1)\n\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic  p.value\n   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept) 182.      114.         1.60  0.110   \n 2 IMD          -0.294     0.106     -2.77  0.00574 \n 3 age0.11       2.22      1.37       1.62  0.105   \n 4 age12.17     -2.37      1.94      -1.22  0.222   \n 5 age18.24     -0.167     1.16      -0.144 0.886   \n 6 age25.34      2.31      1.19       1.94  0.0525  \n 7 age35.39      6.66      2.66       2.51  0.0124  \n 8 age50.59      6.31      1.74       3.62  0.000307\n 9 age60.69      0.346     1.47       0.236 0.813   \n10 age70plus    -0.711     1.24      -0.575 0.565   \n11 density     164.      636.         0.258 0.796   \n12 carebeds      0.0358    0.0149     2.41  0.0163  \n13 AandE         1.13      4.03       0.281 0.779   \n\n\nCode\nglance(ols1)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.204         0.193  31.7      19.4 4.88e-38    12 -4500. 9027. 9095.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nLooking at R’s standard diagnostic plots there is not any strong evidence that the standard ]residual Normality assumption](https://thestatsgeek.com/2013/08/07/assumptions-for-linear-regression/){target=“_blank”} of the regression statistics is being violated although, not very surprisingly, the variance inflation values (VIF) do suggest high levels of colinearity between the age variables.\n\n\nCode\npar(mfrow = c(2,2))\nplot(ols1)\n\n\n\n\n\nCode\nif(!(\"car\" %in% installed.packages()[,1])) install.packages(\"car\")\nrequire(car)\nvif(ols1)\n\n\n      IMD   age0.11  age12.17  age18.24  age25.34  age35.39  age50.59  age60.69 \n 2.834069 18.836543  6.050232 38.347245 27.513979 10.228388 16.430177 14.397087 \nage70plus   density  carebeds     AandE \n38.881759  2.123517  1.135127  1.030432 \n\n\n\nThere are no hard and fast rules but, in broad terms, a VIF value of 4 or 5 or above is worth considering as a potential issue and a value above 10 suggests a very high level of multicollinearity. The simple solution to the issue is to drop some of the colinear variables but not all: if X1 and X2 are highly correlated, you only need to consider dropping X1 or X2, not both.\n Although I am generally cautious about automated selection procedures, in this case a stepwise model selection appears useful to address the colinearity:\n\n\nCode\nols2 <- step(ols1)\n\n\nThe results are,\n\n\nCode\ntidy(ols2)\n\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 147.       14.4        10.3  1.96e-23\n2 IMD          -0.260     0.0941     -2.76 5.90e- 3\n3 age0.11       1.81      0.532       3.40 6.95e- 4\n4 age25.34      2.91      0.476       6.11 1.48e- 9\n5 age35.39      7.28      1.54        4.74 2.48e- 6\n6 age50.59      6.66      0.638      10.4  3.60e-24\n7 carebeds      0.0333    0.0142      2.35 1.91e- 2\n\n\nCode\nglance(ols2)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.201         0.196  31.7      38.5 8.36e-42     6 -4501. 9018. 9057.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nCode\nvif(ols2)\n\n\n     IMD  age0.11 age25.34 age35.39 age50.59 carebeds \n2.236588 2.865175 4.394042 3.425196 2.212515 1.031632 \n\n\nAn analysis of variance (ANOVA) shows there is no statistically gain in using the model with more variables (ols1) so we can prefer the simpler (ols2).\n\n\nCode\nanova(ols2, ols1)\n\n\nAnalysis of Variance Table\n\nModel 1: Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + carebeds\nModel 2: Rate ~ IMD + age0.11 + age12.17 + age18.24 + age25.34 + age35.39 + \n    age50.59 + age60.69 + age70plus + density + carebeds + AandE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    917 921337                           \n2    911 918176  6    3161.6 0.5228 0.7913\n\n\nLooking again at the model (tidy(ols2)), it may seem surprising that the deprivation index is negatively correlated with the COVID rate – implying more deprivation, fewer infections (geographies of health often work in the opposite direction; being poorer can come with a ‘health premium’) – but this is due to the later variants of the disease that spread quickly through more affluent populations when restrictions on mobility and social interaction have been relaxed."
  },
  {
    "objectID": "spregress.html#spatial-dependencies-in-the-model-residuals",
    "href": "spregress.html#spatial-dependencies-in-the-model-residuals",
    "title": "Spatial Regression",
    "section": "Spatial dependencies in the model residuals",
    "text": "Spatial dependencies in the model residuals\nAlthough the model appears to fit the data reasonably well, there is a problem. The residuals – the differences between what the model predicts as the COVID-19 rate at each location and what those rates actually are – are supposed to be random noise, meaning their values should be independent of their location and of each other, with no spatial structure. They are not. In fact, if we apply a Moran’s test to the residuals, using the test for regression residuals, lm.morantest(), we find that they are significantly spatially correlated:\n\n\nCode\nlm.morantest(ols2, spweight)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age35.39 +\nage50.59 + carebeds, data = covid)\nweights: spweight\n\nMoran I statistic standard deviate = 22.357, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.4461249282    -0.0033794098     0.0004042296 \n\n\n The pattern is evident if we map the standardised residuals from the function rstandard(). At the risk of geographic over-simplification, there is something of a north-south divide, with a patch of negative residuals to the north of the study region. These are rural neighbouhoods where the model is under-predicting the rate of COVID-19 cases.\n\n\nCode\ncovid$resids <- rstandard(ols2)\nbrks <- c(min(covid$resids), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$resids))\ncovid$resids_gp <- cut(covid$resids, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = resids_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"Standardised residual\", values = pal) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\n\nLooking again at the model\nIt is possible that the spatial structure in the residuals exists because the model has been mis-specified. In particular, we might wonder if it was a mistake to drop the population density variable which perhaps had a polynomial (non-linear) relationship with the COVID rates. Let’s find out.\n\n\nCode\nols3 <- update(ols2, . ~ . + poly(density, 2))\n\n\n Certainly, of the three models, this is the best fit to the data, as we can see from the various model fit diagnostics that are easily gathered together using the glance() function. Note the highest, adjusted r-squared value and lowest AIC value, for example.\n\n\nCode\nbind_rows(glance(ols1), glance(ols2), glance(ols3))\n\n\n# A tibble: 3 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.204         0.193  31.7      19.4 4.88e-38    12 -4500. 9027. 9095.\n2     0.201         0.196  31.7      38.5 8.36e-42     6 -4501. 9018. 9057.\n3     0.248         0.241  30.8      37.7 5.93e-52     8 -4473. 8966. 9015.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nAn analysis of variance also suggests that the model fit has improved significantly.\n\n\nCode\nanova(ols2, ols3)\n\n\nAnalysis of Variance Table\n\nModel 1: Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + carebeds\nModel 2: Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + carebeds + \n    poly(density, 2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    917 921337                                  \n2    915 867305  2     54032 28.502 9.819e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHowever, there is still spatial autocorrelation left in the model residuals, albeit slightly reduced from before.\n\n\nCode\nlm.morantest(ols2, spweight)$estimate\n\n\nObserved Moran I      Expectation         Variance \n    0.4461249282    -0.0033794098     0.0004042296 \n\n\nCode\nlm.morantest(ols3, spweight)$estimate\n\n\nObserved Moran I      Expectation         Variance \n    0.4133475904    -0.0037610429     0.0004036032 \n\n\nThe map of the residuals now looks like:\n\n\nCode\ncovid$resids <- rstandard(ols3)\nbrks <- c(min(covid$resids), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$resids))\ncovid$resids_gp <- cut(covid$resids, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = resids_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"Standardised residual\", values = pal) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "spregress.html#spatial-regression-models",
    "href": "spregress.html#spatial-regression-models",
    "title": "Spatial Regression",
    "section": "Spatial regression models",
    "text": "Spatial regression models\n\nSpatial error model\nOne way to handle the error structure is to fit a spatial simultaneous autoregressive error model which decomposes the error (the residuals) into two parts: a spatially lagged component (the bit that allows for geographical clustering in the residuals) and a remaining error: \\(y = X\\beta + \\lambda W \\xi + \\epsilon\\). The model can be fitted using R’s spatialreg package.\n\n\nCode\nif(!(\"spatialreg\" %in% installed.packages()[,1])) install.packages(\"spatialreg\", dependencies = TRUE)\nrequire(spatialreg)\nerrmod <- errorsarlm(formula(ols3), data = covid, listw = spweight)\nsummary(errmod)\n\n\n\nCall:errorsarlm(formula = formula(ols3), data = covid, listw = spweight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-88.9057 -13.5631   1.0775  15.1597 108.7168 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                     Estimate  Std. Error z value  Pr(>|z|)\n(Intercept)        228.423031   13.194612 17.3118 < 2.2e-16\nIMD                 -0.595479    0.089535 -6.6508 2.915e-11\nage0.11              1.885127    0.449453  4.1943 2.738e-05\nage25.34             2.612570    0.416075  6.2791 3.406e-10\nage35.39             3.729976    1.369957  2.7227  0.006475\nage50.59             3.339294    0.608468  5.4880 4.064e-08\ncarebeds             0.051339    0.010181  5.0426 4.592e-07\npoly(density, 2)1 -111.989485   38.123934 -2.9375  0.003309\npoly(density, 2)2 -120.005334   28.931983 -4.1478 3.356e-05\n\nLambda: 0.69751, LR test value: 348.94, p-value: < 2.22e-16\nAsymptotic standard error: 0.028893\n    z-value: 24.142, p-value: < 2.22e-16\nWald statistic: 582.82, p-value: < 2.22e-16\n\nLog likelihood: -4298.755 for error model\nML residual variance (sigma squared): 572.93, (sigma: 23.936)\nNumber of observations: 924 \nNumber of parameters estimated: 11 \nAIC: 8619.5, (AIC for lm: 8966.5)\n\n\nThe spatial component, \\(\\lambda\\), the spatial autocorrelation in the residuals, is significant as a number of test statistics that are with it in the summary above show. What it confirms is what we could interpret from the earlier Moran’s test of the regression residuals: having allowed for the variables that help to predict the COVID-rates there is still an unexplained geographic pattern whereby places for which the model over-predict the rate tend to be surrounded by other places where it does the same, and places where it under-predicts are surrounded by other under-predictions. The spatial error model gives a better fit to the data than the standard regression, as the following diagnostics tell us (the lower the AIC the better)\n\n\nCode\nglance(ols3)$r.squared\n\n\n[1] 0.2479886\n\n\nCode\nglance(errmod)$r.squared\n\n\n[1] 0.5543411\n\n\nCode\nAIC(ols3)\n\n\n[1] 8966.455\n\n\nCode\nAIC(errmod)\n\n\n[1] 8619.511\n\n\nThe differences are such that there is little doubt that the error model offers a much improved fit but if we do wish to test that difference statistically then\n\n\nCode\nlogLik(ols3)\n\n\n'log Lik.' -4473.228 (df=10)\n\n\nCode\nlogLik(errmod)\n\n\n'log Lik.' -4298.755 (df=11)\n\n\nCode\ndegf <- attr(logLik(errmod), \"df\") - attr(logLik(ols3), \"df\")\nLR <- -2 * (logLik(ols3) - logLik(errmod))\nLR > qchisq(0.99, degf)\n\n\n[1] TRUE\n\n\nUsing the error model changes the estimates of the regression coefficients.\n\n\nCode\ntidy(ols3)\n\n\n# A tibble: 9 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        185.       15.2       12.2   7.36e-32\n2 IMD                 -0.338     0.0941    -3.59  3.45e- 4\n3 age0.11              1.51      0.519      2.92  3.61e- 3\n4 age25.34             3.15      0.464      6.80  1.87e-11\n5 age35.39             3.96      1.56       2.55  1.11e- 2\n6 age50.59             5.67      0.696      8.15  1.15e-15\n7 carebeds             0.0290    0.0138     2.10  3.56e- 2\n8 poly(density, 2)1   23.8      43.1        0.553 5.81e- 1\n9 poly(density, 2)2 -267.       35.5       -7.54  1.11e-13\n\n\nCode\ntidy(errmod)\n\n\n# A tibble: 10 × 5\n   term               estimate std.error statistic  p.value\n   <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)        228.       13.2        17.3  0       \n 2 IMD                 -0.595     0.0895     -6.65 2.91e-11\n 3 age0.11              1.89      0.449       4.19 2.74e- 5\n 4 age25.34             2.61      0.416       6.28 3.41e-10\n 5 age35.39             3.73      1.37        2.72 6.48e- 3\n 6 age50.59             3.34      0.608       5.49 4.06e- 8\n 7 carebeds             0.0513    0.0102      5.04 4.59e- 7\n 8 poly(density, 2)1 -112.       38.1        -2.94 3.31e- 3\n 9 poly(density, 2)2 -120.       28.9        -4.15 3.36e- 5\n10 lambda               0.698     0.0289     24.1  0       \n\n\nFor example, where the estimate for age50.59, which is the estimate of the effect size, used to be 5.67, now it is 3.34. The 95% confidence intervals for those and the other coefficient estimates change too:\n\n\nCode\nconfint(ols3)\n\n\n                          2.5 %        97.5 %\n(Intercept)        1.555316e+02  215.12202256\nIMD               -5.225593e-01   -0.15331307\nage0.11            4.956890e-01    2.53253071\nage25.34           2.244363e+00    4.06480619\nage35.39           9.064758e-01    7.01079426\nage50.59           4.308470e+00    7.03988250\ncarebeds           1.958051e-03    0.05604307\npoly(density, 2)1 -6.079177e+01  108.44040596\npoly(density, 2)2 -3.369641e+02 -197.81320842\n\n\nCode\nconfint(errmod)\n\n\n                          2.5 %       97.5 %\nlambda               0.64088619   0.75414281\n(Intercept)        202.56206649 254.28399509\nIMD                 -0.77096349  -0.41999361\nage0.11              1.00421453   2.76603896\nage25.34             1.79707796   3.42806128\nage35.39             1.04490945   6.41504355\nage50.59             2.14672005   4.53186895\ncarebeds             0.03138441   0.07129323\npoly(density, 2)1 -186.71102210 -37.26794826\npoly(density, 2)2 -176.71097755 -63.29968970\n\n\n\n\nSpatially lagged y model\nAlthough the spatial error model fits the data better than the standard regression model, it tells us only that there is an unexplained spatial structure to the residuals, not what caused it. It may offer better estimates of the model parameters and their statistical significance but it does not presuppose any particular spatial process generating the patterns in the values. A different model that explicitly tests for whether the value at a location is functionally dependent on the values of neighbouring location is the spatially lagged y model: \\(y = \\rho Wy + X\\beta + \\epsilon\\). It models an ‘overspill’ or chain effect where the outcome (the Y value) at any location is affected by the outcomes at surrounding locations.\n\n\nCode\nlagmod <- lagsarlm(formula(ols3), data = covid, listw = spweight)\nsummary(lagmod)\n\n\n\nCall:lagsarlm(formula = formula(ols3), data = covid, listw = spweight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-98.8140 -14.8770   1.0467  14.7420 110.1081 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n                     Estimate  Std. Error z value  Pr(>|z|)\n(Intercept)         17.353050   14.403449  1.2048 0.2282866\nIMD                 -0.371501    0.074903 -4.9598 7.057e-07\nage0.11              1.377790    0.411239  3.3503 0.0008071\nage25.34             2.678105    0.367344  7.2905 3.089e-13\nage35.39             1.885234    1.235376  1.5260 0.1269996\nage50.59             3.636195    0.553621  6.5680 5.099e-11\ncarebeds             0.042747    0.010887  3.9264 8.623e-05\npoly(density, 2)1  -69.714325   34.062239 -2.0467 0.0406900\npoly(density, 2)2 -156.910145   28.073145 -5.5893 2.279e-08\n\nRho: 0.63167, LR test value: 342.73, p-value: < 2.22e-16\nAsymptotic standard error: 0.02966\n    z-value: 21.297, p-value: < 2.22e-16\nWald statistic: 453.57, p-value: < 2.22e-16\n\nLog likelihood: -4301.863 for lag model\nML residual variance (sigma squared): 591.59, (sigma: 24.323)\nNumber of observations: 924 \nNumber of parameters estimated: 11 \nAIC: 8625.7, (AIC for lm: 8966.5)\nLM test for residual autocorrelation\ntest value: 3.5747, p-value: 0.058666\n\n\nIts test for residual autocorrelation does not generate a statistically significant result at a 95% level, meaning there is no statistically significant spatial structure now left in the residuals, although it is close.\nNote that the beta estimates of the lagged y-model cannot be interpreted in the same way as for a standard regression model. For example, the beta estimate of -0.372 for the IMD variable does not mean that if (hypothetically) we increased that variable by one unit at each location we should then expect the COVID-19 to everywhere decrease by 0.372 holding the other X variables constant. That is the correct interpretation for a standard (OLS) regression model and also for the spatial error model but not for where the lag of the Y variable is included as a predictor variable. The reason is because if we did raise the IMD value it would start something akin to a ‘chain reaction’ through the feedback of Y via the lagged Y values: the (hypothetical) increase in deprivation at the one location, decreases the COVID-19 rate at neighbouring locations, which decrease the rate at their neighbours and so forth. The total impact is a sum of the direct effect – that predicted to happen through the 1 unit change in IMD – and the indirect effect, which is that caused by ‘the chain reaction’ / feedback / ‘overspill’ in the system:\n\n\nCode\nimpacts(lagmod, listw = spweight)\n\n\nImpact measures (lag, exact):\n                        Direct     Indirect       Total\nIMD                 -0.4123766   -0.5962428   -1.008619\nage0.11              1.5293852    2.2112917    3.740677\nage25.34             2.9727718    4.2982407    7.271013\nage35.39             2.0926626    3.0257175    5.118380\nage50.59             4.0362780    5.8359322    9.872210\ncarebeds             0.0474502    0.0686068    0.116057\npoly(density, 2)1  -77.3848558 -111.8884195 -189.273275\npoly(density, 2)2 -174.1746600 -251.8338663 -426.008526\n\n\nAlthough it wasn’t especially noticeable, there was a short pause as those impacts were calculated. The calculations could take much longer if the size of the spatial weights matrix was larger. As this author notes, after Lesage and Pace, 2009, a faster approximation method can be used, here with R = 1000 simulated distributions for the impact measures.\n\n\nCode\nW <- as(spweight, \"CsparseMatrix\")\ntrMC <- trW(W, type = \"MC\")\nim <-summary(impacts(lagmod, tr = trMC, R = 1000), zstats = TRUE)\ndata.frame(im$res, row.names = names(lagmod$coefficients)[-1])\n\n\n                         direct      indirect        total\nIMD                 -0.41253815   -0.59608012   -1.0086183\nage0.11              1.52998451    2.21068851    3.7406730\nage25.34             2.97393676    4.29706823    7.2710050\nage35.39             2.09348266    3.02489210    5.1183748\nage50.59             4.03785978    5.83434025    9.8722000\ncarebeds             0.04746879    0.06858809    0.1160569\npoly(density, 2)1  -77.41518121 -111.85789817 -189.2730794\npoly(density, 2)2 -174.24291529 -251.76517024 -426.0080855\n\n\nCode\n                                          # The [-1] is to omit the intercept\n\n\nAn advantage of this approach is that we can also obtain z and p-values for the impact measures; i.e. measures of statistical significance.\n\n\nCode\ndata.frame(im$zmat, row.names = names(lagmod$coefficients)[-1])\n\n\n                     Direct  Indirect     Total\nIMD               -5.224707 -4.543510 -4.955190\nage0.11            3.352826  3.125896  3.264302\nage25.34           7.455163  5.550816  6.496947\nage35.39           1.569590  1.557996  1.569911\nage50.59           6.754634  5.527799  6.268183\ncarebeds           3.702060  3.358888  3.553594\npoly(density, 2)1 -1.957001 -1.897208 -1.933039\npoly(density, 2)2 -5.494869 -4.688478 -5.167561\n\n\nCode\ndata.frame(im$pzmat, row.names = names(lagmod$coefficients)[-1])\n\n\n                        Direct     Indirect        Total\nIMD               1.744313e-07 5.532518e-06 7.225959e-07\nage0.11           7.999084e-04 1.772644e-03 1.097343e-03\nage25.34          8.970602e-14 2.843400e-08 8.196599e-11\nage35.39          1.165106e-01 1.192342e-01 1.164358e-01\nage50.59          1.431966e-11 3.242729e-08 3.652847e-10\ncarebeds          2.138561e-04 7.825687e-04 3.800053e-04\npoly(density, 2)1 5.034731e-02 5.780055e-02 5.323143e-02\npoly(density, 2)2 3.910014e-08 2.752441e-06 2.371682e-07\n\n\nMost but not all of the impacts are significant at, say, a 95% confidence (i.e p < 0.05). We can drop age35.39 from the model with little loss of fit.\n\n\nCode\nlagmod2 <- lagsarlm(Rate ~  IMD + age0.11 + age25.34 + age50.59 + carebeds + \n    poly(density, 2), data = covid, listw = spweight)\nanova(lagmod, lagmod2)\n\n\n        Model df    AIC  logLik Test L.Ratio p-value\nlagmod      1 11 8625.7 -4301.9    1                \nlagmod2     2 10 8626.1 -4303.0    2  2.3408 0.12602\n\n\nCode\nbind_rows(glance(lagmod), glance(lagmod2))\n\n\n# A tibble: 2 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      <dbl> <dbl> <dbl>    <dbl>  <dbl> <int>\n1     0.532 8626. 8679.  546629. -4302.   924\n2     0.532 8626. 8674.  547460. -4303.   924\n\n\n In fact, an increase of 1 unit in, for example, the IMD variable, will play out slightly differently in different places because they have different neighbours with different Y values and are at different distances from each other. The code below, which is from the first edition of Ward & Gleditsch (2008, pp.47), will calculate the impact (of a one unit change in IMD) at each location but below I have limited it to the first ten so that it doesn’t take too long to run.\n\n\nCode\nn <- nrow(covid)\nI <- matrix(0, nrow = n, ncol = n)\ndiag(I) <- 1\nrho <- lagmod$rho\nbeta <- lagmod$coefficients[\"age25.34\"]\nweights.matrix <- listw2mat(spweight)\nresults <- rep(NA, times=10)\nresults <- sapply(1:10, \\(i) {\n  xvector <- rep(0, times=n)\n  xvector[i] <- 1\n  impact <- solve(I - rho * weights.matrix) %*% xvector * beta\n  results[i] <- impact[i]\n})\nresults\n\n\n [1] 2.953840 2.973231 2.949515 2.908907 3.025483 2.901597 2.949316 2.960332\n [9] 2.949171 2.946591\n\n\n\n\nChoosing between the spatial error and lagged y model\nBefore fitting the spatial error and lagged y models, we could have looked for evidence in support of them using the function lm.LMtests(). This tests the basic OLS specification against the more general spatial error and lagged y models. Anselin and Rey (2014, p.110) offer the following decision tree that can, in the absence of a more theoretical basis for the model choice (e.g. the type of process being modelled), be used in conjunction with the test results to help select the model.\n\nSource: Modern Spatial Econometrics in Practice\nIn the first step, we find that in this instance both the LM-Error and LM-Lag tests are significant.\n\n\nCode\nols4 <- lm(Rate ~  IMD + age0.11 + age25.34 + age50.59 + carebeds + poly(density, 2), data = covid)\nlm.LMtests(ols4, spweight, test=c(\"LMerr\", \"LMlag\"))\n\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nLMerr = 411.48, df = 1, p-value < 2.2e-16\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nLMlag = 463.85, df = 1, p-value < 2.2e-16\n\n\nMoving on to the robust tests, only the LM-Lag test is significant. Given the nature of COVID-19 as an infectious disease, it does seem reasonable to suppose that high rates of infection in any an area will ‘overspill’ into neighbouring areas too.\n\n\nCode\nlm.LMtests(ols4, spweight, test=c(\"RLMerr\", \"RLMlag\"))\n\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nRLMerr = 2.3319, df = 1, p-value = 0.1267\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nRLMlag = 54.693, df = 1, p-value = 1.409e-13"
  },
  {
    "objectID": "spregress.html#a-geographically-weighted-spatial-weights-matrix",
    "href": "spregress.html#a-geographically-weighted-spatial-weights-matrix",
    "title": "Spatial Regression",
    "section": "A geographically weighted spatial weights matrix",
    "text": "A geographically weighted spatial weights matrix\nAs with the tests of spatial autocorrelation in an earlier session and as with the geographically weighted statistics, the results of the spatial regression models are dependent on the specification of the spatial weights matrix which can suffer from being somewhat arbitrary. We could, if we wish, try calibrating it around the geographically weighted mean.\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(Rate ~ 1, data = covid_sp, adaptive = TRUE, kernel = \"bisquare\")\n\n\nTo now create the corresponding inverse distance spatial weights matrix for the models, we can use the nn2 function in the RANN package to find the $bw = $ 18 nearest neighbours to each centroid point:\n\n\nCode\nif(!(\"RANN\" %in% installed.packages()[,1]))  install.packages(\"RANN\")\nrequire(RANN)\ncoords <- st_coordinates(st_centroid(st_geometry(covid)))\nknn <- nn2(coords, coords, k = bw)\n\n\nHaving done so, those neighbours can be geographically weighted using a bisquare kernel, as in the GWR calibration above. (A Gaussian kernel is obtained if (1 - (x / max(x))^2)^2 is replaced by exp(-0.5 * (x / max(x))^2) in the code below).\n\n\nCode\nd <- knn$nn.dists\nglist <- apply(d, 1, \\(x) {\n  (1 - (x / max(x))^2)^2\n}, simplify = FALSE)\nknearnb <- knn2nb(knearneigh(st_centroid(st_geometry(covid)), k = bw))\nspweightK <- nb2listw(knearnb, glist, style = \"C\")\n\n\nThe results are not very successful though. The lag model with contiguous spatial weights fits the data better,\n\n\nCode\nlagmod3 <- lagsarlm(formula(ols4), data = covid, listw = spweightK)\nbind_rows(glance(lagmod2), glance(lagmod3))\n\n\n# A tibble: 2 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      <dbl> <dbl> <dbl>    <dbl>  <dbl> <int>\n1     0.532 8626. 8674.  547460. -4303.   924\n2     0.254 8959. 9008.  860562. -4470.   924\n\n\nIn addition, the inverse distance weights are not row-standardised, which may cause some knock-on problems. They could become row-standardised (spweightK <- nb2listw(knearnb, glist, style = \"W\")) but if they are, by re-scaling each row of the weights to equal one, then the inverse distance weighting is altered."
  },
  {
    "objectID": "spregress.html#geographically-weighted-regression",
    "href": "spregress.html#geographically-weighted-regression",
    "title": "Spatial Regression",
    "section": "Geographically Weighted Regression",
    "text": "Geographically Weighted Regression\nThe following charts shows the simple, bivariate regression relationship between Rate and age0.11.\n\n\nCode\nggplot(covid, aes(x = age0.11, y = Rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nIntriguingly, it doesn’t seem to be linear relationship but a polynomial one.\n\n\nCode\nggplot(covid, aes(x = age0.11, y = Rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2))\n\n\n\n\n\nIt could be that the relationship is indeed curved but it might also be that the nature of the relationship is spatially dependent – that the relationship between Rate and age0.11 depends upon where it is measured. A coplot() from R’s base graphics lends weight to the second idea: note how the regression relationship changes with X and Y, which are the centroids of the neighbourhoods in the North West of England.\n\n\nCode\ncoplot_df <- data.frame(st_coordinates(st_centroid(covid)), Rate = covid$Rate, age0.11 = covid$age0.11)\nPointsWithLine = function(x, y, ...) {\n    points(x=x, y=y, pch=20, col=\"black\")\n    abline(lm(y ~ x))\n}\ncoplot(Rate ~ age0.11 | X * Y, data = coplot_df, panel = PointsWithLine, overlap = 0)\n\n\n\n\n\nImagine, now, that instead of dividing the geography of the North West’s neighbourhoods into a \\(6 \\times 6\\) grid, as in the coplot, we instead go from location to location within the study region, calculating a geographically weighted regression at and around each one of them. This builds on the idea of geographically weighted statistics that allow the measured value of the statistic to vary locally from location to location across the study region; with geographically weighted it is the locally estimated regression coefficients that are allowed to vary. These are used to look for spatial variation in the regression relationships.\nIf the bandwidth is not known or there is no theoretical justification for it, it can be found using an automated selection procedure, here with a more complex model than the bivariate relationship shown in the coplot, instead using all the variables included in the choice between the spatial error and lagged y model, above.\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(formula(ols4), data = covid_sp, adaptive = TRUE)\n\n\nThe model can then be fitted.\n\n\nCode\ngwrmod <- gwr.basic(formula(ols4), data = covid_sp, adaptive = TRUE, bw = bw)\ngwrmod\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-07 15:15:36 \n   Call:\n   gwr.basic(formula = formula(ols4), data = covid_sp, bw = bw, \n    adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density\n   Number of data points: 924\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-123.194  -18.380    1.473   20.623  127.554 \n\n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)        188.50375   15.17560  12.422  < 2e-16 ***\n   IMD                 -0.39649    0.09149  -4.334 1.63e-05 ***\n   age0.11              2.30191    0.41777   5.510 4.66e-08 ***\n   age25.34             3.93517    0.34898  11.276  < 2e-16 ***\n   age50.59             5.80810    0.69596   8.345 2.59e-16 ***\n   carebeds             0.02904    0.01382   2.101   0.0359 *  \n   poly(density, 2)1   29.14870   43.19300   0.675   0.4999    \n   poly(density, 2)2 -292.74904   34.12420  -8.579  < 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 30.88 on 916 degrees of freedom\n   Multiple R-squared: 0.2427\n   Adjusted R-squared: 0.2369 \n   F-statistic: 41.93 on 7 and 916 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 873446.4\n   Sigma(hat): 30.77887\n   AIC:  8970.975\n   AICc:  8971.172\n   BIC:  8151.892\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 80 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                            Min.     1st Qu.      Median     3rd Qu.     Max.\n   Intercept         -3.9934e+01  1.3693e+02  2.1422e+02  2.6180e+02 419.5064\n   IMD               -2.7499e+00 -7.8059e-01 -4.4223e-01 -1.6702e-01   0.6501\n   age0.11           -4.0541e+00  2.3759e-01  3.4220e+00  6.1701e+00  12.5921\n   age25.34          -3.9387e+00  1.6469e+00  2.6274e+00  4.2843e+00  13.8157\n   age50.59          -9.5346e+00  2.1727e+00  5.3255e+00  8.6233e+00  14.3515\n   carebeds          -5.6126e-02  2.5983e-02  5.8662e-02  8.7463e-02   0.2220\n   poly(density, 2)1 -2.7204e+03 -4.1814e+02 -1.5602e+02  4.1180e-01 667.9061\n   poly(density, 2)2 -2.1980e+03 -5.4619e+02 -2.5300e+02 -1.2247e+02 643.0451\n   ************************Diagnostic information*************************\n   Number of data points: 924 \n   Effective number of parameters (2trace(S) - trace(S'S)): 270.2441 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 653.7559 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 8552.959 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 8216.682 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 8513.32 \n   Residual sum of squares: 313860.8 \n   R-square value:  0.7278617 \n   Adjusted R-square value:  0.6151951 \n\n   ***********************************************************************\n   Program stops at: 2023-03-07 15:15:36 \n\n\nThe output requires some interpretation! The results of the global regression are those obtained from a standard OLS regression (i.e. summary(lm(formula(ols4), data = covid_sp))). It is ‘global’ because it is a model for the entire study region, without spatial variation in the regression estimates. The results of the geographically weighted regression are the results from, in this case, 924 separate but spatially overlapping regression models fitted, with geographic weighting, to spatial subsets of the data. All the local estimates are contained in the spatial data frame, gwrmod$SDF, including those for age0.11 which are in gwrmod$SDF$age0.11 and have the following distribution,\n\n\nCode\nsummary(gwrmod$SDF$age0.11)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-4.0541  0.2376  3.4220  3.3645  6.1701 12.5921 \n\n\nThis is the same summary of the distribution that is included in the GWR coefficient estimates. What it means is that in at least one location the relationship of age0.11 to Rate is, conditional on the other variables, negative. It is more usual for it to be positive – more children, more infections (perhaps a school effect?) – but the estimated effect size varies quite substantially across the locations in the study region. Its interquartile range is from 0.238 to 6.17. Mapping these local estimates reveals an interesting geography – it is largely in and around Manchester where a greater number of children of primary school age or younger appears to lower instead of raising the infection rate.\n\n\nCode\ncovid$age0.11GWR <- gwrmod$SDF$age0.11\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nWith a bit of effort we can get all the variables on the same chart…\n\n\nCode\nif(!(\"gridExtra\" %in% installed.packages()[,1])) install.packages(\"gridExtra\",\n                                                          dependencies = TRUE)\nrequire(gridExtra)\ng <- lapply(2:8, \\(j) {\n  st_as_sf(gwrmod$SDF[, j]) %>%\n    rename(beta = 1) %>%\n    ggplot(aes(fill = beta)) +\n      geom_sf(col = NA) +\n      scale_fill_gradient2(\"beta\", mid = \"grey90\", trans = \"reverse\") +\n      theme_void() +\n      guides(fill = guide_colourbar(reverse = TRUE)) +\n      ggtitle(names(gwrmod$SDF[j]))\n})\ngrid.arrange(grobs = g)\n\n\n\n\n\n… however, not all of these regression estimates are necessarily statistically significant. If we use the estimated t-values to isolate those that are not (t-values less than -1.96 or greater than +1.96 can be considered significant at a 95% confidence), then, for the age0.11 variable we obtain,\n\n\nCode\ncovid$age0.11GWR[abs(gwrmod$SDF$age0.11_TV) < 1.96] <- NA\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\",\n                       trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nand, with a bit more data wrangling, for the full set,\n\n\nCode\nsdf <- slot(gwrmod$SDF, \"data\")\ng <- lapply(2:8, \\(j) {\n  x <- st_as_sf(gwrmod$SDF[, j]) \n  t <- which(names(sdf) == paste0(names(x)[1],\"_TV\"))\n  x[abs(sdf[, t]) < 1.96, 1] <- NA\n  x %>%\n    rename(beta = 1) %>%\n    ggplot(aes(fill = beta)) +\n      geom_sf(col = NA) +\n      scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\",\n                           trans = \"reverse\") +\n      theme_void() +\n      guides(fill = guide_colourbar(reverse = TRUE)) +\n      ggtitle(names(gwrmod$SDF[j]))\n})\ngrid.arrange(grobs = g)\n\n\n\n\n\n\nI am sure there is a more elegant way of doing this. Don’t worry about the code especially - the point is that not every estimate, everywhere, is significant.\n As with the basic GW statistics, we can undertake a randomisation test to suggest whether the spatial variation in the coefficient estimates is statistically significant. However this takes some time to run – with the default nsims = 99 simulations it took about 12 minutes on my laptop. I have ‘commented it out’ in the code block below with the suggestion that you don’t run it now.\n\n\nCode\n# I suggest you do not run this\n# gwr.mc <- gwr.montecarlo(formula(ols4), covid_sp, adaptive = TRUE, bw = bw)\n\n\nInstead, let’s jump straight to the results, which I have saved in a pre-prepared workspace.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/gwrmc.RData\")\nload(url)\nclose(url)\ngwr.mc\n\n\n                  p-value\n(Intercept)          0.40\nIMD                  0.38\nage0.11              0.00\nage25.34             0.15\nage50.59             0.22\ncarebeds             0.97\npoly(density, 2)1    0.00\npoly(density, 2)2    0.00\n\n\n\nMixed GWR\nIt appears that only the age0.11 and density variables exhibit significant spatial variation (p-value < 0.05). In principle it is possible to drop the other variables out from the local estimates and treat them as fixed, global estimates instead, in a Mixed GWR model (mixed global and local regression estimates). In practice, it is generating an error.\n\n\nCode\n# This appears to generate an error\ngwrmixd <- gwr.mixed(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds +\n                       poly(density, 2),\n          data = covid_sp,\n          fixed.vars = c(\"IMD\", \"age25.34\", \"age50.59\", \"carebeds\"),\n          bw = bw, adaptive = TRUE)\n\n\nWe can work around this, though, using partial regressions, regressing Rate, age0.11 and poly(density, 2) on the fixed variables and using their residuals.\n\n\nCode\ndensity.1 <- poly(covid$density, 2)[, 1]\ndensity.2 <- poly(covid$density, 2)[, 2]\ncovid_sp$RateR <- residuals(lm(Rate ~ IMD + age25.34 + age50.59 + carebeds,\n                               data = covid_sp))\ncovid_sp$age0.11R <- residuals(lm(age0.11 ~ IMD + age25.34 + age50.59 +\n                                    carebeds, data = covid_sp))\ncovid_sp$density.1R <- residuals(lm(density.1 ~ IMD + age25.34 + age50.59 +\n                                    carebeds, data = covid_sp))\ncovid_sp$density.2R <- residuals(lm(density.2 ~ IMD + age25.34 + age50.59 +\n                                    carebeds, data = covid_sp))\nfml <- formula(RateR ~ age0.11R + density.1R + density.2R)\nbw <- bw.gwr(fml, data = covid_sp, adaptive = TRUE)\ngwrmixd <- gwr.basic(fml, data = covid_sp, bw = bw, adaptive = TRUE)\n\n\nHere are the statistically significant results for the age0.11 variable.\n\n\nCode\ncovid$age0.11GWR <- gwrmixd$SDF$age0.11R\ncovid$age0.11GWR[abs(gwrmixd$SDF$age0.11R_TV) > 1.96] <- NA\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\n\n\nMultiscale GWR\nIt is also possible to fit a Multiscale GWR, which allows for the possibility of a different bandwith for each variable – see ?gwr.multiscale. Since there is no particular reason to presume that the bandwidth should be the same for all the variables, arguably Multiscale GWR should be preferred over standard GWR, at least in the first instance. Its main drawback, however, is that it takes a while to run – over 3 hours for the code chunk below! (In principle it can be parallelised but it generated an error for me when I tried this with parallel.method = cluster.)\n\n\nCode\ndensity.1 <- poly(covid$density, 2)[, 1]\ndensity.2 <- poly(covid$density, 2)[, 2]\n# Do not run!\n# Took over 3 hours!\n#gwrmulti <- gwr.multiscale(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds +\n# density.1 + density.2, data = covid_sp, adaptive = TRUE)\n\n\nTo save time, the results are available below. The bandwidths do appear to vary.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/gwrmulti.RData\")\nload(url)\nclose(url)\ngwrmulti\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-08-21 14:44:09 \n   Call:\n   gwr.multiscale(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 + \n    carebeds + density.1 + density.2, data = covid_sp, adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density.1 density.2\n   Number of data points: 924\n   ***********************************************************************\n   *                       Multiscale (PSDM) GWR                          *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidths for each coefficient(number of nearest neighbours): \n              (Intercept) IMD age0.11 age25.34 age50.59 carebeds density.1\n   Bandwidth           28 922      34      922      132      841       380\n              density.2\n   Bandwidth        186\n\n   ****************Summary of GWR coefficient estimates:******************\n                    Min.     1st Qu.      Median     3rd Qu.     Max.\n   Intercept   40.390229  158.475456  197.569654  250.106703 341.7481\n   IMD         -0.457266   -0.429212   -0.427347   -0.425449  -0.4210\n   age0.11     -4.383024   -0.168031    3.233327    5.482906  15.0608\n   age25.34     2.970842    2.980326    2.990406    2.999560   3.0909\n   age50.59    -1.540099    3.976661    4.868116    6.199428  10.9802\n   carebeds     0.041884    0.045506    0.050615    0.056445   0.0873\n   density.1 -228.739192 -171.871215 -117.550561   -6.118907  67.3146\n   density.2 -639.370049 -216.959473 -130.236970  -77.397916  26.1626\n   ************************Diagnostic information*************************\n   Number of data points: 924 \n   Effective number of parameters (2trace(S) - trace(S'S)): 227.0379 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 696.9621 \n   AICc value:  8395.1 \n   AIC value:  8142.156 \n   BIC value:  8216.88 \n   Residual sum of squares:  301719.5 \n   R-square value:  0.738389 \n   Adjusted R-square value:  0.6530458 \n\n   ***********************************************************************\n   Program stops at: 2022-08-21 17:46:34 \n\n\n\n\nGeographically and temporally weighted regression\nIt also possible to add a time dimension and undertake geographically and temporally weighted regression in GWmodel. The following workspace loads monthly COVID-19 data for Manchester neighbourhoods.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/manchester.RData\")\nload(url)\nclose(url)\n\n\nHere are the monthly rates.\n\n\nCode\nggplot(covid, aes(fill = Rate )) +\n  geom_sf() +\n  facet_wrap(~ month) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme_void()\n\n\n\n\n\nIf now want to explore how the regression relationships vary in space-time then we can use the bw.gtwr() and gtwr() functions, for which we also need to specify a vector of time tags (obs.tv = ...). In these data, they are simply a numeric value, covid$month_code but see ?bw.gtwr() and ?gtwr() for further possibilities and for further information about the model.\n\nThe model will take a little time to run – enough time for you to get a cup of tea or coffee!\n\n\nCode\ncovid_sp <- as_Spatial(covid)\nfml <- formula(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds + poly(density, 2))\nbw <- bw.gtwr(fml, data = covid_sp, obs.tv = covid$month_code, adaptive = TRUE)\ngtwrmod <- gtwr(fml, data = covid_sp, st.bw = bw, obs.tv = covid$month_code, adaptive = TRUE)\n\n\nHere are the summary results of the model,\n\n\nCode\ngtwrmod\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-08-21 09:11:51 \n   Call:\n   gtwr(formula = fml, data = covid_sp, obs.tv = covid$month_code, \n    st.bw = bw, adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density\n   Number of data points: 1768\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-3.8388 -2.4526 -0.6694  0.8773 23.0668 \n\n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)   \n   (Intercept)        1.2277082  0.8772151   1.400  0.16182   \n   IMD                0.0022343  0.0088457   0.253  0.80062   \n   age0.11           -0.0311138  0.0307405  -1.012  0.31161   \n   age25.34           0.0316351  0.0174764   1.810  0.07044 . \n   age50.59           0.1492102  0.0483997   3.083  0.00208 **\n   carebeds           0.0009431  0.0014616   0.645  0.51886   \n   poly(density, 2)1  8.4668105  5.2762201   1.605  0.10874   \n   poly(density, 2)2 -5.6870143  3.6957704  -1.539  0.12404   \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 3.542 on 1760 degrees of freedom\n   Multiple R-squared: 0.007583\n   Adjusted R-squared: 0.003636 \n   F-statistic: 1.921 on 7 and 1760 DF,  p-value: 0.06261 \n   ***Extra Diagnostic information\n   Residual sum of squares: 22079.68\n   Sigma(hat): 3.535908\n   AIC:  9499.228\n   AICc:  9499.331\n   ***********************************************************************\n   *    Results of Geographically and Temporally Weighted Regression     *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function for geographically and temporally weighting: bisquare \n   Adaptive bandwidth for geographically and temporally  weighting: 606 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric for geographically and temporally  weighting: A distance matrix is specified for this model calibration.\n\n   ****************Summary of GTWR coefficient estimates:*****************\n                              Min.       1st Qu.        Median       3rd Qu.\n   Intercept          -7.467273452  -1.429007024  -0.431890521   0.188930147\n   IMD                -0.060866478   0.000867536   0.006925644   0.016466529\n   age0.11            -0.263923302  -0.041615611   0.007456504   0.032878161\n   age25.34           -0.081155344   0.017377385   0.030977770   0.063555110\n   age50.59           -0.284989530   0.110990723   0.159690984   0.238809884\n   carebeds           -0.008797590   0.000070766   0.000863682   0.001820529\n   poly(density, 2)1 -22.971393286   5.633766857  12.761674565  22.069777132\n   poly(density, 2)2 -32.065614018  -8.663319026  -6.150064261  -3.539132987\n                        Max.\n   Intercept          6.5631\n   IMD                0.0631\n   age0.11            0.1432\n   age25.34           0.2475\n   age50.59           1.0677\n   carebeds           0.0066\n   poly(density, 2)1 94.7435\n   poly(density, 2)2 38.3984\n   ************************Diagnostic information*************************\n   Number of data points: 1768 \n   Effective number of parameters (2trace(S) - trace(S'S)): 57.81766 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1710.182 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 9304.009 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 9247.793 \n   Residual sum of squares: 18798.49 \n   R-square value:  0.1550625 \n   Adjusted R-square value:  0.1264803 \n\n   ***********************************************************************\n   Program stops at: 2022-08-21 09:13:00 \n\n\nThey should be treated cautiously because the Rate value is strongly positively skewed such that it may have been better to have transformed it (e.g. taken its square root) prior to analysis. Even so, there is some evidence that the predictor age0.11 variable, for example, varies spatially and temporally.\n\n\nCode\ncovid$age0.11GWR <- gtwrmod$SDF$age0.11\ncovid$age0.11GWR[abs(gtwrmod$SDF$age0.11_TV) < 1.96] <- NA\nggplot(covid, aes(fill = age0.11GWR)) +\n  geom_sf() +\n  facet_wrap(~ month) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\n\n\nGeneralised GWR models with Poisson and Binomial options\nSee ?ggwr.basic."
  },
  {
    "objectID": "spregress.html#summary",
    "href": "spregress.html#summary",
    "title": "Spatial Regression",
    "section": "Summary",
    "text": "Summary\nIn this session we have looked at spatial regression models as a way to address the problem of spatial autocorrelation in regression residuals and to allow for the possibility that regression relationships vary across a map. In presenting these methods, at least some could be taken as offering a ‘technical fix’ to the statistical issue of spatial dependencies in the data, violating assumptions of independence. Whilst that may be true, and the more spatial approaches can be used as diagnostic tools to check for a mis-specified model, notably one that lacks a critical explanatory variable or where the relationship between the X and Y variables is non-linear, the limitation of this thinking is that it treats geography as an error to be resolved and/or it rests on the belief that with sufficient information the geographical differences will be explained. What geographically weighted regression, for example, reminds us is that the nature of the relationship may be complex because it is changing in form across the study region as the socio-spatial causes also vary from place to place. Such spatial complexity may be challenging to accommodate within the parameters of conventional aspatial or spatially naïve statistical thinking but ignoring the geographical variation is not a solution. What spatial thinking brings to the table is the recognition of geographical context and the knowledge that geographically rooted social outcomes are not independent of where the processes giving rise to those outcomes are taking place."
  },
  {
    "objectID": "spregress.html#further-reading",
    "href": "spregress.html#further-reading",
    "title": "Spatial Regression",
    "section": "Further Reading",
    "text": "Further Reading\n\nSpatial Regression Models (2nd edition) by MD Ward & KS Gleditsch (2018)\nComber et al. (2022) A Route Map for Successful Applications of Geographically Weighted Regression. https://doi.org/10.1111/gean.12316"
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Getting Started",
    "section": "",
    "text": "For this course we will be using R. R is a free software environment for statistical computing and graphics. To run the code blocks for this course on your own computer you will need to have installed R. This is available for Linux, MacOS and Windows."
  },
  {
    "objectID": "start.html#install-r-studio",
    "href": "start.html#install-r-studio",
    "title": "Getting Started",
    "section": "Install R Studio",
    "text": "Install R Studio\nRStudio is an integrated development environment (IDE) that can make programming and other tasks easier in R. An open source edition is available to download and install.\n\nYou need to install R before you install R Studio."
  },
  {
    "objectID": "start.html#open-r-studio",
    "href": "start.html#open-r-studio",
    "title": "Getting Started",
    "section": "Open R Studio",
    "text": "Open R Studio\nOnce R and R Studio are installed, open R Studio on your computer and type the following in the Console to the left or bottom left of the screen, alongside the prompt, >.\n\n\nCode\n1 + 1\n\n\nthen hit Enter/Return. You should, of course, obtain the answer 2, as below.\n\n\n[1] 2\n\n\nYou will also find that if you move your mouse to over the code block above, an option appears to copy the code to the clipboard."
  },
  {
    "objectID": "start.html#install-additional-librariespackages",
    "href": "start.html#install-additional-librariespackages",
    "title": "Getting Started",
    "section": "Install additional libraries/packages",
    "text": "Install additional libraries/packages\nThe base functions of R are greatly extended by the very many packages/libraries that have been developed for it. At the time of writing, there are 19275 of these on CRAN, which is the main repository for them. Many of these have been grouped into ‘tasks’ and topic areas, which can be viewed here.\nMost of the packages that will be needed for this course will be installed as they are needed. However, some will be used so regularly that we should install them now. Cut and paste the following code chunk into the Console and hit Enter/Return.\n\n\nCode\ninstall.packages(\"proxy\", dependencies = TRUE)\ninstall.packages(\"sf\", dependencies = TRUE)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n\n\n\nRun the code above even if you have the packages installed already so that you also have available all the packages that these depend upon and link to."
  },
  {
    "objectID": "start.html#changing-the-working-directory",
    "href": "start.html#changing-the-working-directory",
    "title": "Getting Started",
    "section": "Changing the working directory",
    "text": "Changing the working directory\nIf you type getwd() into the R Console you will obtain your current working directory – the default location to look for files and to save content to. Mine is,\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/profr/Dropbox/github/MandM\"\n\n\nYou may wish to change this to something else each time you start R. You can do this using the drop-down menus. There is also the function setwd(dir) – type ?setwd in the R Console to learn more."
  },
  {
    "objectID": "start.html#organising-your-files-in-a-project",
    "href": "start.html#organising-your-files-in-a-project",
    "title": "Getting Started",
    "section": "Organising your files in a project",
    "text": "Organising your files in a project\nYou could also create a new project in R by using File –> New Project… from the dropdown menus and create it either in a new directory (probably most sensible) or an existing one. There is nothing especially magical about a project in R. As stated here, “a project is simply a working directory designated with a .RProj file. When you open a project (using File/Open Project in RStudio or by double–clicking on the .Rproj file outside of R), the working directory will automatically be set to the directory that the .RProj file is located in.” However, it is that which makes it useful: when you open a project you know that you are going to be working in a specific folder on your computer which them becomes the default ‘container’ to save files to or to download them from.”\n\nIt would be a good idea to create a new project now which can then be the folder and working directory for this course and its contents."
  },
  {
    "objectID": "start.html#changing-the-appearance-of-r-studio",
    "href": "start.html#changing-the-appearance-of-r-studio",
    "title": "Getting Started",
    "section": "Changing the appearance of R Studio",
    "text": "Changing the appearance of R Studio\nYou may notice that I prefer a blue to a white screen when working in R. To change it to this, from the drop-down menus use Tools –> Global Options… -> Appearance and select Solarized Dark as the Editor theme. You may, of course, have your own preference."
  },
  {
    "objectID": "themap.html",
    "href": "themap.html",
    "title": "The Spatial Variable",
    "section": "",
    "text": "When we look at a map such as the following, which is a choropleth (or thematic) map showing the percentage of the population with no experience of schooling in each of the South African municipalities in 2011, one thing should be immediately obvious: the areas are shaded in a range of colours; they are not all the same. This is because the values that the colours represent vary across the country with some places having a greater percentage of their population without schooling than others. In this way, the map reveals and also visually represents the spatial (geographic) variation in the variable of interest. The map portrays the geographic pattern. Knowing something about the pattern might provide information about the processes that generated the pattern. At a minimum, it can reveal socio-spatial inequalities in an outcome of interest across a study region.\n\nIt is not surprising to find spatial variation. It is improbable that all the values would be the same. It is nearly always possible to find that some places have lower or higher values than others and to colour the map accordingly. Nevertheless, three characteristics of the spatial variation appear evident in the map.\n\nSpatial heterogeneity. This is the idea that the values typical in one part of the map are not typical in another. To put it simply, some parts of the map are shaded in blue whereas others are in red and those parts seem neither randomly nor regularly distributed because of…\nSpatial clustering. This is the idea that values found in one part of the map tend to be surrounded by similar values in neighbouring parts of the map. In other words, there are patches of blue and patches of red coloured areas on the map – blue tends be near blue and red tends to be near red. Another name for this is positive spatial autocorrelation: values tend to be more similar to nearby other values than they are to distant ones.\n\nEvidence of spatial clustering supports Waldo Tobler’s much cited ‘first law’ of geography: “everything is related to everything else, but near things are more related than distant things.” However, it isn’t really a law because it is by no means always true. If we look at the map, we can also see,\n\nSpatial discontinuities (negative spatial autocorrelation) because sometimes neighbouring places can have very different characteristics – there there can be sharp changes across borders (red next to blue).\n\nNevertheless, Tobler’s ‘law’ does suggest that places tend to be situated within broader spatial contexts such that the processes that both generate and are generated by those contexts have a spatial expression and root\nTaken together, these characteristics of spatial variation indicate spatial dependencies, whereby the measured attributes of one place are not independent of other places. This dependence has statistical consequences if assumptions of independence are violated. Of more substantive geographic interest is how they have arisen – which processes are they caused by or associated with? Why are places not all the same? Why is there a geographical pattern? Complicating the answers to these questions is that what we see in the map is not just a function of underlying social or other processes but also the ways the data are collected and the map constructed. For example, the geographic scale of the data and where the boundaries are drawn between places. This is the Modifiable Areal Unit Problem (MAUP)."
  },
  {
    "objectID": "themap.html#from-the-map-towards-models",
    "href": "themap.html#from-the-map-towards-models",
    "title": "The Spatial Variable",
    "section": "From the map towards models",
    "text": "From the map towards models\nWith the above questions in mind, we might imagine the map as a first stage in a process of geographical enquiry where what we do is look for and then quantify some of the geographical patterns in the data before beginning to model them and to look for correlates, associations and causes. Here the map is not simply a tool for visualising and communicating data, it is also a tool for exploring data and thinking geographically about them.\n\n\n\n\n\n\n In practice, the process of analysis is likely to involve greater cycling between the various stages. Nevertheless, there is a good argument for starting with the map.\n\n\n\n\n\n\nThe point is that the map serves both as a representation of ‘the spatial variable’ and as a tool for understanding it."
  },
  {
    "objectID": "themap.html#further-reading",
    "href": "themap.html#further-reading",
    "title": "The Spatial Variable",
    "section": "Further Reading",
    "text": "Further Reading\nThe Spatial Variable was the name of the inaugural lecture given by Ron Johnston, one of the most influential geographers of recent times, on his appointment as Professor at the University of Sheffield. A transcript and brief commentary on that lecture is available here and is highly recommended reading."
  },
  {
    "objectID": "thematicmaps.html",
    "href": "thematicmaps.html",
    "title": "Thematic maps in R",
    "section": "",
    "text": "There are lots of ways to produce maps in R. But, however, they are drawn, two things are usually needed to produce a choropleth map of the sort seen in previous sessions: some data and a map to join the data to. Once we have those, R offers plenty of options to produce quick or publication quality maps, which may have either static or dynamic content. The two we shall focus on are ggplot2 and tmap."
  },
  {
    "objectID": "thematicmaps.html#getting-started",
    "href": "thematicmaps.html#getting-started",
    "title": "Thematic maps in R",
    "section": "Getting Started",
    "text": "Getting Started\nAs before, if you are keeping all the files and outputs from these exercises together in an R Project (which is a good idea) then open that Project now.\n\nLoad the data\nLet’s begin with the easy bit and load the data, which are from http://superweb.statssa.gov.za. These includes the variable No_schooling which is the percentage of the population without schooling per South African municipality in 2011.\n\n\nCode\n# A quick check to see if the Tidyverse packages are installed...\ninstalled <- installed.packages()[,1]\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\",\n                                                   dependencies = TRUE)\nrequire(tidyverse)\n\neducation <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/education.csv\")\nslice_head(education, n = 3)\n\n\n# A tibble: 3 × 8\n  LocalMunicipality… LocalMunicipali… No_schooling Some_primary Complete_primary\n  <chr>              <chr>                   <dbl>        <dbl>            <dbl>\n1 EC101              Camdeboo                 13.4         34.5             8.94\n2 EC102              Blue Crane Route         16.0         36.2             8.80\n3 EC103              Ikwezi                   18.4         35.4             8.59\n# … with 3 more variables: Some_secondary <dbl>, Grade_12_Std_10 <dbl>,\n#   Higher <dbl>\n\n\n\n\nLoading the map\nNext we need a ‘blank map’ of the same South African municipalities that are included in the data above. It is read-in below in geoJSON format but it would not have been unusual if it had been in .shp (shapefile) or .kml format, instead. The source of the data is https://dataportal-mdb-sa.opendata.arcgis.com/. There are several ways of reading this file into R but it is better to use the sf package because older options such as maptools::readShapePoly() (which was for reading shapefiles) or rgdal::readOGR are either deprecated already or in the process of being retired.\n\n\nCode\nif(!(\"proxy\" %in% installed)) install.packages(\"proxy\")\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nrequire(sf)\n\nmunicipal <- read_sf(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/MDB_Local_Municipal_Boundary_2011.geojson\")\n\n\n If we now look at the top of the municipal object then we find it is of class sf, which is short for simple features. It has a vector geometry (it is of type multipolygon) and has its coordinate reference system (CRS) set as WGS 84. It also contains some attribute data, although not the schooling data we are looking to map.\n\n\nCode\nslice_head(municipal, n = 1)\n\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 27.15761 ymin: -33.28488 xmax: 28.0811 ymax: -32.67573\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 11\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n# … with 6 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>, geometry <MULTIPOLYGON [°]>\n\n\nHere are just the attribute data\n\n\nCode\nst_drop_geometry(municipal) |>\n  slice_head(n = 5)\n\n\n# A tibble: 5 × 10\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n2        2 EC           Eastern Cape EC101                 Camdeboo             \n3        3 EC           Eastern Cape EC102                 Blue Crane Route     \n4        4 EC           Eastern Cape EC103                 Ikwezi               \n5        5 EC           Eastern Cape EC104                 Makana               \n# … with 5 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>\n\n\nAnd here is the ‘blank’ map.\n\n\nCode\npar(mai=c(0, 0, 0, 0))  # Removes the plot margins\nmunicipal |>\n  st_geometry() |>\n  plot()\n\n\n\n\n\nHad it been necessary to set the coordinate reference system then the function st_set_crs() would be used. Instead, and just for fun, we will change it: here is the map transformed on to a ‘south up’ coordinate reference system, achieved by changing its EPSG code to 2050 with the function st_transform().\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nmunicipal |>\n  st_transform(2050) |>\n  st_geometry() |>\n  plot()\n\n\n\n\n\n\nNote how functions with the sf library tend to start with st_. Personally, I find this slightly confusing and I am not sure it doesn’t make it harder to find what I looking for in the package’s help pages but it is consistent for the functions and methods that operate on spatial data and is, I believe, short for spatial type.\n\n\nsf and sp\nAt the risk of over-simplification, sf (simple features) can be viewed as a successor to the earlier sp (spatial) and related packages, which are well documented in the book Applied Spatial Data Analysis with R. Sometimes other packages are still reliant on sp and so the spatial objects need to be changed into its native format prior to use.\n\n\nCode\n# From sf to sp\nmunicipal_sp <- as(municipal, \"Spatial\")\nclass(municipal_sp)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nCode\n# From sp to sf\nmunicipal_sf <- st_as_sf(municipal_sp)\nclass(municipal_sf)\n\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "href": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "title": "Thematic maps in R",
    "section": "Joining the attribute data to the map",
    "text": "Joining the attribute data to the map\nIf we look again at the map and schooling data, we find that they have two variables in common which suggests a means to join them together based on a common field.\n\n\nCode\nintersect(names(municipal), names(education))\n\n\n[1] \"LocalMunicipalityCode\" \"LocalMunicipalityName\"\n\n\nThis is encouraging but, in this example, we need to be careful using the municipal names because not all of those in the map are in the education data or vice versa.\n\n\nCode\n# anti_join() return all rows from x without a match in y\nanti_join(municipal, education, by = \"LocalMunicipalityName\")\n\n\nSimple feature collection with 7 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 27.42423 ymin: -32.05935 xmax: 32.05014 ymax: -24.96652\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 11\n  OBJECTID ProvinceCode ProvinceName  LocalMunicipalityCode LocalMunicipalityNa…\n     <int> <chr>        <chr>         <chr>                 <chr>               \n1       34 EC           Eastern Cape  EC157                 King Sabata Dalindye\n2       74 KZN          KwaZulu-Natal KZN214                uMuziwabantu        \n3       75 KZN          KwaZulu-Natal KZN215                Ezinqoleni          \n4       97 KZN          KwaZulu-Natal KZN262                uPhongolo           \n5      146 MP           Mpumalanga    MP301                 Chief Albert Luthuli\n6      149 MP           Mpumalanga    MP304                 Dr Pixley Ka Isaka S\n7      165 NW           North West    NW372                 Local Municipality o\n# … with 6 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>, geometry <MULTIPOLYGON [°]>\n\n\nCode\nanti_join(education, municipal, by = \"LocalMunicipalityName\")\n\n\n# A tibble: 7 × 8\n  LocalMunicipality… LocalMunicipali… No_schooling Some_primary Complete_primary\n  <chr>              <chr>                   <dbl>        <dbl>            <dbl>\n1 EC157              King Sabata Dal…         21.7         33.2             6.08\n2 KZN214             UMuziwabantu             20.7         42.1             7.35\n3 KZN215             Ezingoleni               21.9         40.1             6.84\n4 KZN262             UPhongolo                22.5         35.1             6.80\n5 MP301              Albert Luthuli           23.8         31.1             6.57\n6 MP304              Pixley Ka Seme           24.0         31.9             6.25\n7 NW372              Madibeng                 12.8         27.1             6.99\n# … with 3 more variables: Some_secondary <dbl>, Grade_12_Std_10 <dbl>,\n#   Higher <dbl>\n\n\nFortunately, the municipal codes are consistent even where the names are not.\n\n\nCode\nanti_join(municipal, education, by = \"LocalMunicipalityCode\")\n\n\nSimple feature collection with 0 features and 10 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 11\n# … with 11 variables: OBJECTID <int>, ProvinceCode <chr>, ProvinceName <chr>,\n#   LocalMunicipalityCode <chr>, LocalMunicipalityName <chr>,\n#   DistrictMunicipalityCode <chr>, DistrictMunicipalityName <chr>, Year <int>,\n#   Shape__Area <dbl>, Shape__Length <dbl>, geometry <GEOMETRY [°]>\n\n\nCode\nanti_join(education, municipal, by = \"LocalMunicipalityCode\")\n\n\n# A tibble: 0 × 8\n# … with 8 variables: LocalMunicipalityCode <chr>, LocalMunicipalityName <chr>,\n#   No_schooling <dbl>, Some_primary <dbl>, Complete_primary <dbl>,\n#   Some_secondary <dbl>, Grade_12_Std_10 <dbl>, Higher <dbl>\n\n\nWe therefore join the data to the map using the variable LocalMunicipalityCode and check that the schooling data are now attached to the map. They are.\n\n\nCode\nmunicipal <- left_join(municipal, education, by = \"LocalMunicipalityCode\")\nnames(municipal)\n\n\n [1] \"OBJECTID\"                 \"ProvinceCode\"            \n [3] \"ProvinceName\"             \"LocalMunicipalityCode\"   \n [5] \"LocalMunicipalityName.x\"  \"DistrictMunicipalityCode\"\n [7] \"DistrictMunicipalityName\" \"Year\"                    \n [9] \"Shape__Area\"              \"Shape__Length\"           \n[11] \"geometry\"                 \"LocalMunicipalityName.y\" \n[13] \"No_schooling\"             \"Some_primary\"            \n[15] \"Complete_primary\"         \"Some_secondary\"          \n[17] \"Grade_12_Std_10\"          \"Higher\"                  \n\n\n\nNote that the variables LocalMunicipalityName.x and LocalMunicipalityName.y have been created in the process of the join. This is because there are non-joined variables with duplicated names in the data, i.e. LocalMunicipalityName from municipal and LocalMunicipalityName from education."
  },
  {
    "objectID": "thematicmaps.html#mapping-the-data",
    "href": "thematicmaps.html#mapping-the-data",
    "title": "Thematic maps in R",
    "section": "Mapping the data",
    "text": "Mapping the data\n\nUsing plot{sf}\nThe ‘one line’ way of plotting the data is to use the in-built plot() function for sf.\n\n\nCode\nplot(municipal[\"No_schooling\"])\n\n\n\n\n\nAs ‘rough and ready’ way to check for spatial variation and patterns in the data, it is quick and easy. It is important to specify the variable(s) you wish to include in the plot or else it will plot them all up to the value specified by the argument max.plot, which has a default of nine:\n\n\nCode\nplot(municipal)\n\n\n\n\n\n The map can be customised. For example,\n\n\nCode\nif(!(\"RColorBrewer\" %in% installed)) install.packages(\"RColorBrewer\",\n                                                      dependencies = TRUE)\nrequire(RColorBrewer)\n\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"jenks\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\nFor the above map RColorBrewer package has been used to create a diverging red-yellow-blue colour palette that is reversed using the function rev so that red is assigned to the highest values, not lowest.\nRColorBrewer provides colour palettes based on https://colorbrewer2.org/. A ‘natural breaks’ (jenks) classification with 7 colours has been used (breaks = \"jenks\"). Compare it with the result from using breaks = \"equal\",\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"equal\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\n… or breaks = \"quantile\".\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"quantile\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"% of Population with No Schooling\")\n\n\n\n\n\nClearly the maps do not all appear alike. The geographical patterns and therefore the geographical information that we view in the map are a function of how the map is constructed, including the number, colouring and widths (ranges) of the map classes. Ideally, these should be set to reflect the distribution of the data and what is being look for in it.\nThe following histograms show the break points in the distributions used in the various maps. The code works by creating a list of plots (specifically, a list of ggplots, see below) – one plot each for the jenks, equal and quantile styles – and then using a package called gridExtra to arrange those plots into a single grid. However, the code matters less than what it reveals, which is that Jenks or other ‘natural breaks’ classifications are reasonably good for identifying break points that reflect the distribution of the data in the absence of the user having cause to set those break points in some other way.\n\n\nCode\nif(!(\"gridExtra\" %in% installed)) install.packages(\"gridExtra\",\n                                                   dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"classInt\",\n                                                  dependencies = TRUE)\n\nrequire(gridExtra)\nrequire(classInt)\n\nstyles <- c(\"jenks\", \"equal\", \"quantile\")\ng <- lapply(styles, \\(x) {\n        ggplot(municipal, aes(x = No_schooling)) +\n        geom_histogram(fill = \"light grey\") +\n        xlab(\"% of Population with No Schooling\") +\n        geom_vline(xintercept = classIntervals(municipal$No_schooling,\n                                               n = 7, style = x)$brks,\n                   col = \"dark red\") +\n        geom_rug() +\n        theme_minimal() +\n        ggtitle(paste(x,\"classification\"))\n    })\ngrid.arrange(grobs = g)\n\n\n\n\n\n For further information on using plot{sf} see here and look at the help menu, ?sf::plot.\n\n\nUsing ggplot2\nWhilst the plot() function for sf objects is useful for producing quick maps, I tend to prefer ggplot2 for better quality ones that I am wanting to customise or annotate in particular ways. We already have seen examples of ggplot2 output in earlier sessions and also in the histograms above.\nggplot2 is based on The Grammar of Graphics. I find it easiest to think of it in four stages:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\nIn the code chunk below, those four stages are applied to a boxplot showing the distribution of the no schooling variable by South African Provinces.\nFirst, the data = municipal. Second, consulting with the ggplot2 cheatsheet, I find that the aesthetics, aes(), for the boxplot, require a discrete x and a continuous y, which are provided by ProvinceName and No_schooling, respectively. ProvinceName has also been used to assign a fill colour to each box. Third, the optional changes arise from me preferring theme_minimal() to the default style, although I have then modified it to remove the legend, change the angle of the text on the x-axis, remove the x-axis label and change the y-axis label.\n\n\nCode\nggplot(data = municipal, aes(x = ProvinceName, y = No_schooling,\n                             fill = ProvinceName)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) +\n  xlab(element_blank()) +\n  ylab(\"% No schooling within municipalities\")\n\n\n\n\n\n Let’s now take that process and apply it to create a map, using the same RColorBrewer colour palette as previously and adding the map using geom_sf (for a full list of geoms available for ggplot2 see here). The line scale_fill_distiller is an easy way to shade the map using the colour palette from RColorBrewer. Modifying the theme with the arguments theme(... = element_blank(), ...) suppress the axis titles. What labs() does should be obvious.\n\n\nCode\nggplot(municipal, aes(fill = No_schooling)) +\n  geom_sf() +\n  scale_fill_distiller(\"%\", palette = \"RdYlBu\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )  \n\n\n\n\n\n Presently the map has a continuous shading scheme. This can be changed to discrete map classes and colours by converting the continuous municipal$No_schooling variable to a factor, using the cut() function, here with break points found using ClassIntervals(style = \"jenks\"). Because we have changed from continuous to discrete data but still want to use an RColorBrewer palette, so scale_fill_brewer() replaces scale_fill_distiller(), wherein the argument direction = -1 reverses the RdYlBu palette so that the highest values are coloured red. Adding guides(fill = guide_legend(reverse = TRUE)) reverses the legend so that the highest values are on top in the legend, which is another preference of mine.\n\n\nCode\n# Find the break points in the distribution using a Jenks classification\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\n\n# Factor the No_schooling variable using those break points\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\n\nggplot(municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nAt the point, we may note that the four stages of the map production that I referred to earlier was an over-simplification and we can add a fifth:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart – geom_sf for mapping;\n‘Scale’ the data – in the above examples, link the mapped variables to map classes and colour codes. The scaling is what scale_fill_... were doing;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\n\nAnnotating the map with ggspatial\nHaving created the basic map using ggplot2, we can add some additional map elements using ggspatial which provides some extra cartographic functions. The following code chunk adds a backdrop to the map. Different backgrounds (alternative map tiles) can be chosen from the list at rosm::osm.types(); see here for what they look like.\nBefore running the code, we may note a change from the previous code chunk (above) which is in addition to installing and requiring ggspatial and adding the map tile as a backdrop. Specifically, if you look at the previous code chunk you will find that the data, municipal are handed-to ggplot in the top line ggplot(municipal, ...). In essence, this sets municipal as a global parameter: it is where ggplot will, by default, look for the variable called for in aes(fill = No_schooling_gp) and where it will look for other variables too. Whilst this would work just fine in the code below, too, a little later I introduce a second geom_sf into the plot and no longer want municipal to be the default choice for all the aesthetics of the chart. To pre-empt any problems that might otherwise arise, I no longer specify as the default dataset in ggplot() but, instead, specifically name municipal as the fill data, in the line geom_sf(data = municipal, aes(fill = No_schooling_gp)), which will leave me free to associate other data with different aesthetics in due course.\n\n\nCode\nif(!(\"ggspatial\" %in% installed)) install.packages(\"ggspatial\",\n                                                   dependencies = TRUE)\nrequire(ggspatial)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n A north arrow and a scale bar can also be added, although including the scale bar generates a warning because the map-to-true life distance ratio is not actually constant across the map but varies with longitude and latitude. The argument, location = \"tl\" is short for top left; location = \"br\" for bottom right. See ?annotation_north_arrow and ?annotation_scale for further details and options. Note also the use of the last_plot() function to more easily add content to the last ggplot.\n\n\nCode\nlast_plot() +\n  annotation_north_arrow(location = \"tl\",\n                         style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\n\n\n\n\n In the next example, the locations of South African cities are added to the map, with a symbol drawn in proportion to their population size. The source of the data is a shapefile from https://data.humdata.org/dataset/hotosm_zaf_populated_places. The symbol shape that is specified by pch = 3 has the same numeric coding as those in ?graphics::points (i.e. 0 is a square, 1, is a circle, 2 is a triangle, and so forth).\n\nThe function scales::label_comma() forces decimal display of numbers to avoid displaying scientific notation.\n\n\nCode\nif(!(\"scales\" %in% installed)) install.packages(\"scales\", dependencies = TRUE)\n\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\nunzip(\"cities.zip\")\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |>\n  filter(place == \"city\") |>\n  mutate(population = as.numeric(population)) ->\n  cities\n\nlast_plot() +\n  geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma())\n\n\n\n\n\n\nSlightly confusingly, a shapefile actually consists of at least three files, one with the extension .shp (the coordinate/shape data), one .shx (an index file) and one .dbf (the attribute data). If you use a shapefile you need to make sure you download all of them and keep them together in the same folder.\n\n\nLabelling using ggsflabel\nNice labelling of the cities is provided by ggsflabel, in this example using a pipe |> to filter and only label cities with over a million population. The function geom_sf_label_repel() is designed to stop labels from being placed over each other. We could use last_plot() again to create this map but, instead, here is the code in full:\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n    geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma()) +\n  geom_sf_label_repel(data = cities |> filter(population > 1e6),\n                    aes(label = name), alpha = 0.7, size = 3) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n\n\nSaving the map\nHaving created the map, it can now be saved. If you are not using an R Project to save your files into, you may wish to change your working directory before saving the graphic, using setwd(dir) and substituting dir with the pathname to the preferred directory, or by using Session -> Set Working Directory -> Choose Directory from the dropdown menus. Once you have done so, the last_plot() is easily saved using the function ggsave(). For example, in .pdf format, to a print quality,\n\n\nCode\nggsave(\"no_schooling.pdf\", device = \"pdf\", width = 6, units = \"in\",\n       dpi = \"print\")\n\n\nAlternatively, we can write directly to a graphics device, using one of the functions bmp(), jpeg(), png(), tiff() or pdf(). For instance,\n\n\nCode\njpeg(\"no_schooling.jpg\", res = 72)\nlast_plot()\ndev.off()\n\n\n\n\nCreating an interactive map using ggiraph\nSo far all the maps we have created have been static. This is obviously better for anything that will be printed but, for a website or similar, we may wish to include more ‘interaction’. The package ggiraph package creates dynamic ggplot2 graphs and we can use it to create an interactive map where information about the areas appears as we brush over those areas on the map with the mouse pointer. This is achieved by replacing, in the code, geom_sf() with the geom_sf_interactive() function from ggiraph, specifying the text to show with the tooltip (the example below pastes a number of character elements together without a space between them, hence paste0() but does include a carriage return, \\n) and rendering the resulting ggplot2 object with girafe().\n\n\nCode\nif(!(\"ggiraph\" %in% installed)) install.packages(\"ggiraph\", dependencies = TRUE)\nrequire(ggiraph)\n\ng <- ggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf_interactive(aes(tooltip = paste0(LocalMunicipalityName.x, \"\\n\",\n                                           round(No_schooling,1), \"%\"),\n                             fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) +\n  annotation_north_arrow(location = \"tl\",\n                         style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\ngirafe(ggobj = g)\n\n\n\n\n\n\n\n\n\nUsing tmap\nClearly there is a lot of scope to produce high quality maps using ggplot2 and various associated packages. However, it has a rival, in tmap, which is arguably easier to use. Like ggplot2, tmap adopts the Grammar of Graphics but approaches it in a slightly different way that uses layers: it builds-up the layers of the graphic by first specifying a spatial object or background, then doing things to the map based on it, then specifying another spatial object and/or other map elements to do things with, and so forth. The types of layers available can be viewed here and here. A brief introduction to tmap is available here.\nThe code chunk a little further below builds-up the layers of the map to produce one quite like that previously created in ggplot2. First, however, there is a error in the geometry of the underlying municipal map file to deal with:\n\n\nCode\nall(st_is_valid(municipal))\n\n\n[1] FALSE\n\n\nThe problem lies in the 128th area, where one edge crosses another,\n\n\nCode\nst_is_valid(municipal[128,], reason = TRUE)\n\n\n[1] \"Edge 25 crosses edge 27\"\n\n\ntmap is less forgiving of this error than ggplot2 is. A temporary ‘fix’ – more of a side-step really – is achieved by changing the coordinate reference system, which presently is using EPSG: 4326 (you can see this with st_crs(municipal)), to EPSG: 3857.\n\n\nCode\nmunicipal <- st_transform(municipal, 3857)\nall(st_is_valid(municipal))\n\n\n[1] TRUE\n\n\nNow we can produce the plot,\n\n\nCode\nif(!(\"tmap\" %in% installed)) install.packages(\"tmap\", dependencies = TRUE)\nrequire(tmap)\n\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal, is.master = TRUE) +\n  tm_fill(\"No_schooling\", palette = \"-RdYlBu\", title = \"%\", style = \"jenks\",\n          n = 7) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities |> filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\n\n\n\n\n The map looks pretty good and can be saved using the function tmap_save. For example,\n\n\nCode\ntmap_save(tmap_last(), \"no_schooling2.jpg\", width = 7, units = \"in\")\n\n\nHowever, there is a cartographic irritation. If you look at the map classes, they are non-unique: e.g. 5.64 to 9.87, 9.87 to 13.38, 13.38 to 17.19, and so forth. Which category would a value of 9.87 (or 13.38, etc.) fall into?\nTo solve this problem, we can do what we did for the ggplots, which is to create a factor from the municipal$No_schooling variable, which is what the first two lines of code below do. The third line reverses the order of the factors, so that the highest not lowest valued group is treated as the first level and so forth. The reason I have added this is because of my preference for the highest values to appear top in the legend.\n\n\nCode\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\nmunicipal$No_schooling_gp <- factor(municipal$No_schooling_gp,\n                                levels = rev(levels(municipal$No_schooling_gp)))\n\ntm_graticules(col = \"lightgrey\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %>% filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\n\n\n\n\n Where tmap really excels is in rendering interactive maps to leaflet by changing the tmap_mode from tmap_mode(\"plot\")to tmap_mode(\"view\"). The following allows panning, can be zoomed in and out of, can have different map layers displayed (move your mouse cursor over the map layers icon to do so) and, if you right click on any of the areas shown, will bring-up information about them. Unfortunately, it also reveals that the earlier ‘fix’ to the municipal object doesn’t work here so I have omitted the problem area, although that isn’t much of a solution.\n\n\nCode\ntmap_mode(\"view\")\n\ntm_basemap(c(Stamen = \"Stamen.Watercolor\",\n             Carto = \"CartoDB\",\n             OSM = \"OpenStreetMap\")) +\n  tm_shape(municipal[-128,], name = \"municipalities\") +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\",\n          id = \"LocalMunicipalityName.x\",\n          popup.vars = c(\"% No schooling:\" = \"No_schooling\",\n                         \"Province: \" = \"ProvinceName\"),\n          popup.format = list(digits = 1)) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\",\n          id = \"name\",\n          popup.vars = c(\"Population: \" = \"population\")) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_view(view.legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n \nDifferent layers are available dependent upon the tmap_mode. For example, there is no straightforward way of adding a maptile (tm_basemap) as the backdrop to a \"plot\" map, nor a compass (tm_compass) or a scale bar (tm_scale_bar) to a \"view\".\n We can also have some fun! Here is an animated map where the animation is produced from a combination of the tm_facets(along = \"ProvinceName\", free.coords = FALSE) layer and the use of tmap_animation() function. Notice how I add municipal to the map twice, as two separate layers. The first is to provide a general backdrop to the map with all the municipalities shaded grey. They second is linked to the animation with the municipalities shaded by the percentage of their population without schooling.\n\n\nCode\nif(!(\"gifski\" %in% installed)) install.packages(\"gifski\", dependencies = TRUE)\n\ntmap_mode(\"plot\")\n\nt <- tm_graticules(col = \"light grey\") +\n  tm_shape(municipal) +\n  tm_polygons(col = \"grey\", border.col = \"black\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"white\") +\n  tm_facets(along = \"ProvinceName\", free.coords = FALSE) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\ntmap_animation(t, delay = 100)\n\n\n  \nThe animation may be saved as a .gif file by including the argument filename (see ?tmap_animation).\n Faceting can also be used on static maps, as in the following example, where tm_facets(by = \"ProvinceName\", free.coords = TRUE) creates a choropleth map for each Province, with a common legend, positioned outside each provincial map provided by tm_layout(legend.outside.position = \"bottom\").\n\n\nCode\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\",\n          title = \"% Population with No Schooling\",\n          legend.is.portrait = FALSE) +\n  tm_borders(col = \"white\") +\n  tm_facets(by = \"ProvinceName\", free.coords = TRUE) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.outside.position = \"bottom\")"
  },
  {
    "objectID": "thematicmaps.html#geofacets",
    "href": "thematicmaps.html#geofacets",
    "title": "Thematic maps in R",
    "section": "Geofacets",
    "text": "Geofacets\nTo this point of the session we have been using maps to represent the spatial distribution of one or more variables whose values are plotted as an attribute of the map such as colour or shape size. A different approach is offered by the geofacet package which uses geography as a ‘placeholder’ to position graphical summaries of data for different parts of the map. The idea is to flexibly visualise data for different geographical regions by providing a ggplot2 faceting function facet_geo() that works just like ggplot2’s built-in faceting, except that the resulting arrangement of panels follows a grid that mimics the original geographic topology as closely as possible.\nHere is an example of it in use, showing the distribution of municipalities within South African provinces in terms of the percentage of the population with higher education (university) qualifications.\n\n\nCode\nif(!(\"geofacet\" %in% installed)) install.packages(\"geofacet\")\nrequire(geofacet)\n\n# Define a grid that mimics the geographical distribution of the provinces\nmygrid <- data.frame(\n  code = c(\"LIM\", \"GT\", \"NW\", \"MP\", \"NC\", \"FS\", \"KZN\", \"EC\", \"WC\"),\n  name = c(\"Limpopo\", \"Gauteng\", \"North West\", \"Mpumalanga\", \"Northern Cape\",\n           \"Free State\", \"KwaZulu-Natal\", \"Eastern Cape\", \"Western Cape\"),\n  row = c(1, 2, 2, 2, 3, 3, 3, 4, 4),\n  col = c(3, 3, 2, 4, 1, 2, 3, 2, 1),\n  stringsAsFactors = FALSE\n)\n\n# Plot the data with the geofaceting\nggplot(municipal, aes(Higher)) +\n  geom_boxplot(col = \"dark grey\") +\n  geom_density() +\n  geom_rug() +\n  facet_geo(~ ProvinceName, grid = mygrid) +\n  scale_y_continuous(breaks = c(0, 0.2, 0.4)) +\n  theme_bw() +\n  labs(\n    title = \"Percentage of Population with higher education\",\n    caption = \"Source: 2011 Census / Statistics South Africa\"\n  ) +\n  xlab(\"% per municipality\")"
  },
  {
    "objectID": "thematicmaps.html#saving-the-map-and-attribute-data",
    "href": "thematicmaps.html#saving-the-map-and-attribute-data",
    "title": "Thematic maps in R",
    "section": "Saving the map and attribute data",
    "text": "Saving the map and attribute data\nThat’s almost it for now! However, before finishing, we will save the map with the joined attribute data to the working directory as an R object.\n\n\nCode\nsave(municipal, file = \"municipal.RData\")"
  },
  {
    "objectID": "thematicmaps.html#summary",
    "href": "thematicmaps.html#summary",
    "title": "Thematic maps in R",
    "section": "Summary",
    "text": "Summary\nThis session has demonstrated that R is a powerful tool for drawing publication quality maps. The native plot functions for sf objects are useful as a quick way to draw a map and both ggplot2 and tmap offer a range of functionality to customise their cartographic outputs to produce really nice looking maps. I tend to use ggplot2 but that is really more out of habit than anything else as tmap might actually be the easier to use. It depends a bit on whether I am drawing static maps (usually in ggplot2) or interactive ones (probably better in tmap). There are other packages available, too, including mapview, which the following code chunk uses (see also, here), and Leaflet to R. Perhaps the key take-home point is that these maps can look at lot better than those produced by some conventional GIS and have the advantage that they can be linked to other analytically processes in R, as future sessions will demonstrate.\n\n\nCode\nif(!(\"mapview\" %in% installed)) install.packages(\"mapview\", dependencies = TRUE)\nrequire(mapview)\n\nmapview(municipal %>% mutate(No_schooling = round(No_schooling, 1)), \n        zcol = \"No_schooling\",\n        layer.name = \"% No Schooling\",\n        map.types = \"Stamen.Watercolor\",\n        col.regions = colorRampPalette(rev(brewer.pal(9, \"RdYlBu\"))))"
  },
  {
    "objectID": "thematicmaps.html#further-reading",
    "href": "thematicmaps.html#further-reading",
    "title": "Thematic maps in R",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 2 on Spatial data and R packages for mapping from Geospatial Health Data by Paula Morga (2019)\n\nChapter 9 on Making maps with R from Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow.\nSee also: Elegant and informative maps with tmap, which is a work in progress by Martijn Tennekes and Jakub Nowosad."
  },
  {
    "objectID": "tidyverse.html#introduction",
    "href": "tidyverse.html#introduction",
    "title": "Tidyverse",
    "section": "Introduction",
    "text": "Introduction\nIf base R is R Classic then tidyverse is a new flavour of R, designed for data science. It consists of a collection of R packages that “share an underlying design philosophy, grammar, and data structures”.\nTidyverse is easier to demonstrate then to pin-down to some basics so let’s work through an example using both base R and tidyverse to illustrate some differences."
  },
  {
    "objectID": "tidyverse.html#to-start",
    "href": "tidyverse.html#to-start",
    "title": "Tidyverse",
    "section": "To Start",
    "text": "To Start\nIf, as suggested in ‘Getting Started’, you have created an R Project to contain all the files you create and download for this course then open it now by using File –> Open Project… from the dropdown menus in R Studio. If you have not created one then now might be a good time!\nWe will begin by downloading a data file to use. It will be downloaded to your working directory, which is the folder associated with your R Project if you are using one. You can check the working directory by using getwd() and change it using Session –> Set Working Directory or with the function setwd(dir) where dir is the chosen directory.\nThe data are an extract of the Covid Data Dashboard for England in December 2021. Some prior manipulation and adjustments to these data have been undertaken for another project so treat them as indicative only. The actual reported numbers may have been changed slightly from their originals although only marginally so.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_extract.csv\", \"covid.csv\", mode = \"wb\", quiet = TRUE) \n\n\nWe also need to require(tidyverse) ready for use.\n\n\nCode\nrequire(tidyverse)\n\n\n\nIf you get a warning message saying there is no package called tidyverse then you need to install it: install.packages(\"tidyverse\", dependencies = TRUE). You will find that some people prefer to use library() instead of require(). The difference between them is subtle but you can find an argument in favour of using library() here even though I usually don’t."
  },
  {
    "objectID": "tidyverse.html#reading-in-the-data",
    "href": "tidyverse.html#reading-in-the-data",
    "title": "Tidyverse",
    "section": "Reading-in the data",
    "text": "Reading-in the data\nLet’s read-in and take a look at the data. First in base R.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\nhead(df1)\n\n\n   MSOA11CD regionName X2021.12.04 X2021.12.11 X2021.12.18 X2021.12.25 All.Ages\n1 E02000002     London          25          48         148         176     7726\n2 E02000003     London          46          58         165         215    11246\n3 E02000004     London          24          44         100         141     6646\n4 E02000005     London          58          97         185         231    10540\n5 E02000007     London          38          94         153         205    10076\n6 E02000008     London          54         101         232         245    12777\n\n\nNow using tidyverse,\n\n\nCode\ndf2 <- read_csv(\"covid.csv\")\nslice_head(df2, n = 6)\n\n\n# A tibble: 6 × 7\n  MSOA11CD  regionName `2021-12-04` `2021-12-11` `2021-12-18` `2021-12-25`\n  <chr>     <chr>             <dbl>        <dbl>        <dbl>        <dbl>\n1 E02000002 London               25           48          148          176\n2 E02000003 London               46           58          165          215\n3 E02000004 London               24           44          100          141\n4 E02000005 London               58           97          185          231\n5 E02000007 London               38           94          153          205\n6 E02000008 London               54          101          232          245\n# … with 1 more variable: `All Ages` <dbl>\n\n\nThere are already some differences. First, tidyverse has, in this case, handled the names of the variables better. It has also created what is described as a tibble which is “a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” You can find out more about them and how they differ from traditional data frames here. Basically, they are a form of data frame that fit into tidyverse’s philosophy to try and keep ‘things’ tidy through a shared underlying design philosophy, grammar and data structures."
  },
  {
    "objectID": "tidyverse.html#selecting-and-renaming-variables",
    "href": "tidyverse.html#selecting-and-renaming-variables",
    "title": "Tidyverse",
    "section": "Selecting and renaming variables",
    "text": "Selecting and renaming variables\nWe will now select the regionName, 2021-12-04 and All Ages variables, rename the second of these as cases and the third as population, and look at the data again to check that it has worked.\nIn base R,\n\n\nCode\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[2:3] <- c(\"cases\", \"population\")\nhead(df1)\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\nIn tidyverse,\n\n\nCode\ndf2 <- select(df2, regionName, `2021-12-04`, `All Ages`)\ndf2 <- rename(df2, cases = `2021-12-04`, population = `All Ages`)\nslice_head(df2, n = 6)\n\n\n# A tibble: 6 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n\n\nComparing the two, the tidyverse code may be more intuitive to understand because of its use of verbs as functions: select(), rename() and so forth."
  },
  {
    "objectID": "tidyverse.html#piping",
    "href": "tidyverse.html#piping",
    "title": "Tidyverse",
    "section": "Piping",
    "text": "Piping\nNow we shall bring the two previous stages together, using what is referred to as a pipe. Without worrying about the detail, which we will return to presently, here is an example of a pipe, |> being used in base R:\n\n\nCode\nread.csv(\"covid.csv\") |>\n  (\\(x) x[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")])() -> df1\nnames(df1)[2:3] <- c(\"cases\", \"population\")\ndf1 |>\n  head()\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\n\nThe above will only work if you are using R version 4.1.0 or above. You can check which version you are running by using R.Version()$version.\nNow using tidyverse and a different pipe, %>%,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  slice_head(n = 6)\n\n\n# A tibble: 6 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n\n\nThe obvious difference here is that the tidyverse code is more elegant. But what is the pipe and what is the difference between |> in the base R code and %>% in the tidyverse example?\nA pipe is really just a way of sending (’piping`) something from one line of code to the next, to create a chain of commands (forgive the mixed metaphors). For example,\n\n\nCode\nx <- 0:10\nmean(x)\n\n\n[1] 5\n\n\nCould be calculated as\n\n\nCode\n0:10 |>\n  mean()\n\n\n[1] 5\n\n\nAs\n\n\nCode\n0:10 %>%\n  mean()\n\n\n[1] 5\n\n\nor, if you want to save on a few characters of code,\n\n\nCode\n0:10 %>%\n  mean\n\n\n[1] 5\n\n\nHowever, this won’t work:\n\n\nCode\n0:10 |>\n  mean\n\n\nThis is all a little confusing, of course, but it is because of the difference pipes, one (|>) a more recent development than the other (%>%).\nA more complicated example of piping is below. It employs the function sapply(), a variant of the function lapply(X, FUN) that takes a list X and applies the function FUN to each part of it. In the example, it is the function mean.\n\n\nCode\nx <- list(0:10, 10:20)\n  # Creates a list with two parts: the numbers 0 to 10, and 10 to 20\ny <- sapply(x, mean)\n  # Calculates the mean for each part of the list, which are 5 and 15\nsum(y)\n\n\n[1] 20\n\n\nCode\n  # Sums together the two means, giving 20\n\n\nThe above could instead be written as\n\n\nCode\nlist(0:10, 10:20) |>\n  sapply(mean) |>\n  sum()\n\n\n[1] 20\n\n\nor as\n\n\nCode\nlist(0:10, 10:20) %>%\n  sapply(mean) %>%\n  sum\n\n\n[1] 20\n\n\nAll three arrive at the same answer, which is 20.\nSo far, so good but what is the difference between |> and %>%? The answer is that %>% was developed before |> in the magrittr package, whereas |> is R’s new native pipe. They are often interchangeable but not always.\nAt the moment, the |> pipe is less flexible to use than %>%. Consider the following example. The final two lines of code work fine using %>% to pipe the data frame into the regression model (the function lm() fits a linear model).\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)\n  # Adds some random noise to the relationship between y and x\ndata.frame(x, y) %>%\n  lm(y ~ x, data = .)\n\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n    -0.1509       2.0040  \n\n\nHowever, it does not work with the pipe, |> because it does not recognise the place holder . that would receive the data frame from the line above and contains the variables for the model.\n\n\nCode\n# The following code does not work\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  lm(y ~ x, data = .)\n\n\nTo solve the problem, the above code can be modified by wrapping the regression part in another function but the end result is rather ‘clunky’.\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  (\\(z) lm(y ~ x, data = z))() \n\n\n\nCall:\nlm(formula = y ~ x, data = z)\n\nCoefficients:\n(Intercept)            x  \n   -0.05641      2.00254  \n\n\nOver time, expect |> to be developed and to supersede %>%. For now, you are unlikely to encounter errors using %>% as a substitute for |> but you might using |> instead of %>%. In other words, %>% is the safer choice if you are unsure."
  },
  {
    "objectID": "tidyverse.html#back-to-the-example",
    "href": "tidyverse.html#back-to-the-example",
    "title": "Tidyverse",
    "section": "Back to the example",
    "text": "Back to the example\nAfter that digression into piping, let’s return to our example that is comparing base R and tidyverse to read-in a table of data, select variables and rename one, and, in the following, to calculate the number of COVID-19 cases per English region as a percentage of their estimated populations in the week ending 2021-12-04.\nFirst, in base R:\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)  # Total cases per region\ncases\n\n\n           East Midlands          East of England                   London \n                   25472                    35785                    43060 \n              North East               North West               South East \n                   10796                    31185                    62807 \n              South West            West Midlands Yorkshire and The Humber \n                   33846                    26554                    21079 \n\n\nCode\n  # This step isn't necessary but is included\n  # to show the result of the line above\npopulation <- tapply(df1$population, df1$regionName, sum)\n  # Total population per region\nrate <- round(cases / population * 100, 3)\nrate\n\n\n           East Midlands          East of England                   London \n                   0.524                    0.571                    0.479 \n              North East               North West               South East \n                   0.403                    0.423                    0.681 \n              South West            West Midlands Yorkshire and The Humber \n                   0.598                    0.445                    0.381 \n\n\nNow using tidyverse,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  group_by(regionName) |>\n  summarise(across(where(is.numeric), sum)) |>\n  mutate(rate = round(cases / population * 100, 3)) |>\n  print(n = Inf)\n\n\n# A tibble: 9 × 4\n  regionName               cases population  rate\n  <chr>                    <dbl>      <dbl> <dbl>\n1 East Midlands            25472    4865583 0.524\n2 East of England          35785    6269161 0.571\n3 London                   43060    8991550 0.479\n4 North East               10796    2680763 0.403\n5 North West               31185    7367456 0.423\n6 South East               62807    9217265 0.681\n7 South West               33846    5656917 0.598\n8 West Midlands            26554    5961929 0.445\n9 Yorkshire and The Humber 21079    5526350 0.381\n\n\nEither way produces the same answers but, again, there is an elegance and consistency to the tidyverse way of doing it that is, perhaps, missing from base R."
  },
  {
    "objectID": "tidyverse.html#plotting",
    "href": "tidyverse.html#plotting",
    "title": "Tidyverse",
    "section": "Plotting",
    "text": "Plotting\nAs a final step for the comparison, we will extend the code to visualise the regional COVID-19 rates in a histogram, with a rug plot included. A rug plot is a way of preserving the individual data values that would otherwise be ‘lost’ within the bins of a histogram.\nAs previously, we begin with base R,\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)\npopulation <- tapply(df1$population, df1$regionName, sum)\nrate <- round(cases / population * 100, 3)\nhist(rate, xlab = \"rate (cases as % of population)\",\n     main = \"Regional COVID-19 rates: week ending 2021-12-04\")\nrug(rate, lwd = 2)\n\n\n\n\n\n…and continue with tidyverse, creating the output in such a way that it mimics the previous plot.\n\n\nCode\nrequire(ggplot2)\nread_csv(\"covid.csv\") |>\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  group_by(regionName) |>\n  summarise(across(where(is.numeric), sum)) |>\n  mutate(rate = round(cases / population * 100, 3)) ->\n  df2\n\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05,\n                   center = -0.025) +\n    geom_rug(size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nIn this instance, it is the tidyverse code that is the more elaborate. This is partly because there is more customisation of it to mimic the base R plot. However, it is also because it is using the package ggplot2 to produce the histogram. We return to ggplot2 more in later sessions. For now it is sufficient to scan the code and observe how it is ‘layering up’ the various components of the graphic, which those components separated by the + in the lines of code.\n\nThe use of the + notation in ggplot2 operates like a pipe in that the outcome of one operation is handed on to the next to modify the graphic being produced. It doesn’t use the pipe because the package’s origins are somewhat older but just think of the + as layering-up – adding to – the graphic.\nI prefer the ggplot2 to the hist() graphics plot but that may be a matter of personal taste. However, ggplot2 can do ‘clever things’ with the visualisation, a hint of which is shown below.\n\n\nCode\nrequire(ggplot2)\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05,\n                   center = -0.025) +\n    geom_rug(aes(colour = regionName), size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    scale_colour_discrete(name = \"Region\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank()) \n\n\n\n\n\n Please don’t form that impression that ggplot2 is hard-wired to tidverse and base R to the base graphics. In practice, they are interchangeable.\nHere is an example of using ggplot2 after a sequence of base R commands.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ndf1$rate <- round(df1$cases / df1$population * 100, 3)\nggplot(df1, aes(x = rate, y = regionName)) +\n  geom_boxplot() +\n  labs(x = \"rate (cases as % of population)\",\n       y = \"region\",\n       title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n  theme_minimal()\n\n\n\n\n\nAnd here is an example of using the base R graphic boxplot() after a chain of tidyverse commands.\n\n\nCode\nread_csv(\"covid.csv\") |>\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  mutate(rate = round(cases / population * 100, 3)) -> df2\npar(mai=c(0.8,2,0.5,0.5), bty = \"n\", pch = 20)  # See text below\nboxplot(df2$rate ~ df2$regionName, horizontal = TRUE,\n        whisklty = \"solid\", staplelty = 0,\n        col = \"white\", las = 1, cex = 0.9, cex.axis = 0.75,\n        xlab = \"rate (cases as % of population)\", ylab=\"\",\n        main = \"Regional COVID-19 rates: week ending 2021-12-04\")\ntitle(ylab = \"region\", line = 6)\n\n\n\n\n\nI would argue that, in this instance, the base R graphic is as nice as the ggplot2 one but it took more customisation to get it that way and I had to go digging around in the help files, ?boxplot, ?bxp and ?par to find what I needed, which included changing the graphic’s margins (par(mai=...))), moving and changing the size of the text on the vertical axis (the argument cex.axis and the use of the title function), changing the appearance of the ‘whiskers’ (whisklty = \"solid\" and staplelty = 0), and so forth. Still, it does demonstrate that you can have a lot of control over what is produced, if you have the patience and tenacity to do so."
  },
  {
    "objectID": "tidyverse.html#which-is-better",
    "href": "tidyverse.html#which-is-better",
    "title": "Tidyverse",
    "section": "Which is better?",
    "text": "Which is better?\nHaving provided a very small taste of tidyverse and how it differs from base R, we might ask, “which is better?” However, the question is misguided: it is a little like deciding to go to South America and asking whether Spanish or Portuguese is the better language to use. It depends, of course, on what you intend to do and where you intend to travel.\nI use both base R and tidyverse packages in my work, sometimes drifting between the two in rather haphazard ways. If I can get what I want to work then I am happy. Outcomes worry me more than means so, although I use tidyverse a lot, I am not always as tidy as it would want me to be!"
  },
  {
    "objectID": "tidyverse.html#futher-reading",
    "href": "tidyverse.html#futher-reading",
    "title": "Tidyverse",
    "section": "Futher reading",
    "text": "Futher reading\n\nThere is much more to tidyverse than has been covered here. See here for further information about it and its core packages.\nA full introduction to using tidyverse for Data Science is provided by the book R for Data Science by Hadley Wickham and Garrett Grolemund. There is a free online version of it available and a second version in development."
  },
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "A Cartographic Answer",
    "section": "",
    "text": "Let’s answer the ‘why?’ question with a quick example of R in use. We will not worry about the exact detail of what the code means at this stage or attempt to explain it in full. Instead, we will largely take it as we find it, copying and pasting from this webpage into the R Console. The focus is on some of what R can do from a geographic perspective and not, at this stage, on how it does it.\n\nIf you find that the + sign stays on your screen, in the R Console, for a while and isn’t followed by > then you have either forgotten to hit Enter/Return or have not included all of the code that is needed to complete an operation (to complete a function, for example). You can always press esc on your keyboard and try again.\nIt was suggested in ‘Getting Started’ that you might want to create a new project for this course (a folder in which to save all the files). If you followed that advice, begin by using File –> Open Project… from the dropdown menus. Or, if you didn’t, you could create a new project now (File –> New Project…). Remember, all it really does is create a new folder in which to store all your files but that is useful because it will ensure your working directory is the same as that folder.\n\n\nFirst, we will check that the necessary packages are installed and then require them, which means to load them so they are available to use. The usual way to install a package is with the function, install.packages() so, for example, the graphics package ggplot2 is installed using install.packages(\"ggplot2\"). The code below is a bit more elaborate as it checks which packages have not yet been installed and installs them. However, the two-step process is the same: install and then require – use install.packages() to install packages (only needs to be done once on your computer, unless you re-install R / replace it with a more recent version); then require() to load the desired packages (needs to be done each time R is restarted).\n\n\nCode\n# Checks to see which packages are already installed:\ninstalled <- installed.packages()[,1]\n# Creates a character vector of packages that are required:\nrequired <- c(\"XML\", \"tidyverse\", \"readxl\", \"proxy\", \"sf\", \"ggplot2\",\n              \"classInt\", \"ggspatial\")\n# Checks which of the required packages have not yet been installed:\ninstall <- required[!(required %in% installed)]\n# Installs any that have not yet been installed:\nif(length(install)) install.packages(install, dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(readxl)\nrequire(sf)\nrequire(ggplot2)\nrequire(classInt)\nrequire(ggspatial)\n\n\n\nThe use of # indicates a comment in the code. It is there just for explanation. It is not executable code (it is ignored not run).\n\n\n\nNext, we will download a data table published by Statistics South Africa that provides estimates of the number of people speaking various languages in the South African Provinces in 2011. These data were downloaded from https://superweb.statssa.gov.za/webapi/. The data are found in an Excel spreadsheet, which is read in and manipulated, converting the counts into percentages.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/table_2022-06-22_17-36-26.xlsx?raw=true\", \"language.xlsx\", quiet = TRUE, mode = \"wb\")\n\nread_xlsx(\"language.xlsx\", sheet = \"Data Sheet 0\", skip = 8) |>\n  rename(Name = 2) |>\n  drop_na(Afrikaans) |>\n  select(-1) |>\n  mutate(across(where(is.numeric), ~ round(. / Total * 100, 2))) -> languages\n\n\nHere is the top of the data, viewed in the R environment:\n\n\nCode\nhead(languages)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# … with 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\nThere is often more than one way of achieving something in R. Here we could also use,\n\n\nCode\nslice_head(languages, n = 6)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# … with 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\n\n\n\nWhat R allows is the opportunity to map the data without needing to go outside R to use separate software such as GIS. To do so, we will need a ‘blank map’ of the Provinces that can be joined with the data to create a choropleth map (a type of thematic map).\nFirst, we will download a pre-existing map, also from https://superweb.statssa.gov.za/webapi/.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/mapview.kmz?raw=true\", \"map.kmz\", quiet = TRUE, mode = \"wb\")\nunzip(\"map.kmz\")\nst_read(\"doc.kml\") |>\n  select(-Description) -> map\n\n\nHere is the outline of that map:\n\n\nCode\nggplot(data = map) +\n  geom_sf()\n\n\n\n\n\n\n\n\nNow we can link the data table to the map\n\n\nCode\nmap |>\n  left_join(languages, by = \"Name\") -> map\n\n\nand then plot one of the variables.\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(aes(fill = IsiXhosa), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark blue\") +\n  ggtitle(\"% Population speaking Xhosa\")\n\n\n\n\n\n The really nice thing about this is that it is now very easy to change the appearance of the map with only minor updates to the code.\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"stamenwatercolor\", progress = \"none\") +\n  geom_sf(aes(fill = English), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  ggtitle(\"% Population speaking English\")\n\n\n\n\n\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"thunderforestlandscape\", progress = \"none\") +\n  geom_sf(aes(fill = Afrikaans), alpha = 0.8, col = \"transparent\") +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  annotation_north_arrow(which_north = \"grid\", location = \"topright\") +\n  ggtitle(\"% Population speaking Afrikaans\")\n\n\n\n\n\n\n\n\nFinally, once we are happy with it, we can export the image in a format suitable for a journal publication or to insert into other documents such as Microsoft Word.\nAs jpeg, to print quality:\n\n\nCode\nggsave(\"mymap.jpg\", device = \"jpeg\", width = 7, height = 6, units = \"in\",\n       dpi = \"print\")\n\n\nAs pdf:\n\n\nCode\nggsave(\"mymap.pdf\", device = \"pdf\", width = 7, height = 6, units = \"in\")\n\n\nAs bmp, to screen quality:\n\n\nCode\nggsave(\"mymap.bmp\", device = \"bmp\", width = 7, height = 6, units = \"in\",\n       dpi = \"screen\")\n\n\nIf we now look in your working directory, they should be there:\n\n\nCode\nlist.files(pattern = \"mymap\")\n\n\n[1] \"mymap.bmp\" \"mymap.jpg\" \"mymap.pdf\""
  },
  {
    "objectID": "why.html#another-example",
    "href": "why.html#another-example",
    "title": "A Cartographic Answer",
    "section": "Another example",
    "text": "Another example\nThe following example is much more complex so please don’t be put off by it. I have included it to make a simple point – it does not take too many lines of code to produce a high quality visual output. It might take a little bit of searching around online to find the code and instruction to produce exactly what you want but I rarely struggle to find an answer fairly quickly.\nI originally developed the following maps in response to the release of the 2021 UK Census data showing the ethnic composition of small area neighbourhoods. The four cities – Birmingham, Leicester, London and Manchester – are the ones that are no longer majority White British (i.e. less than half their population self-identified as White British). A consequence of this demographic change is that the cities are becoming more ethnically diverse, which is what the maps show, using a standardised census geography that I also created in R.\n\n\nCode\n# Read-in the attribute data and the boundary file:\ndf <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/diversity.csv?raw=true\")\nmap <- st_read(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/cities.geojson\", quiet = TRUE)\n\n# Although more complex, at heart what the following code does is\n# join the map to the data and then produce a separate map for\n# each city and time period, using a consistent style\ndf |>\n  pivot_longer(where(is.numeric), values_to = \"index\", names_to = \"year\") %>%\n  mutate(year = paste0(\"20\",substring(year, 3, 4))) %>%\n  left_join(map, ., by = \"OAXXCD\") %>%\n  mutate(group = paste(CITY, year, sep = \" ~ \")) %>%\n  split(.$group) %>%\n\n  lapply(function(x) {\n    \n    ggplot(x, aes(fill = index)) +\n      geom_sf(col = \"transparent\") +\n      scale_fill_viridis_c(\"Diversity\",\n                           values = c(0,0.25,0.5,0.7,0.85,0.95,1)) +\n      annotation_north_arrow(location = \"tl\",\n                            style = north_arrow_minimal(text_size = 10),\n                            height = unit(0.6, \"cm\"), width = unit(0.6, \"cm\")) +\n      annotation_scale(location = \"br\", style = \"ticks\", line_width = 0.5,\n                       text_cex = 0.5, tick_height = 0.4,\n                       height = unit(0.15, \"cm\"), text_pad = unit(0.10, \"cm\")) +\n      theme_minimal() +\n      theme(axis.text = element_blank(),\n            axis.ticks = element_blank(),\n            plot.title = element_text(size = 8, hjust = 0.5),\n            legend.title = element_text(size = 7, vjust = 3),\n            legend.text =element_text(size = 6), \n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            plot.margin = margin(t = 0,  \n                                 r = 0,  \n                                 b = 0,\n                                 l = 0)) +\n      labs(title = paste0(x$CITY[1], \": \", x$year[1]))\n  }) -> g\n\n# The cowplot library offers some additional plotting functionality\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\")\nrequire(cowplot)\n\n# The following gets the common legend for the maps\n# and stops it being printed 12 times -- once will be enough!\nlegend <- get_legend(g[[1]])\nlapply(g, function(x) {\n  x + theme(legend.position='none')\n}) -> g\n\n# This brings all the maps together as one\nggdraw(plot_grid(plot_grid(plotlist = g, ncol=3, align='v'),\n                 plot_grid(NULL, legend, ncol=1, scale = 0.5),\n                 rel_widths=c(1, 0.1),\n                 rel_heights=c(1, 0,1))) -> g\n\nprint(g)"
  },
  {
    "objectID": "why.html#convinced",
    "href": "why.html#convinced",
    "title": "A Cartographic Answer",
    "section": "Convinced?",
    "text": "Convinced?\nOf course, maps can also be produced in open source software such as QGIS and GIS software certainly have their use. R is not automatically better or necessarily a replacement for these. However, what it does offer is an integrated environment for what we might call geographic data science: we can download data from external websites, load and tidy-up those data, fit statistical or other models to them and map the results – all from within R. Our stages of working can be saved as scripts, which are faster to change and modify than using ‘point-and-click’ operations, and we can share our code with other people (even those using different operating systems) facilitating collaborative working and reproducible social-/ science. Finally, there are lots of packages available for reading, visualising, and analysing spatial data in R. Some of them are summarised here. These are attractive reasons for mapping and modelling within R."
  },
  {
    "objectID": "why.html#alternatives",
    "href": "why.html#alternatives",
    "title": "A Cartographic Answer",
    "section": "Alternatives",
    "text": "Alternatives\n\nAside from software such as QGIS, an interesting area of development is Geographic Data Science with Python. You can learn more about it here."
  },
  {
    "objectID": "why.html#need-more-convincing",
    "href": "why.html#need-more-convincing",
    "title": "A Cartographic Answer",
    "section": "Need more convincing?",
    "text": "Need more convincing?\nIf you have time, have a look at this exercise that we sometimes use with prospective students at University open days. The idea of the exercise is not to teach the students R but to show them how we use R for geographic data science in the School of Geographical Sciences. What the exercise does is take COVID-19 data for English neighbourhoods, fit statistical models to it and map the results – all in R. Again, it is the ability to use R for all the stages shown below that makes it so useful.\n\nSource: R for Data Science"
  }
]