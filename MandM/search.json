[
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "Measuring spatial autocorrelation",
    "section": "",
    "text": "We begin by recreating one of the maps from the previous session. You may wish to change your working directory to be the same as previously if it is not already.\n\n\nCode\ninstalled <- installed.packages()[,1]\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nif(!(\"RColorBrewer\" %in% installed)) install.packages(\"RColorBrewer\", dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"classInt\", dependencies = TRUE)\nif(!(\"ggplot2\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(sf)\nrequire(RColorBrewer)\nrequire(classInt)\nrequire(ggplot2)\n\nif(!file.exists(\"municipal.RData\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/municipal.RData?raw=true\", \"municipal.RData\", mode = \"wb\")\nload(\"municipal.RData\")\n\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\n\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population with no schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities, creating clusters of red and of blue values (and of yellow too). However, simply ‘eye-balling’ the map to look for patterns isn’t very scientific and it can be very deceptive. You can probably see patterns in the following map, too, but they arise from an entirely random permutation of the previous map’s data."
  },
  {
    "objectID": "autocorrelation.html#morans-test",
    "href": "autocorrelation.html#morans-test",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran’s test",
    "text": "Moran’s test\nThe classic way of quantifying how similar places are to their neighbours is to calculate the Moran’s statistic, which is a measure of spatial autocorrelation – of how much the values of a variable exhibit spatial clustering of alike values (positive spatial autocorrelation) or of ‘opposite’ values (negative spatial autocorrelation). We can calculate this statistic in R using the spdep package. We will slice out area 128 from the analysis as this is the one with an invalid geometry (see previous session and try which(!st_is_valid(municipal)) to confirm).\n\n\nCode\nif(!(\"spdep\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\nrequire(spdep)\n\nmunicipal %>%\n  slice(-128) ->\n  municipal\n\n\n\nCreating a neighbours list\nThe first step is to define neighbours. In the following example, these are places that share a border (which are contiguous). Presently it is sufficient for them to meet at a single point. If the requirement is that they share an edge not merely a corner then change the default argument from queen = TRUE to queen = FALSE (see ?poly2nb for details).\n\n\nCode\nneighbours <- poly2nb(municipal)\n\n\nThe summary of neighbours reveals that, on average, each South African municipality has 5.2 neighbours but it can range from 1 (the 182nd region in the municipal data) to 10 (region 69). The most frequent number is 6.\n\n\nCode\nsummary(neighbours)\n\n\nNeighbour list object:\nNumber of regions: 233 \nNumber of nonzero links: 1210 \nPercentage nonzero weights: 2.228812 \nAverage number of links: 5.193133 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  8 22 43 51 70 30  3  4  1 \n1 least connected region:\n182 with 1 link\n1 most connected region:\n69 with 10 links\n\n\nThe neighbours of region 69 are,\n\n\nCode\nneighbours[[69]]\n\n\n [1]  60  64  68 138 152 153 156 157 163 164\n\n\nIt is instructive to write the list of neighbours to an external neighbours file,\n\n\nCode\nwrite.nb.gal(neighbours, \"neighbours.gal\")\n\n\n… which can then be viewed by using file.edit(\"neighbours.gal\"). The file will look like the below and has a very simple format. The top line say there are 233 regions. Going down, the first of these has 4 neighbours, which are regions 13, 14, 15 and 16. The second has 6 neighbours, which are 3, 4, 8, 18, 188, 233, and so forth. The same information could be encoded in a \\(233\\times233\\) matrix, where cell \\((i, j)\\) is given a value of one if \\(i\\) and \\(j\\) are considered neighbours, else zero. The problem with that approach is most of the matrix is sparse because most regions are not neighbours. It is quicker to state which regions are neighbours. The rest, by definition, are not.\n\nThese neighbourhood relationships can be viewed as a graph by extracting the coordinate points (st_coordinates()), of the centroids (st_centroid()), of the polygons that represent each municipality, and by using the plot functions for sf (simple features) and nb (neighbours list) objects. The argument of_largest_polygon = TRUE returns the centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the whole MULTIPOLYGON, a multipolygon being when one place is represented by multiple polygons (e.g. a mainland and an offshore island). Setting this argument to true means that the centroid will like within the boundary of the place, which isn’t otherwise guarenteed (e.g. it could be in the sea between the mainland and an island).\n\n\nCode\ncoords <- st_centroid(municipal, of_largest_polygon = TRUE)\npts <- st_coordinates(coords)\n\npar(mai = c(0, 0, 0, 0))  # Remove the margins and white space around the plot\nplot(st_geometry(municipal), border = \"grey\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\n\nCreating spatial weights\nThe neighbourhood list simply defines which places are neighbours. The spatial weights give a weight to each neighbourhood link. One motivation for doing this is to stop any statistic based on a sum across neighbourhood links to be dominated by those neighbourhoods with most neighbours. Moran is one such statistic. Hence, if a region has six neighbours, each of those is given a weight of \\(1/6\\). If it has four, \\(1/4\\), and so forth. This is called row-standardisation and is the default style in the conversion of a neighbourhood to a spatial weights list with the function nb2listw(). See ?nb2listw for alternative specifications.\n\n\nCode\nspweight <- nb2listw(neighbours)\n# Here are the neighbours of and weights for the first region:\nspweight$neighbours[[1]]\n\n\n[1] 13 14 15 16\n\n\nCode\nspweight$weights[[1]]\n\n\n[1] 0.25 0.25 0.25 0.25\n\n\n\n\nCalculating the Moran’s value\nNow we have the spatial weights, we can run a Moran’s test to measure the strength of spatial autocorrelation in the municipal$No_schooling variable.\n\n\nCode\nmoran <- moran.test(municipal$No_schooling, spweight)\nmoran\n\n\n\n    Moran I test under randomisation\n\ndata:  municipal$No_schooling  \nweights: spweight    \n\nMoran I statistic standard deviate = 13.968, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.576519069      -0.004310345       0.001729177 \n\n\nThe Moran statistic is 0.577 and the 95% confidence interval is,\n\n\nCode\nz <- c(-1.96, 1.96)\nround(moran$estimate[1]  + z * sqrt(moran$estimate[3]), 3)\n\n\n[1] 0.495 0.658\n\n\nSince the confidence interval does not include the expected value of -0.004, we can conclude that there is statistically significant positive autocorrelation in the variables – municipalities with higher percentages of no schooling tend to be surrounded by other municipalities with the same, and similarly, low values tends to be surrounded by other ones that are low.\n\nThe expected value is very close to zero so what we are almost saying is that because the confidence interval does not span zero so there is evidence of positive spatial autocorrelation. Although this is quite close to being true, to actually be correct would require that the expected value of the statistic is zero with no spatial autocorrelation. It isn’t. It is presently -0.004 and approaches zero as the number of observations increases: \\(E(I) = -1 / (n - 1).\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "autocorrelation.html#moran-plot-and-local-moran-values",
    "href": "autocorrelation.html#moran-plot-and-local-moran-values",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran plot and local Moran values",
    "text": "Moran plot and local Moran values\nWhilst there is positive spatial autocorrelation in the values overall, not everywhere is surrounded by similar values. The following plot has 4 quadrants marked upon it. The top right indicates places where both they and their average neighbour have above average values of municipal$No_schooling. We can describe these as high-high clusters on the map. The bottom left indicates places where they and their average neighbour have below average values. These are low-low clusters. Both the high-high and the low-low contribute to positive spatial autocorrelation because, for these, the places and their neighbours display similar values. The two other quadrants do not. In the top left are low-high clusters. In the bottom right are high-low. There reveal clusters of dissimilar values (negative spatial autocorrelation). In the chart, the high-high and low-low places are more plentiful than the low-high and high-low ones, hence the upwards sloping line of best fit and the positive Moran statistic.\n\n\nCode\nmoran.plot(municipal$No_schooling, spweight)\n\n\n\n\n\nIt is straightforward to map which quadrant each place belongs to. First, we calculate the local Moran statistics proposed by Anselin (1995). A local statistic is one that applies to a subspace of the map, whereas a global statistic is a summary measure for the whole map. Anselin showed that the (global) Moran statistic can be decomposed into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours. There are 233 municipalities in the data so there will be 233 local Moran values too.\n\n\nCode\nlocalm <- localmoran(municipal$No_schooling, spweight)\n\n\nUsefully, if we look at the attributes of localm, we discover an attribute named quadr which contains what we want and which can be mapped. In fact, it includes three different versions of what we might want, the differences being due to which average low and high are defined by (see below the section Value in ?localmoran).\n\n\nCode\nnames(attributes(localm))\n\n\n[1] \"dim\"      \"dimnames\" \"call\"     \"class\"    \"quadr\"   \n\n\nCode\nquadr <- attr(localm, \"quadr\")\nhead(quadr)\n\n\n      mean   median    pysal\n1 Low-High Low-High Low-High\n2  Low-Low  Low-Low  Low-Low\n3  Low-Low  Low-Low  Low-Low\n4 High-Low High-Low High-Low\n5  Low-Low  Low-Low  Low-Low\n6  Low-Low  Low-Low  Low-Low\n\n\nCode\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"orange\", \"purple\", \"red\")) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nUnfortunately, the resulting map is somewhat misleading because not all of the local Moran values are statistically significant and some of the various high-high, low-low, etc. pairings my be only trivially alike or dissimilar. Looking at top of the local Moran data suggests a way of isolating those that are statistically significant and is adopted in the following map.\n\n\nCode\nhead(localm, n = 3)   # The p-values are in column 5\n\n\n          Ii          E.Ii     Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.1271124 -0.0055224972 0.31575429 -0.2163830      0.8286892\n2  0.1047729 -0.0019928929 0.07556466  0.3883944      0.6977242\n3  0.1385483 -0.0003126832 0.01187599  1.2742226      0.2025845\n\n\nCode\nquadr[localm[,5] > 0.05, ] <- NA\n\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"orange\", \"purple\", \"red\"), na.value = \"grey80\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nArguably, this new map may still not apply a strict enough definition of statistical significance because the issue of repeat testing has not been tackled. Remember, there are 233 places, 233 local Moran values and therefore 233 tests of significance. The p-values can be adjusted for this using R’s p.adjust() function. The following example uses a false discovery rate method (method = fdr) but other alternatives include method = bonferroni. A question is whether this is now too strict given that there are not 233 independent tests. Rather, the data have overlapping geographies (places share neighbours) as well as spatial dependencies.\n\n\nCode\nquadr[p.adjust(localm[,5], method = \"fdr\") > 0.05, ] <- NA\n\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"red\"), na.value = \"grey80\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant (p-adjusted) local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#issues-with-the-moran-statistic",
    "href": "autocorrelation.html#issues-with-the-moran-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Issues with the Moran statistic",
    "text": "Issues with the Moran statistic\n\nHow to define neighbours?\nAny statistic that includes spatial weights is dependent upon how those weights are defined: how, then, to decide which places are neighbours and also how much weight each neighbourhood connection is given in the calculation? The calculations above use first order contiguity (places that share a boundary) but we could extend that definition to include neighbours of neighbours (or more):\n\n\nCode\nneighbours <- nblag(neighbours, maxlag = 2)\n\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))   # Plot graphics in a 1 row by 2 column grid\nplot(st_geometry(municipal), border = \"grey\", main = \"First order contiguity\")\nplot(neighbours[[1]], pts, add = T)\nplot(st_geometry(municipal), border = \"grey\", main = \"Second order contiguity\")\nplot(neighbours[[2]], pts, add = T)\n\n\n\n\n\nChanging the definition of neighbours does, of course, change the Moran statistic and it will change the local Moran values too.\n\n\nCode\nsapply(neighbours, \\(x) {\n  nb2listw(x) %>%\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\n\nMoran I statistic Moran I statistic \n        0.5765191         0.3883236 \n\n\nThere is no particular reason to stick with a contiguity-based definition. We could, for example, look for the \\(k\\) nearest neighbours (or, more precisely, the \\(k\\) nearest centroids to the centroid of each region), as in the two examples below.\n\nThe function knearneigh() contains the default argument, longlat = NULL. It should be ok not to change this here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. If you suspect it isn’t or if coords is simply a matrix of point coordinates, not an explicitly spatial object, change the default argument to longlat = TRUE.\n\n\nCode\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))\n\nneighbours <- knn2nb(knearneigh(coords, k = 5))\nplot(st_geometry(municipal), border = \"grey\", main = \"Five nearest neighbours\")\nplot(neighbours, pts, add = T)\n\nneighbours <- knn2nb(knearneigh(coords, k = 10))\nplot(st_geometry(municipal), border = \"grey\", main = \"Ten nearest neighbours\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\nIn fact, we can run through all the possible values of \\(k\\) (from \\(1\\) to \\(k_{max} = (n - 1)\\) where \\(n\\) is the number of municipalities) and consider the Moran statistics that they generate. The resulting chart shows that the statistic is highly dependent on the scale of the analysis: as \\(k\\) increases, neighbourhood relationships extend over an increasing portion of the map, and the statistic tends to decline. It declines because nearby places tend to have similar values whereas those that are further away are more varied.\n\nThe code will generate a lot of warning messages. You can ignore them. They are warning you that \\(k\\) has become a large subset of all \\(n\\).\n\n\nCode\nn <- nrow(municipal)\ny <- sapply(1: (n-1), \\(k) {\n  knearneigh(coords, k) %>%\n    knn2nb %>%\n    nb2listw %>%\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\nggplot(data.frame(k = 1:(n-1), y = y), aes(x = k, y = y)) +\n  geom_line() +\n  ylab(\"Moran statistic\")\n\n\n\n\n\nIn principle the chart could be used to identify an ‘optimal’ value of \\(k\\). Unfortunately, the present case offers few clues as to what that optimal value should be. The p-value for these statistics is not shown but is least when \\(k = 27\\). That might be a justification, of sorts, for choosing \\(k = 27\\) but it is splitting hairs somewhat when all the p-values are tiny and well below \\(p = 0.001\\), for example.\nOther ways of defining neighbours include whether they lie within a lower or upper distance bound of each other: see ?dnearneigh and this vignette on Creating Neighbours.\n\n\nHow to interpet the I statistic\nConceptually, the Moran statistic is easy to understand: it is a (global) measure of the correlation between the values of a variable at a set of locations, and the value of the same variable for those locations’ neighbours. More simply, it is the correlation between locations and their neighbours.\nExcept it isn’t. Or, rather, it is a measure of correlation, just not the more commonly used Pearson correlation. The difference is evident in the following comparison where the Moran value is almost half the Pearson correlation between locations and their average neighbour (where the value for the average neighbour is calculated using last.listw()).\n\n\nCode\nspweight <- nb2listw(knn2nb(knearneigh(coords, 30)))\npearson <- cor(lag.listw(spweight, municipal$No_schooling), municipal$No_schooling)\nmoran <- moran.test(municipal$No_schooling, spweight)$estimate[1]\ndata.frame(person = pearson, moran = moran, row.names = \"correlation\")\n\n\n              person     moran\ncorrelation 0.636569 0.3346272\n\n\nAs Brunsdon and Comber note, the value of Moran’s I is not constrained to be in the range from -1 to +1 but changes with the spatial weights matrix. Following, Jong et al. (1984, p. 20), the extremes for the current spatial weights are,\n\n\nCode\nlistw2mat(spweight) %>%\n  range(eigen((. + t(.)) / 2))\n\n\n[1] -0.3985514  1.0317251\n\n\nNot only is this not in the range -1 to 1, it is not symmetric around the expected (null) value of -0.004.\nPersonally, I am not persuaded that the global Moran’s value is preferable to the more readily interpreted Pearson correlation between locations and their average neighbour. That correlation with \\(k = 30\\) nearest neighbours was calculated above and is 0.637. A confidence interval for this can be calculated using a permutation approach, for example. Given the geography of the South African municipalities, their neighbourhood relations and given the data values, then permuting those values randomly 1000 times around the municipalities generates a 95% confidence interval that the correlation is a long way out of, indicating the positive autocorrelation to be significant. (A permutation approach is also available for the global and local Moran statistics: see ?moran.mc and ?localmoran_perm.)\n\n\nCode\nn <- length(municipal$No_schooling)\nsapply(1:1000, \\(x) {\n  sample(municipal$No_schooling, n) %>% \n    cor(lag.listw(spweight, .))\n}) %>%\n  quantile(prob = c(0.025, 0.975))\n\n\n      2.5%      97.5% \n-0.2347918  0.1346138"
  },
  {
    "objectID": "autocorrelation.html#getis-and-ord-g-statistic",
    "href": "autocorrelation.html#getis-and-ord-g-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Getis and Ord G-Statistic",
    "text": "Getis and Ord G-Statistic\nA different method for identifying ‘hot’ or ‘cold spots’ of a variable is provided by the G-statistic. As Brunsdon and Comber observe, the statistic – which is a local statistic, calculated for each location in turn – is based on the proportion of the total sum of standardised attribute values that are within a threshold distance, \\(d\\), of each area centroid. Looking at the map below, which is of South African wards (simplified slightly from this source), we may suspect there are clusters of places with greater percentages of their populations having relatively high incomes and that these are spatial anomalies. Because the percentages are extremely skewed, they have been plotted with a square root transformation applied to the scale of the map (trans = \"sqrt\") – notice how the values 0 to 4% (which are the original values, not the square roots) occupy more of the legend than the values 4 to 8 do, and so forth.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nggplot(data = wards, aes(fill = High_income)) +\n  geom_sf(colour = \"transparent\") +\n  scale_fill_distiller(\"%\", guide = \"colourbar\", direction = 1, trans = \"sqrt\",\n                       palette = \"Reds\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Percentage of Population with income R614401 or greater\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )\n\n\n\n\n\nThe G-statistic requires a binary spatial weights (style = \"B\", below), whereby locations either are or are not within the threshold distance of each other. The following has a threshold distance of 20km. Not all of the ward centroids will be within 20km of another which creates situations of zero neighbours, which is tolerated by setting zero.policy = TRUE in the conversion from a neighbours to spatial list.\n\nAs with the function knearneigh(), dnearneigh() has the default argument, longlat = NULL, which should not need changing here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. However, don’t take this for granted.\n\n\nCode\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nneighbours <- dnearneigh(coords, 0, 20)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\n\nHaving calculated the local G values, we can map them, using a manually generated fill palette. The argument na.translate = F removes the NA values from the legend but not from the map, wherein they are shaded white (na.value = \"white\"). The G values are also standardised z-values, hence values of 1.96 or greater are relatively rate if the assumption that the statistic is Normally distributed is warranted. (If it isn’t, consider using localG_perm.)\n\n\nCode\nbrks <- c(min(wards$localG, na.rm = TRUE), -1.96, 1.96, 2.58, 3.29, max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\npal <- c(\"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 20km threshold\"\n  )\n\n\n\n\n\nLet’s now add a refinement to the map and, based on what we learned from the previous session, label the cities that appear to contain a hot spot of higher percentages of higher earners. Note the use of st_join() which is a spatial join: it identifies, from geography, which ward the cities are located in and appends the ward data, including the G statistics, to the cities, retaining only those that are in the most significant hot spots. Note that this definition of ‘most significant’ doesn’t address the problem of repeat testing which we also saw with the local Moran statistics and, of course, the results are dependent on the distance threshold defining which places are or are not neighbours.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\nunzip(\"cities.zip\")\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  filter(place == \"city\") %>%\n  st_join(wards) %>%\n  filter(localG > 3.29) ->\n  cities\n\nlast_plot() +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)\n\n\n\n\n\nHere are the results with a distance threshold of 100km.\n\n\nCode\nneighbours <- dnearneigh(coords, 0, 100)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\nbrks <- c(min(wards$localG, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 100km threshold\"\n  ) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)"
  },
  {
    "objectID": "autocorrelation.html#summary",
    "href": "autocorrelation.html#summary",
    "title": "Measuring spatial autocorrelation",
    "section": "Summary",
    "text": "Summary\nThis session has been about identifying and quantifying spatial clustering in data, using a combination of ‘global’ (whole map) and local statistics; specifically, Moran’s I, its local equivalent and the Getis-Ord G statistic. These statistics are dependent on the spatial weights matrix used in their calculation, which does, unfortunately, create something of a vicious circle: ideally that matrix would be calibrated to the spatial patterns evident in the data but they are only evident once the weights matrix has been specified. One option is to take the view that looking at the patterns at a range of scales is itself revealing so choose lots of bandwidths, not just one, and treat it as a multiscale analysis, as in the following.\n\n\nCode\n# Sets the threshold distance from 20 to 180km in intervals of 20\n# I chose 9 distances because they can be shown in a 3 x 3 grid of maps\nd <- seq(from = 20, by = 20, length.out = 9)\n\n# Calculate the G statistics for each distance and save them\n# as a list of sf features (maps)\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nmaps <- lapply(d, \\(x) {\n  neighbours <- dnearneigh(coords, 0, x)\n  spweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\n  wards$localG <- localG(wards$High_income, spweight)\n  wards$distance <- x\n  return(wards)\n})\n\n# Bind the maps together into a single object to be used\n# for the faceting in ggplot2\nmaps <- do.call(rbind, maps)\n\nbrks <- c(min(maps$localG, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(maps$localG, na.rm = TRUE))\nmaps$localG_gp <- cut(maps$localG, brks, include.lowest = TRUE)\n\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot(data = maps, aes(fill = localG_gp)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  facet_wrap(~ distance) +\n  theme_light() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank(),\n        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),\n        axis.text.x = element_blank(), axis.text.y = element_blank(),\n        legend.position = \"bottom\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "autocorrelation.html#further-reading",
    "href": "autocorrelation.html#further-reading",
    "title": "Measuring spatial autocorrelation",
    "section": "Further Reading",
    "text": "Further Reading\n\nChapter 8 on Localised Spatial Analysis in An Introduction to R for Spatial Analysis & Mapping by Chris Brunsdon and Lex Comber."
  },
  {
    "objectID": "base.html#a-short-guide-to-base-r",
    "href": "base.html#a-short-guide-to-base-r",
    "title": "Base R",
    "section": "A Short Guide to Base R",
    "text": "A Short Guide to Base R\nBase R is what you download from CRAN. You might think of it as classic R. A short introduction of ‘the basics’ is provided below."
  },
  {
    "objectID": "base.html#functions",
    "href": "base.html#functions",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\nR is a functional programming language where functions ‘do things’ to objects. What they do is dependent upon the class/type and attributes of the objects that go into the function, and also on the arguments of the function.\nFor example, try typing the following into the R Console, which is the bottom left panel of R Studio. Type it alongside the prompt symbol, > then hit Enter/Return.\n\n\nCode\nround(10.32, digits = 0)\n\n\n[1] 10\n\n\nThis calls the function round(), which is operating on the numeric object, 10.32. The argument digits specifies the number of digits to round to. It is set to zero in the example above. Since digits = 0 is the default value for the function, we could just write\n\n\nCode\nround(10.32)\n\n\n[1] 10\n\n\nand obtain the same answer. I know that digits = 0 is the default value because, as I type the name of the function into the R Console, I see the arguments of the function and any default values appear.\n\nWe can also find out more about the function, including some examples of its use, by opening its help file.\n\n\nCode\n?round\n\n\n Because digits = 0 is the default value, if we want to round to one digit, we need to specify the argument explicitly.\n\n\nCode\nround(10.32, digits = 1)\n\n\n[1] 10.3\n\n\nThe following also works because it preserves the order of the arguments in the function.\n\n\nCode\nround(10.32, 1)\n\n\n[1] 10.3\n\n\nIn other words, if we do not specifically state that x = 10.32 and digits = 1 then they will be taken to be the first and second arguments of the function.\n In these examples, both the input to and output from the function are a numeric vector of type double. The input is:\n\n\nCode\nclass(10.32)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(10.32)\n\n\n[1] \"double\"\n\n\nThe output is:\n\n\nCode\nclass(round(10.32, digits = 1))\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(round(10.32, digits = 1))\n\n\n[1] \"double\"\n\n\nNote how a function can be wrapped within a function, as in the example above: class(round(...)).\nWhereas 10.32 is a numeric vector of length 1,\n\n\nCode\nlength(10.32)\n\n\n[1] 1\n\n\nthe round() function can operate on numeric vectors of other lengths too.\n\n\nCode\nround(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7))\n\n\n[1] 1 2 3 4 6 7 8\n\n\nHere the combine function, c() is used to create a vector of length 7, which is the input into round(). The output is of length 7 too.\n\n\nCode\nlength(round(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7)))\n\n\n[1] 7\n\n\n\nThere are lots of functions for R and I often forget what I need. Fortunately, there is a large user community too and so a quick web search often helps me quickly to find what I need.\n\nWriting a new function\nWe can write our own functions. The following will take a number and report whether it is a prime number or not.\n\n\nCode\nis.prime <- function(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}\n\n\nLet’s try it.\n\n\nCode\nis.prime(2)\n\n\n[1] TRUE\n\n\nCode\nis.prime(10)\n\n\n[1] FALSE\n\n\nCode\nis.prime(13)\n\n\n[1] TRUE\n\n\nCode\nis.prime(3.3)\n\n\n[1] NA\n\n\nThere is quite a lot to unpack about the function. It is not all immediately relevant but it is instructive to have an overview of what it is doing. First of all the function takes the form\nf <- function(x) {\n  ...\n}\nwhere x is the input into the function in much the same way that x is the number to be rounded in round(x = ...). It a ‘place holder’ for the input into the function.\nStatements such as if(x == 2) are logical statements: if(...) is true then do whatever follows. Where what is to be done spans over multiple lines, they are enclosed (like the function itself) by ‘curly brackets’, {...}.\nThe statement if(x < 2 | x %% floor(x) != 0) in the function is also a logical statement with the inclusion of an or statement, denoted by |. What it is checking is whether x < 2 or if x is a fraction. Had we needed to have both conditions to be met, then an and statement would be used, denoted by & instead of |. Note that ! means not, so != tests for not equal to and is the opposite of ==, which tests for equality.\nWhere it says, 2:(x-1), this is equivalent to the function, seq(from = 2, to = (x-1), by = 1). It generates a sequence of integer numbers from 2 to (x-1).\nifelse is another logical statement. It takes the form, ifelse(condition, a, b): if the condition is met then do a, else do b. In the prime number function it is checking whether dividing \\(x\\) by any of the numbers from 2 to \\(x-1\\) generates a whole number.\nFinally, the function return() returns an output from the function; specifically, a logical vector of length 1 that is TRUE, FALSE or NA dependent upon whether \\(x\\) is or is not a prime number, or if it is not a whole number above 1.\nNote that in newer versions of R, functions can also take the form,\nf <- \\(x) {\n  ...\n}\nTherefore the following is exactly equivalent to before.\n\n\nCode\nis.prime <- \\(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "base.html#objects-and-classes",
    "href": "base.html#objects-and-classes",
    "title": "Base R",
    "section": "Objects and Classes",
    "text": "Objects and Classes\nOur function that checks for a prime number is stored in the object is.prime.\n\n\nCode\nclass(is.prime)\n\n\n[1] \"function\"\n\n\nThere are other classes of object in R. Some of the most common are listed below.\n\nLogical\nThe output from the is.prime() function is an example of an object of class logical because the answer is TRUE or FALSE (or NA, not applicable).\n\n\nCode\nx <- is.prime(10)\nprint(x)\n\n\n[1] FALSE\n\n\nCode\nclass(x)\n\n\n[1] \"logical\"\n\n\nSome other examples:\n\n\nCode\ny <- 10 > 5\nprint(y)\n\n\n[1] TRUE\n\n\nCode\nclass(y)\n\n\n[1] \"logical\"\n\n\nCode\nz <- 2 == 5   # is 2 equal to 5?\nprint(z)\n\n\n[1] FALSE\n\n\n\nNote that # indicates a comment in the code. If you cut and paste the line y <- 2 == 5 # is 2 equal to 5? into the Console then the command y <- 2 == 5 will be run, whereas the comment # is 2 equal to 5? will be ignored as it is just there for information.\n\n\nNumeric\nWe have already seen that some objects are numeric.\n\n\nCode\nx <- mean(0:100)\nprint(x)\n\n\n[1] 50\n\n\nCode\nclass(x)\n\n\n[1] \"numeric\"\n\n\nThis presently is of type double (i.e. it allows for decimal places even where they are not required)\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nbut could be converted to class integer.\n\n\nCode\nx <- as.integer(x)\nclass(x)\n\n\n[1] \"integer\"\n\n\n\n\nCharacter\nOther classes include character. Note the difference between the length() of a character vector and the number of characters, nchar(), that any element of that vector contains.\n\n\nCode\nx <- \"Mapping and Modelling in R\"\nprint(x)\n\n\n[1] \"Mapping and Modelling in R\"\n\n\nCode\nlength(x)   # There is only one element in this vector\n\n\n[1] 1\n\n\nCode\nnchar(x)    # And that element contains 26 letters\n\n\n[1] 26\n\n\nCode\nclass(x)\n\n\n[1] \"character\"\n\n\nCode\ny <- paste(x, \"with Richard Harris\")\nprint(y)\n\n\n[1] \"Mapping and Modelling in R with Richard Harris\"\n\n\nCode\nlength(y)   # There is still only one element\n\n\n[1] 1\n\n\nCode\nnchar(y)    # But now it contains more letters\n\n\n[1] 46\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nCode\nz <- unlist(strsplit(x, \" \"))\nprint(z)\n\n\n[1] \"Mapping\"   \"and\"       \"Modelling\" \"in\"        \"R\"        \n\n\nCode\nlength(z)   # The initial vectors has been split into 5 parts\n\n\n[1] 5\n\n\nCode\nnchar(z)\n\n\n[1] 7 3 9 2 1\n\n\nCode\nclass(z)\n\n\n[1] \"character\"\n\n\n\nAs the name suggests, print() is a function that prints its contents to screen. Often it can be omitted in favour of referencing the object directly. For instance, in the example above, rather than typing print(z) it would be sufficient just to type z. Just occasionally though you will find that an object does not print as you intended when the function is omitted. If this happens, try putting print() back in.\n\n\nMatrix\nAn example of a matrix is\n\n\nCode\nx <- matrix(1:9, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nCode\nncol(x)   # Number of columns\n\n\n[1] 3\n\n\nCode\nnrow(x)   # Number of rows\n\n\n[1] 3\n\n\nCode\nclass(x)\n\n\n[1] \"matrix\" \"array\" \n\n\nHere the argument byrow is changed from its default value of FALSE to be TRUE:\n\n\nCode\ny <- matrix(1:9, ncol = 3, byrow = TRUE)\n\n\nThis result is equivalent to the transpose of the original matrix.\n\n\nCode\nt(x)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\nData frame\nA data.frame is a table of data, such as,\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nncol(df)\n\n\n[1] 4\n\n\nCode\nnrow(df)\n\n\n[1] 7\n\n\nCode\nlength(df)  # The length is the number of columns\n\n\n[1] 4\n\n\nCode\nnames(df)   # The names of the variables in the data frame\n\n\n[1] \"Day\"   \"Date\"  \"Month\" \"Year\" \n\n\nNote that the length of each column should be equal. The following will generate an error because the Date column is now too short. You might wonder why the Month and Year columns were fine previously. It is because R recycled them the requisite number of times (i.e. it gave all the rows the same value for Month and Year).\n# This will generate an error\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:25,\n                 Month = \"June\",\n                 Year = 2022)\n\n\nFactors\nEarlier versions of R would, by default, convert character fields in a data frame into factors, as in,\n\n\nCode\ndf2 <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022, stringsAsFactors = TRUE)\n\n\nThe difference is not immediately obvious,\n\n\nCode\nhead(df, n= 2)    # with stringsAsFactors = FALSE (the default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nCode\nhead(df2, n = 2)  # with stringsAsFactors = TRUE\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nbut begins to be apparent in the following:\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf2$Day\n\n\n[1] Mon   Tues  Wed   Thurs Fri   Sat   Sun  \nLevels: Fri Mon Sat Sun Thurs Tues Wed\n\n\nCode\ndf$Month\n\n\n[1] \"June\" \"June\" \"June\" \"June\" \"June\" \"June\" \"June\"\n\n\nCode\ndf2$Month\n\n\n[1] June June June June June June June\nLevels: June\n\n\nBasically, a factor is a categorical variable: it encodes which groups/categories (which levels) are to be found in the variable. Knowing this, it is possible to count the number of each group, as in,\n\n\nCode\nsummary(df2)\n\n\n    Day         Date       Month        Year     \n Fri  :1   Min.   :20.0   June:7   Min.   :2022  \n Mon  :1   1st Qu.:21.5            1st Qu.:2022  \n Sat  :1   Median :23.0            Median :2022  \n Sun  :1   Mean   :23.0            Mean   :2022  \n Thurs:1   3rd Qu.:24.5            3rd Qu.:2022  \n Tues :1   Max.   :26.0            Max.   :2022  \n Wed  :1                                         \n\n\nbut not\n\n\nCode\nsummary(df)\n\n\n     Day                 Date         Month                Year     \n Length:7           Min.   :20.0   Length:7           Min.   :2022  \n Class :character   1st Qu.:21.5   Class :character   1st Qu.:2022  \n Mode  :character   Median :23.0   Mode  :character   Median :2022  \n                    Mean   :23.0                      Mean   :2022  \n                    3rd Qu.:24.5                      3rd Qu.:2022  \n                    Max.   :26.0                      Max.   :2022  \n\n\nFactors can be useful but do not always behave as you might anticipate. For example,\n\n\nCode\nx <- factor(c(\"2021\", \"2022\"))\nas.numeric(x)\n\n\n[1] 1 2\n\n\nis different from,\n\n\nCode\nx <- c(\"2021\", \"2022\")\nas.numeric(x)\n\n\n[1] 2021 2022\n\n\nNow the deafult is stringsAsFactors = FALSE, which is better when using functions such as read.csv() to read a .csv file into a data.frame in R.\n\n\nLists\nA list is a more flexible class that can hold together other types of object. Without a list, the following only works because the 1:3 are coerced from numbers in x to characters in y – note the \" \" around them, which shows they are now text.\n\n\nCode\nx <- as.integer(1:3)\nclass(x)\n\n\n[1] \"integer\"\n\n\nCode\ny <- c(\"a\", x)\ny\n\n\n[1] \"a\" \"1\" \"2\" \"3\"\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nOn the other hand,\n\n\nCode\ny <- list(\"a\", x)\n\n\ncreates a ragged list of two parts:\n\n\nCode\nclass(y)\n\n\n[1] \"list\"\n\n\nCode\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n\nThe first part has the character \"a\" in it.\n\n\nCode\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\nclass(y[[1]])\n\n\n[1] \"character\"\n\n\nThe second has the numbers 1 to 3 in it.\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\nclass(y[[2]])\n\n\n[1] \"integer\"\n\n\nNote that the length of the list is the length of its parts, of which the following example has three.\n\n\nCode\ny <- list(\"a\", x, df)\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nlength(y)\n\n\n[1] 3\n\n\nThis should not be confused with the length of any one part.\n\n\nCode\nlength(y[[1]])\n\n\n[1] 1\n\n\nCode\nlength(y[[2]])\n\n\n[1] 3\n\n\nCode\nlength(y[[3]])\n\n\n[1] 4"
  },
  {
    "objectID": "base.html#assignments",
    "href": "base.html#assignments",
    "title": "Base R",
    "section": "Assignments",
    "text": "Assignments\nThroughout this document I have used the assignment term <- to store the output of a function, as in x <- as.integer(1:3) and y <- list(\"a\", x, df), and so forth. The <- is used to assign the result of a function to an object. You can, if you prefer use =. For example, all the following make the same assignment, which is to give x the value of 1.\n\n\nCode\nx <- 1\nx = 1\n1 -> x\n\n\nPersonally, I tend to avoid using = so not to confuse assignments with arguments,\n\n\nCode\nx <- round(10.32, digits = 1)   # I think this is a bit clearer\nx = round(10.32, digits = 1)    # and this a bit less so\n\n\nalso not assignments with logical statements,\n\n\nCode\nx <- 1\ny <- 2\nz <- x == y   # Again, this is a bit clearer\nz = x == y    # and this not so much\n\n\nand, albeit pedantic, to avoid the following sort of situation which makes no sense mathematically…\n\n\nCode\nx = 1\ny = 2\nx = y\n\n\n… but does in terms of what it really means:\n\n\nCode\nx <- 1\ny <- 2\nx <- y # Assign the value of y to x, overwriting its previous value\n\n\nWhich you use is a matter of personal preference and, of course, = has one less character than <- to worry about. However, this course is written with,\n<- (or ->) is as assignment, as in x <- 1;\n= is the value of an argument, as in round(x, digits = 1); and\n== is a logical test for equality, as in x == y.\n\nIt is important to remember that R is case sensitive. An object called x is different from one called X; y is not the same as Y and so forth."
  },
  {
    "objectID": "base.html#manipulating-objects",
    "href": "base.html#manipulating-objects",
    "title": "Base R",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nIn addition to passing objects to functions such as,\n\n\nCode\nx <- 0:100\nmean(x)\n\n\n[1] 50\n\n\nCode\nsum(x)\n\n\n[1] 5050\n\n\nCode\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      25      50      50      75     100 \n\n\nCode\nmedian(x)\n\n\n[1] 50\n\n\nCode\nquantile(x, probs = c(0, 0.25, 0.5, 0.75, 1))\n\n\n  0%  25%  50%  75% 100% \n   0   25   50   75  100 \n\n\nCode\nhead(sqrt(x)) # The square roots of the first of x\n\n\n[1] 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nCode\ntail(x^2)     # The square roots of the last of x\n\n\n[1]  9025  9216  9409  9604  9801 10000\n\n\nCode\nsd(x)         # The standard deviation of x\n\n\n[1] 29.30017\n\n\nthere are other ways we may wish to interact with objects.\n\nMathematical operations\nMathematical operations generally operate on a pairwise basis between corresponding elements in a vector. For example,\n\n\nCode\nx <- 1\ny <- 3\nx + y\n\n\n[1] 4\n\n\nCode\nx <- 1:5\ny <- 6:10\nx + y\n\n\n[1]  7  9 11 13 15\n\n\nCode\nx * y   # Multiplication\n\n\n[1]  6 14 24 36 50\n\n\nCode\nx / y   # Divisions\n\n\n[1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000\n\n\nIf one vector is shorter that the other, values will be recycled. In the following example the results are \\(1\\times6\\), \\(2\\times7\\), \\(3\\times8\\), \\(4\\times9\\) and then \\(5\\times6\\) as y is recycled.\n\n\nCode\nx <- 1:5\ny <- 6:9\nx * y\n\n\n[1]  6 14 24 36 30\n\n\n\n\nSubsets of objects\n\nVectors\nIf x is a vector then x[n] is the nth element (the nth position, the nth item) in the vector. To illustrate,\n\n\nCode\nx <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nx[1]\n\n\n[1] \"a\"\n\n\nCode\nx[3]\n\n\n[1] \"c\"\n\n\nCode\nx[c(1, 3, 5)]\n\n\n[1] \"a\" \"c\" \"e\"\n\n\nCode\nx[length(x)]\n\n\n[1] \"f\"\n\n\nThe notation -n can be used to exclude elements.\n\n\nCode\nx[-3]   # All of x except the 3rd element\n\n\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n\nCode\nx[c(-1, -3, -5)]    # x without the 1st, 3rd and 5th elements\n\n\n[1] \"b\" \"d\" \"f\"\n\n\n\n\nMatrices\nIf x is a matrix then x[i, j] is the value of the ith row of the jth column:\n\n\nCode\nx <- matrix(1:10, ncol = 2)\nx\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[1, 1]     # row 1, column 1\n\n\n[1] 1\n\n\nCode\nx[2, 1]     # row 2, column 1\n\n\n[1] 2\n\n\nCode\nx[c(3, 5), 2]   # rows 3 and 5 of column 2\n\n\n[1]  8 10\n\n\nCode\nx[nrow(x), ncol(x)]   # the final entry in the matrix\n\n\n[1] 10\n\n\nAll of the values in the ith row can be selected using the form x[i, ]\n\n\nCode\nx[1, ]    # row 1\n\n\n[1] 1 6\n\n\nCode\nx[3, ]    # row 3\n\n\n[1] 3 8\n\n\nCode\nx[c(1, 5), ]  # rows 1 and 5\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    5   10\n\n\nCode\nx[c(-1, -3), ]  # All except the 1st and 3rd rows\n\n\n     [,1] [,2]\n[1,]    2    7\n[2,]    4    9\n[3,]    5   10\n\n\nand all of the values in the jth column can be selected using the form x[, j]\n\n\nCode\nx[ ,1]    # column 1\n\n\n[1] 1 2 3 4 5\n\n\nCode\nx[ ,2]    # column 2\n\n\n[1]  6  7  8  9 10\n\n\nCode\nx[ , 1:2]   # columns 1 and 2\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[-3 , 1:2]   # columns 1 and 2 except row 3\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    4    9\n[4,]    5   10\n\n\n\n\nData frames\nData frames are not unlike a matrix.\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf[, 1]   # The first column\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, 1]  # The first row of the first column (Day)\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, 2]  # The second row of the second column (Date)\n\n\n[1] 21\n\n\nHowever, you can also use reference the variable name directly, through the x$variable style notation,\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ndf$Date[2]\n\n\n[1] 21\n\n\nor, if you wish, with the square brackets, using the [, \"variable\"] format.\n\n\nCode\ndf[, \"Day\"]\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, \"Day\"]\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, \"Date\"]\n\n\n[1] 21\n\n\n\n\nLists\nWe have already seen the use of double square brackets, [[...]] to refer to a part of a list:\n\n\nCode\nx <- 1:3\ny <- list(\"a\", x, df)\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\ny[[3]]\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nThe extension to this is to be able to refer to a specific element within a part of the list by combining it with the other notation. Some examples are:\n\n\nCode\ny[[1]][1]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]][3]\n\n\n[1] 3\n\n\nCode\ny[[3]]$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ny[[3]]$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ny[[3]][2, \"Date\"]\n\n\n[1] 21"
  },
  {
    "objectID": "base.html#deleting-objects-and-saving-the-workspace",
    "href": "base.html#deleting-objects-and-saving-the-workspace",
    "title": "Base R",
    "section": "Deleting objects and saving the workspace",
    "text": "Deleting objects and saving the workspace\nMy current workspace is\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/profr/Dropbox/github/MandM\"\n\n\nand it contains the following objects:\n\n\nCode\nls()\n\n\n[1] \"df\"       \"df2\"      \"is.prime\" \"x\"        \"y\"        \"z\"       \n\n\nTo delete a specific object, use rm(),\n\n\nCode\nrm(z)\n\n\nOr, more than one,\n\n\nCode\nrm(df, df2, is.prime)\n\n\nTo save the workspace and all the objects it now contains use the save.image() function.\n\n\nCode\nsave.image(\"workspace1.RData\")\n\n\nTo delete all the objects created in your workspace, use\n\n\nCode\nrm(list=ls())\n\n\n\nIt is a good idea to save a workspace with a new filename before deleting too much from your workspace to allow you to recover it if necessary. Be especially careful if use rm(list=ls()) as there is no undo function.\nTo (re)load a workspace, use load().\n\n\nCode\nload(\"workspace1.RData\")"
  },
  {
    "objectID": "base.html#further-reading",
    "href": "base.html#further-reading",
    "title": "Base R",
    "section": "Further reading",
    "text": "Further reading\nThis short introduction to base R has really only scratched the surface. For a fuller introduction, see the software manual, An Introduction to R.\nDon’t worry if not everything makes sense at this stage. The best way to learn R is to put it into practice and that is what we shall be doing in later sessions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Mapping and Modelling Geographic Data in R\nRichard Harris, School of Geographical Sciences, University of Bristol\n\nThis is a work in progress and a long way from completion."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "",
    "section": "About the course",
    "text": "About the course\nThe aims of this course are to:\n[TO BE ADDED]"
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "",
    "section": "About the author",
    "text": "About the author\nThis course is authored by Richard Harris, Professor of Quantitative Social Geography at the University of Bristol. You can find out more about me, my research and other interests at https://profrichharris.github.io/.\n @profrichharris"
  },
  {
    "objectID": "index.html#useful-resources",
    "href": "index.html#useful-resources",
    "title": "",
    "section": "Useful Resources",
    "text": "Useful Resources\n[TO BE ADDED]"
  },
  {
    "objectID": "index.html#copyright-notice",
    "href": "index.html#copyright-notice",
    "title": "",
    "section": "Copyright notice",
    "text": "Copyright notice\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "markdown_example.html",
    "href": "markdown_example.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "markdown_example.html#scripts",
    "href": "markdown_example.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is based on the opening parts of Geographic Data in ipumsr, where ipumsr provides “an easy way to import census, survey and geographic data provided by ‘IPUMS’ into R”, and IPUMS “provides census and survey data from around the world integrated across time and space.” I have modified their code for greater consistency with other parts of this course but what it does is largely the same:\n\nIt loads some example data and manipulates it to calculate the percentages of people using solid fuels for cooking in regions of Colombia at three time periods. These are the attribute data\nIt loads a geographic boundary file of those regions. This is the map.\nIt simplifies (reduces the detail of) the map to make it faster for plotting.\nIt joins the attribute data to the map by means of their common geography.\nIt produces a choropleth map (thematic map) of the geographic variation in the percentages using solid fuel.\nIt saves the spatially joined data as a shapefile.\n\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the following maps.\n\nBe patient whilst the code takes a few moments to run.\n\n\n\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety.\nAs suggested earlier, an advantage of using a script is that it is easy to make changes to and then re-run it either in part or in full. In the script, find the type = \"cartolight\" argument in the line that says annotation_map_tile(type = \"cartolight\", progress = \"none\") + and change the type to any other of the following, such as cartodark.\n\n\nCode\nrosm::osm.types()\n\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"hikebike\"              \n [7] \"hillshade\"              \"osmgrayscale\"           \"stamenbw\"              \n[10] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[13] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\nIf you wish to view what the different types look like, you can do so here.\nOnce you have made the change, select the parts of the script you wish to rerun (everything under Map the data should be sufficient) and press the Run button or hit command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) to execute the selected code.\n\nA note about the :: notation\nThe use of the :: notation in the code rosm::osm.types() allows a function to be run from a package that has not been loaded. In the example, osm.types() is a function in the rosm package. That package provides access to Open Steet Map and other maps tiles. We could also require(rosm) and then use the function directly as we have in other cases, i.e. using osm.types() instead of rosm::osm.types(). However, sometimes, if a function only is to be used once then there is no need to require the whole package. Also, sometimes we load multiple packages that have functions within them that share the same name. In such circumstances, the package::function format may be required to make sure the correct function (from the correct package) is being called."
  },
  {
    "objectID": "markdown_example.html#r-markdown",
    "href": "markdown_example.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs and to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -> New File -> R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet.\n\nThe source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "markdown_example.html#summary",
    "href": "markdown_example.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "markdown_example.html#further-reading",
    "href": "markdown_example.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/."
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "programming.html#scripts",
    "href": "programming.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is based on the opening parts of Geographic Data in ipumsr, where ipumsr provides “an easy way to import census, survey and geographic data provided by ‘IPUMS’ into R”, and IPUMS “provides census and survey data from around the world integrated across time and space.” I have modified their code for greater consistency with other parts of this course but what it does is largely the same:\n\nIt loads some example data and manipulates it to calculate the percentages of people using solid fuels for cooking in regions of Colombia at three time periods. These are the attribute data\nIt loads a geographic boundary file of those regions. This is the map.\nIt simplifies (reduces the detail of) the map to make it faster for plotting.\nIt joins the attribute data to the map by means of their common geography.\nIt produces a choropleth map (thematic map) of the geographic variation in the percentages using solid fuel.\nIt saves the spatially joined data as a shapefile.\n\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the following maps.\n\nBe patient whilst the code takes a few moments to run.\n\n\n\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety.\nAs suggested earlier, an advantage of using a script is that it is easy to make changes to and then re-run it either in part or in full. In the script, find the type = \"cartolight\" argument in the line that says annotation_map_tile(type = \"cartolight\", progress = \"none\") + and change the type to any other of the following, such as cartodark.\n\n\nCode\nrosm::osm.types()\n\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"hikebike\"              \n [7] \"hillshade\"              \"osmgrayscale\"           \"stamenbw\"              \n[10] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[13] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\nIf you wish to view what the different types look like, you can do so here.\nOnce you have made the change, select the parts of the script you wish to rerun (everything under Map the data should be sufficient) and press the Run button or hit command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) to execute the selected code.\n\nA note about the :: notation\nThe use of the :: notation in the code rosm::osm.types() allows a function to be run from a package that has not been loaded. In the example, osm.types() is a function in the rosm package. That package provides access to Open Steet Map and other maps tiles. We could also require(rosm) and then use the function directly as we have in other cases, i.e. using osm.types() instead of rosm::osm.types(). However, sometimes, if a function only is to be used once then there is no need to require the whole package. Also, sometimes we load multiple packages that have functions within them that share the same name. In such circumstances, the package::function format may be required to make sure the correct function (from the correct package) is being called."
  },
  {
    "objectID": "programming.html#r-markdown",
    "href": "programming.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs and to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -> New File -> R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet.\n\nThe source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "programming.html#summary",
    "href": "programming.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "programming.html#further-reading",
    "href": "programming.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/."
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Getting Started",
    "section": "",
    "text": "For this course we will be using R. R is a free software environment for statistical computing and graphics. To run the code blocks for this course on your own computer you will need to have installed R. This is available for Linux, MacOS and Windows."
  },
  {
    "objectID": "start.html#install-r-studio",
    "href": "start.html#install-r-studio",
    "title": "Getting Started",
    "section": "Install R Studio",
    "text": "Install R Studio\nRStudio is an integrated development environment (IDE) that can make programming and other tasks easier in R. An open source edition is available to download and install.\n\nYou need to install R before you install R Studio."
  },
  {
    "objectID": "start.html#open-r-studio",
    "href": "start.html#open-r-studio",
    "title": "Getting Started",
    "section": "Open R Studio",
    "text": "Open R Studio\nOnce R and R Studio are installed, open R Studio on your computer and type the following in the Console to the left or bottom left of the screen, alongside the prompt, >.\n\n\nCode\n1 + 1\n\n\nthen hit Enter/Return. You should, of course, obtain the answer 2, as below.\n\n\n[1] 2\n\n\nYou will also find that if you move your mouse to over the code block above, an option appears to copy the code to the clipboard."
  },
  {
    "objectID": "start.html#install-additional-librariespackages",
    "href": "start.html#install-additional-librariespackages",
    "title": "Getting Started",
    "section": "Install additional libraries/packages",
    "text": "Install additional libraries/packages\nThe base functions of R are greatly extended by the very many packages/libraries that have been developed for it. At the time of writing, there are 18405 of these on CRAN, which is the main repository for them. Many of these have been grouped into ‘tasks’ and topic areas, which can be viewed here.\nMost of the packages that will be needed for this course will be installed as they are needed. However, some will be used so regularly that we should install them now. Cut and paste the following code chunk into the Console and hit Enter/Return.\n\n\nCode\ninstall.packages(\"sf\", dependencies = TRUE)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n\n\n\nRun the code above even if you have the packages installed already so that you also have available all the packages that these depend upon and link to."
  },
  {
    "objectID": "start.html#changing-some-default-settings",
    "href": "start.html#changing-some-default-settings",
    "title": "Getting Started",
    "section": "Changing some default settings",
    "text": "Changing some default settings\nIf you type getwd() into the R Console you will obtain your current working directory – the default location to look for files and to save content to. Mine is,\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/profr/Dropbox/github/MandM\"\n\n\nYou may wish to change this to something else each time you start R. You can do this using the drop-down menus (there is also the function setwd(dir) – type ?setwd in the R Console to learn more).\n\nYou may notice that I prefer a blue to a white screen when working in R. To change it to this, from the drop-down menus use Tools –> Global Options… -> Appearance and select Solarized Dark as the Editor theme. You may have your own preference."
  },
  {
    "objectID": "themap.html",
    "href": "themap.html",
    "title": "Maps as Representations of Spatial Variation",
    "section": "",
    "text": "When we look at a map such as the following, which is a choropleth (or thematic) map showing the percentage of the population with no experience of schooling in each of the South African municipalities in 2011, one thing should be immediately obvious: the areas are shaded in a range of colours; they are not all the same. This is because the values that the colours represent vary across the country with some places having a greater percentage of their population without schooling than others. In this way, the map reveals and also visually represents the spatial (geographic) variation in the variable of interest. The map portrays the geographic pattern.\n\nIt is not surprising to find variation. It is improbable that all the values would be the same. It is nearly always possible to find that some places have lower or higher values than others and to colour the map accordingly. Nevertheless, two characteristics of the spatial variation appear evident in the map.\n\nSpatial heterogeneity. This is the idea that the values typical in one part of the map are not typical in another. To put it simply, some parts of the map are shaded in blue whereas others are in red and those parts seem neither randomly nor regularly distributed because of…\nSpatial clustering. This is the idea that values found in one part of the map tend to be surrounded by similar values in neighbouring parts of the map. In other words, there are patches of blue and patches of red coloured areas on the map – blue tends be near blue and red tends to be near red Another name for this is positive spatial autocorrelation: values tend to be more similar to nearby other values than they are to distant ones.\n\nEvidence of spatial clustering supports Waldo Tobler’s much cited ‘first law’ of geography: “everything is related to everything else, but near things are more related than distant things.” It isn’t really a law because it is by no means always true: sometimes neighbouring places can have very different characteristics (there can be sharp changes across borders; spatial discontinuities). However, it does suggest that places tend to be situated within broader spatial contexts such that the processes that both generate and are generated by those contexts have a spatial expression and root. They map may reveal evidence of historic and on-going processes of socio-spatial inequalities, for example.\nTaken together, these characteristics of spatial variation indicate spatial dependencies, whereby the measured attributes of one place are not independent of other places. This dependence has statistical consequences if assumptions of independence are violated. Of more substantive geographic interest is how they have arisen – which processes are they caused by or associated with? Why are places not all the same? Why is there a geographical pattern?"
  },
  {
    "objectID": "themap.html#from-the-map-towards-models",
    "href": "themap.html#from-the-map-towards-models",
    "title": "Maps as Representations of Spatial Variation",
    "section": "From the map towards models",
    "text": "From the map towards models\nWith the above questions in mind, we might imagine the map as a first stage in a process of geographical enquiry where what we do is look for and then quantify some of the geographical patterns in the data before beginning to model them. Here the map is not simply a tool for visualising and communicating data, it is also a tool for exploring data and thinking geographically about them.\n\n\n\n\n\n\n In practice, the process of analysis is likely to involve greater cycling between the various stages. Nevertheless, there is a good argument for starting with the map."
  },
  {
    "objectID": "thematicmaps.html",
    "href": "thematicmaps.html",
    "title": "Thematic maps in R",
    "section": "",
    "text": "There are lots of ways to produce maps in R. But, however, they are drawn, two things are usually need to produce a choropleth map of the sort seen in the previous session: some data, and a map to join the data to. Once we have those, R offers plenty of options to produce quick or publication quality maps, which may have either static or dynamic content."
  },
  {
    "objectID": "thematicmaps.html#getting-started",
    "href": "thematicmaps.html#getting-started",
    "title": "Thematic maps in R",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoad the data\nLet’s begin with the easy bit and load the data, which are from http://superweb.statssa.gov.za. These includes the variable No_schooling which is the percentage of the population without schooling per South African municipality in 2011.\n\n\nCode\ninstalled <- installed.packages()[,1]\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\nrequire(tidyverse)\n\neducation <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/education.csv\")\nprint(education, n = 3)\n\n\n# A tibble: 234 × 8\n  LocalMunicipality… LocalMunicipali… No_schooling Some_primary Complete_primary\n  <chr>              <chr>                   <dbl>        <dbl>            <dbl>\n1 EC101              Camdeboo                 13.4         34.5             8.94\n2 EC102              Blue Crane Route         16.0         36.2             8.80\n3 EC103              Ikwezi                   18.4         35.4             8.59\n# … with 231 more rows, and 3 more variables: Some_secondary <dbl>,\n#   Grade_12_Std_10 <dbl>, Higher <dbl>\n\n\n\n\nLoading the map\nNext we need a ‘blank map’ of the same South African municipalities that are included in the data above. It is read-in below in geoJSON format but it would not have been unusual if it had been in .shp (shapefile) or .kml format, instead. The source of the data is https://dataportal-mdb-sa.opendata.arcgis.com/. There are several ways of reading this file into R but it is better to use the sf package because older options such as maptool::readShapePoly() (which was for reading shapefiles) or rgdal::readOGR are either deprecated already or in the process of being retired.\n\n\nCode\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nrequire(sf)\n\nmunicipal <- read_sf(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/MDB_Local_Municipal_Boundary_2011.geojson\")\n\n\n If we now look at the top of the municipal object then we find it is of class sf, which is short for simple features. It has a vector geometry (it is of type multipolygon) and has its coordinate reference system (CRS) set as WGS 84. It also contains some attribute data, although not the schooling data we are looking to map.\n\n\nCode\nprint(municipal, n = 1)\n\n\nSimple feature collection with 234 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 16.45189 ymin: -34.83417 xmax: 32.94498 ymax: -22.12503\nGeodetic CRS:  WGS 84\n# A tibble: 234 × 11\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n# … with 233 more rows, and 6 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>, geometry <MULTIPOLYGON [°]>\n\n\nHere are just the attribute data\n\n\nCode\nst_drop_geometry(municipal) %>% print(n = 5)\n\n\n# A tibble: 234 × 10\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n*    <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n2        2 EC           Eastern Cape EC101                 Camdeboo             \n3        3 EC           Eastern Cape EC102                 Blue Crane Route     \n4        4 EC           Eastern Cape EC103                 Ikwezi               \n5        5 EC           Eastern Cape EC104                 Makana               \n# … with 229 more rows, and 5 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>\n\n\nAnd here is the ‘blank’ map.\n\n\nCode\npar(mai=c(0, 0, 0, 0))  # Removes the plot margins\nmunicipal %>% st_geometry %>% plot\n\n\n\n\n\nHad it been necessary to set the coordinate reference system then the function st_set_crs() would be used. Instead, and just for fun, we will change it: here is the map transformed on to a ‘south up’ coordinate reference system, achieved by changing its EPSG code to 2050 with the function st_transform().\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nmunicipal %>%\n  st_transform(2050) %>%\n  st_geometry %>%\n  plot"
  },
  {
    "objectID": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "href": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "title": "Thematic maps in R",
    "section": "Joining the attribute data to the map",
    "text": "Joining the attribute data to the map\nIf we look again at the map and schooling data, we find that they have two variables in common:\n\n\nCode\nintersect(names(municipal), names(education))\n\n\n[1] \"LocalMunicipalityCode\" \"LocalMunicipalityName\"\n\n\nThis is encouraging but, in this example, we need to be careful using the municipal names because not all of those in the map are in the education data or vice versa.\n\n\nCode\ntable(municipal$LocalMunicipalityName %in% education$LocalMunicipalityName)\n\n\n\nFALSE  TRUE \n    7   227 \n\n\nCode\ntable(education$LocalMunicipalityName %in% municipal$LocalMunicipalityName)\n\n\n\nFALSE  TRUE \n    7   227 \n\n\nFortunately, the municipal codes are consistent even where the names are not.\n\n\nCode\ntable(municipal$LocalMunicipalityCode %in% education$LocalMunicipalityCode)\n\n\n\nTRUE \n 234 \n\n\nCode\ntable(education$LocalMunicipalityCode %in% municipal$LocalMunicipalityCode)\n\n\n\nTRUE \n 234 \n\n\nWe therefore join the data to the map using the variable LocalMunicipalityCode and check that the schooling data are now attached to the map. They are.\n\n\nCode\nmunicipal <- left_join(municipal, education, by = \"LocalMunicipalityCode\")\nnames(municipal)\n\n\n [1] \"OBJECTID\"                 \"ProvinceCode\"            \n [3] \"ProvinceName\"             \"LocalMunicipalityCode\"   \n [5] \"LocalMunicipalityName.x\"  \"DistrictMunicipalityCode\"\n [7] \"DistrictMunicipalityName\" \"Year\"                    \n [9] \"Shape__Area\"              \"Shape__Length\"           \n[11] \"geometry\"                 \"LocalMunicipalityName.y\" \n[13] \"No_schooling\"             \"Some_primary\"            \n[15] \"Complete_primary\"         \"Some_secondary\"          \n[17] \"Grade_12_Std_10\"          \"Higher\"                  \n\n\n\nNote that the variables LocalMunicipalityName.x and LocalMunicipalityName.y have been created in the process of the join. This is because there are non-joined variables with duplicated names in the data, i.e. LocalMunicipalityName from municipal and LocalMunicipalityName from education."
  },
  {
    "objectID": "thematicmaps.html#mapping-the-data",
    "href": "thematicmaps.html#mapping-the-data",
    "title": "Thematic maps in R",
    "section": "Mapping the data",
    "text": "Mapping the data\n\nUsing plot{sf}\nThe ‘one line’ way of plotting the data is to use the in-built plot() function for sf.\n\n\nCode\nplot(municipal[\"No_schooling\"])\n\n\n\n\n\nIt is important to include the variable(s) you wish to include in the plot or else it will plot them all up to the value specified by the argument max.plot, which has a default of nine:\n\n\nCode\nplot(municipal)\n\n\n\n\n\nThe map can be customised. For example,\n\n\nCode\nif(!(\"RColorBrewer\" %in% installed)) install.packages(\"RColorBrewer\", dependencies = TRUE)\nrequire(RColorBrewer)\n\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"jenks\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\nFor the above map RColorBrewer package has been used to create a diverging red-yellow-blue colour palette that has then been reversed using the function rev() so that red is assigned to the highest values, not lowest. RColorBrewer provides colour palettes based on https://colorbrewer2.org/. A ‘natural breaks’ (jenks) classification with 7 colours has been used (breaks = \"jenks\"). Compare it with the result from using breaks = \"equal\",\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"equal\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\n… or breaks = \"quantile\".\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"quantile\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"% of Population with No Schooling\")\n\n\n\n\n\nClearly the maps do not all appear alike. This reveals that the geographical patterns and therefore the geographical information that we view in the map are a function of how the map is constructed, including the number, colouring and widths (ranges) of the map classes. Ideally, these should be set to reflect the distribution of the data and what is being look for in it.\nThe following histograms show the break points in the distributions used in the various maps. The code works by creating a list of plots (specifically, a list of ggplots, see below) – one plot each for the jenks, equal and quantile styles – and then using a package called gridExtra to arrange those plots into a single grid. However, the code matters less than what it reveals, which is that Jenks or other ‘natural breaks’ classifications are reasonably good for identifying break points that reflect the distribution of the data in the absence of the user having cause to set those break points in some other way.\n\n\nCode\nif(!(\"gridExtra\" %in% installed)) install.packages(\"gridExtra\", dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"classInt\", dependencies = TRUE)\n\nrequire(gridExtra)\nrequire(classInt)\n\nstyles <- c(\"jenks\", \"equal\", \"quantile\")\ng <- lapply(styles, \\(x) {\n    ggplot(municipal, aes(x = No_schooling)) +\n    geom_histogram() +\n    xlab(\"% of Population with No Schooling\") +\n    geom_vline(xintercept = classIntervals(municipal$No_schooling, n = 7, style = x)$brks, col = \"white\") +\n    geom_rug() +\n    theme_minimal() +\n    ggtitle(paste(x,\"classification\"))\n})\nplot(arrangeGrob(grobs = g))\n\n\n\n\n\n For further information on using plot{sf} see here and look at the help menu, ?sf::plot.\n\n\nUsing ggplot2\nWhilst the plot() function for sf objects is useful for producing quick maps, I tend to prefer ggplot2 for better quality ones. We already have seen examples of ggplot2 output in earlier sessions and also in the histograms above.\nggplot2 is based on The Grammar of Graphics. I find it easiest to think of it in four stages:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\nHere are those four stages applied to a boxplot showing the distribution of the no schooling variable by South African Provinces. First, the data = municipal. Consulting with the ggplot2 cheatsheet, I find that the aesthetics, aes(...), for the boxplot require a discrete x and a continuous y, which are provided by ProvinceName and No_schooling, respectively. ProvinceName has also been used to assign a fill colour to each box. The optional changes arise from me preferring theme_minimal() to the default style, although I have then modified it to remove the legend, change the angle of the text on the x-axis, remove the x-axis label and change the y-axis one.\n\n\nCode\nggplot(data = municipal, aes(x = ProvinceName, y = No_schooling, fill = ProvinceName)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) +\n  xlab(element_blank()) +\n  ylab(\"% No schooling within municipalities\")\n\n\n\n\n\n Let’s now take that process and apply it to create a map, using the same RColorBrewer colour palette as previously and adding the map using geom_sf (for a full list of geoms available for ggplot2 see here). The arguments theme(... = element_blank(), ...) suppress the axis titles. What labs() does should be obvious.\n\n\nCode\nggplot(municipal, aes(fill = No_schooling)) +\n  geom_sf() +\n  scale_fill_distiller(\"%\", palette = \"RdYlBu\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )  \n\n\n\n\n\n Presently the map has a continuous shading scheme. This can be changed to discrete map classes and colours by converting the continuous municipal$No_schooling variable to a factor, using the cut() function, here with break points found using ClassIntervals(..., style = \"jenks\"). The argument scale_fill_brewer(..., direction = -1) reverses the RdYlBu palette so that the highest values are coloured red. Adding guides(fill = guide_legend(reverse = TRUE)) reverses the legend so that the highest values are on top in the legend, which is another preference of mine.\n\n\nCode\n# Find the break points in the distribution using a Jenks classification\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\n\n# Factor the No_schooling variable using those break points\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\n\nggplot(municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n\nAnnotating the map with ggspatial\nHaving created the basic map using ggplot2, we can add some additional map elements using ggspatial. The following code adds a backdrop to the map. Different backgrounds (alternative map tiles) can be chosen from the list at rosm::osm.types(); see here for what they look like.\n\nIn the previous code chunk (above), the data, municipal are handed-to ggplot in the top line ggplot(municipal, ...). In essence, this sets municipal as a global parameter: it is where ggplot will look for the variable called for in aes(fill = No_schooling_gp) and where it will look for other variables too. Whilst this is fine, for now, to preempt any problems occurring from when I introduce a second geom_sf in few code chunks time, in the code below I specifically name municipal as the fill data, in the line geom_sf(data = municipal, aes(fill = No_schooling_gp)), which will leave me free to associate other data with different aesthetics in due course.\n\n\nCode\nif(!(\"ggspatial\" %in% installed)) install.packages(\"ggspatial\", dependencies = TRUE)\nrequire(ggspatial)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n A north arrow and a scale bar can also be added, although including the scale bar generates a warning because the map-to-true life distance ratio is not actually constant across the map (it varies with longitude and latitude). The argument, location = \"tl\" is short for top left; location = \"br\" for bottom right. See ?annotation_north_arrow() and ?annotation_scale() for further details and options. Note also the use of the last_plot() function to more easily add content to the last ggplot.\n\n\nCode\nlast_plot() +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\n\n\n\n\n In the next example, the locations of South African cities are added to the map, with a symbol drawn in proportion to their population size. The source of the data is a shapefile from https://data.humdata.org/dataset/hotosm_zaf_populated_places. The symbol shape that is specified by pch = 3 has the same numeric coding as those in ?graphics::points() (e.g. 0 is a square, 1, is a circle, 2 is a triangle, and so forth). The function scales::label_comma() forces decimal display of numbers to avoid displaying scientific notation.\n\n\nCode\nif(!(\"scales\" %in% installed)) install.packages(\"scales\", dependencies = TRUE)\n\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\nunzip(\"cities.zip\")\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  filter(place == \"city\") %>%\n  mutate(population = as.numeric(population)) ->\n  cities\n\nlast_plot() +\n  geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma())\n\n\n\n\n\n\nSlightly confusingly, A shapefile actually consists of at least three files, one with the extension .shp (the coordinate/shape data), one .shx (an index file) and one .dbf (the attribute data). If you use a shapefile you need to make sure you download all of them and keep them together in the same folder.\n\n\nLabelling using ggsflabel\nNice labelling of the cities is provided by ggsflabel, in this case showing cities with over a million population. The function geom_sf_label_repel() is designed to stop labels from being placed over each other.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nlast_plot() +\n  geom_sf_label_repel(data = cities %>% filter(population > 1e6),\n                      aes(label = name), alpha = 0.7, size = 3)\n\n\n\n\n\n\n\nSaving the map\nHaving created the map, it can now be saved. You may wish to change your working directory first, using setwd(dir) and substituting dir with the pathname to the preferred directory, or by using Session -> Set Working Directory -> Choose Directory from the dropdown menus. Once you have done so, the last_plot() is easily saved using the function ggsave. For example, in .pdf format, to a print quality,\n\n\nCode\nggsave(\"no_schooling.pdf\", device = \"pdf\", width = 6, units = \"in\", dpi = \"print\")\n\n\nAlternatively, we can write directly to a graphics device, using one of the functions bmp(), jpeg(), png(), tiff() or pdf(). For instance,\n\n\nCode\njpeg(\"no_schooling.jpg\", res = 72)\nlast_plot()\ndev.off()\n\n\n\n\nCreating an interactive map using ggiraph\nSo far all the maps we have created have been static. This is obviously better for anything that will be printed but, for a website or similar, we may wish to include more ‘interaction’. The package ggiraph package creates interactive ggplot2 and we can use it to create an interactive map where information about the areas appears as we brush over those areas on the map with the mouse pointer. This is achieved by replacing in the code geom_sf() with the geom_sf_interactive() function from ggiraph and rendering the resulting ggplot2 object with girafe().\n\n\nCode\nif(!(\"ggiraph\" %in% installed)) install.packages(\"ggiraph\", dependencies = TRUE)\nrequire(ggiraph)\n\ng <- ggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf_interactive(aes(tooltip = paste0(LocalMunicipalityName.x, \"\\n\", round(No_schooling,1), \"%\"),\n                             fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\ngirafe(ggobj = g)\n\n\n\n\n\n\n\n\n\nUsing tmap\nClearly there is a lot of scope to produce high quality maps using ggplot2 and various associated packages. However, it has a rival, in tmap, which is arguably easier to use. Like ggplot2, tmap adopts the Grammar of Graphics but approaches it in a slightly different way that uses layers: it builds-up the layers of the graphic by first specifying a spatial object or background, then doing things to the map based on it, then specifying another spatial object and/or other map elements to do things with, and so forth. The types of layers available can be viewed here and here. A brief introduction to tmap is available here.\nThe following code builds-up the layers of the map to produce one quite like what was previously created in ggplot2.\n\n\nCode\nif(!(\"tmap\" %in% installed)) install.packages(\"tmap\", dependencies = TRUE)\nrequire(tmap)\n\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_fill(\"No_schooling\", palette = \"-RdYlBu\", title = \"%\", style = \"jenks\", n = 7) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %>% filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\n\n\n\n\n\nUnfortunately, there is a error in the geometry of the underlying municipal map file: all(st_is_valid(municipal)). The problem lies in the 128th area, where one edge crosses another: st_is_valid(municipal[128,], reason = TRUE). At the time of writing, this has not been fixed and, because tmap is less forgiving of it than ggplot2 is, it has been omitted from the map. \n\nThe map looks pretty good and can be saved using the function tmap_save. For example,\n\n\nCode\ntmap_save(tmap_last(), \"no_schooling2.jpg\", width = 7, units = \"in\")\n\n\nHowever, there is a cartographic irritation. If you look at the map classes, they are non-unique: e.g. 5.64 to 9.87, 9.87 to 13.38, 13.38 to 17.19, and so forth. Which category would a value of 9.87 (or 13.38, etc.) fall into?\nTo solve this problem, we can do what we did for the ggplots too, which is to create a factor from the municipal$No_schooling variable, which is what the first two lines of code below do. The third line reverses the order of the factors, so that the highest not lowest valued group is treated as the first level and so forth. The reason I have added this is because of my preference for the highest values to appear top in the legend.\n\n\nCode\nbrks <- rev(classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks)\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\nmunicipal$No_schooling_gp <- factor(municipal$No_schooling_gp,\n                                    levels = rev(levels(municipal$No_schooling_gp)))\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %>% filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\n\n\n\n\n Where tmap really excels is in rendering interactive maps to leaflet by changing the tmap_mode to tmap_mode(\"view\") from tmap_mode(\"plot\"). The following allows panning, can be zoomed in and out of, can have different map layers displayed (move your mouse cursor over the map layers icon to do so) and, if you right click on any of the areas shown, will bring-up information about them.\n\n\nCode\ntmap_mode(\"view\")\n\ntm_basemap(c(Stamen = \"Stamen.Watercolor\",\n             Carto = \"CartoDB\",\n             OSM = \"OpenStreetMap\")) +\n  tm_shape(municipal[-128,], is.master = TRUE, name = \"municipalities\") +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\",\n          id = \"LocalMunicipalityName.x\",\n          popup.vars = c(\"% No schooling:\" = \"No_schooling\",\n                         \"Province: \" = \"ProvinceName\"),\n          popup.format = list(digits = 1)) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\",\n          id = \"name\",\n          popup.vars = c(\"Population: \" = \"population\")) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_view(view.legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n \nDifferent layers are available dependent upon the tmap_mode. For example, there is no straightforward way of adding a maptile (tm_basemap) as the backdrop to a \"plot\" map, nor a compass (tm_compass) or a scale bar (tm_scale_bar) to a \"view\".\n We can also have some fun! Here is an animated map where the animation is produced from a combination of the tm_facets(along = \"ProvinceName\", ...) layer and the use of tmap_animation() function. The animation may be saved as .gif file by including the argument filename (see ?tmap_animation).\n\n\nCode\nif(!(\"gifski\" %in% installed)) install.packages(\"gifski\", dependencies = TRUE)\n\ntmap_mode(\"plot\")\n\nt <- tm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_polygons(col = \"grey\", border.col = \"black\") +\n  tm_shape(municipal[-128,]) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"white\") +\n  tm_facets(along = \"ProvinceName\", free.coords = FALSE) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\ntmap_animation(t, delay = 100)\n\n\n\n Faceting can also be used on static maps, as in the following example, where tm_facets(by = \"ProvinceName\", free.coords = TRUE) creates a choropleth map for each Province, with a common legend, positioned outside each provincial map with tm_layout(legend.outside.position = \"bottom\").\n\n\nCode\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,]) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"% Population with No Schooling\",\n          legend.is.portrait = FALSE) +\n  tm_borders(col = \"white\") +\n  tm_facets(by = \"ProvinceName\", free.coords = TRUE) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.outside.position = \"bottom\")"
  },
  {
    "objectID": "thematicmaps.html#saving-the-map-and-attribute-data",
    "href": "thematicmaps.html#saving-the-map-and-attribute-data",
    "title": "Thematic maps in R",
    "section": "Saving the map and attribute data",
    "text": "Saving the map and attribute data\nBefore finishing, we will save the map and joined attribute data to the working directory as an R object.\n\n\nCode\nsave(municipal, file = \"municipal.RData\")"
  },
  {
    "objectID": "thematicmaps.html#summary",
    "href": "thematicmaps.html#summary",
    "title": "Thematic maps in R",
    "section": "Summary",
    "text": "Summary\nThis session has demonstrated that R is a powerful tool for drawing publication quality maps. The native plot functions for sf objects are useful as a quick way to draw a map and both ggplot2 and tmap offer a range of functionality to customise their cartographic outputs to produce really nice looking maps. I tend to use ggplot2 but that is really more out of habit than anything else as tmap might actually be the easier to use. It depends a bit on whether I am drawing static maps (usually in ggplot2) or interactive ones (probably better in tmap). There are other packages available, too, including mapview, which the following code chunk uses (see also, here), and Leaflet to R. Perhaps the key take-home point is that these maps can look at lot better than those produced by some conventional GIS and can be linked to other analytical processes in R.\n\n\nCode\nif(!(\"mapview\" %in% installed)) install.packages(\"mapview\", dependencies = TRUE)\nrequire(mapview)\n\nmapview(municipal %>% mutate(No_schooling = round(No_schooling, 1)), \n        zcol = \"No_schooling\",\n        layer.name = \"% No Schooling\",\n        map.types = \"Stamen.Watercolor\",\n        col.regions = colorRampPalette(rev(brewer.pal(9, \"RdYlBu\"))))"
  },
  {
    "objectID": "thematicmaps.html#further-reading",
    "href": "thematicmaps.html#further-reading",
    "title": "Thematic maps in R",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 2 on Spatial data and R packages for mapping from Geospatial Health Data by Paula Morga (2019)\n\nChapter 9 on Making maps with R from Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow."
  },
  {
    "objectID": "tidyverse.html#what-is-the-tidyverse",
    "href": "tidyverse.html#what-is-the-tidyverse",
    "title": "Tidyverse",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nIf base R is R Classic then tidyverse is a new flavour of R, designed for data science. It consists of a collection of R packages that “share an underlying design philosophy, grammar, and data structures”.\nTidyverse is easier to demonstrate then to pin-down to some basics so let’s work through an example using both base R and tidyverse to illustrate some differences."
  },
  {
    "objectID": "tidyverse.html#to-start",
    "href": "tidyverse.html#to-start",
    "title": "Tidyverse",
    "section": "To Start",
    "text": "To Start\nWe will begin by downloading a data file to use. It is an extract of the Covid Data Dashboard for England in December 2021. Some prior manipulation and adjustments to these data have been undertaken for another project so treat them as indicative only (the actual numbers may have been changed slightly from their originals although only marginally so).\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_extract.csv\", \"covid.csv\", mode = \"wb\", quiet = TRUE) \n\n\nWe also need to require(tidyverse) ready for use.\n\n\nCode\nrequire(tidyverse)\n\n\n\nIf you get a warning message saying there is no package called tidyverse then you need to install it: install.packages(\"tidyverse\", dependencies = TRUE)."
  },
  {
    "objectID": "tidyverse.html#reading-in-the-data",
    "href": "tidyverse.html#reading-in-the-data",
    "title": "Tidyverse",
    "section": "Reading-in the data",
    "text": "Reading-in the data\nLet’s read-in and take a look at the data. First in base R.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\nhead(df1)\n\n\n   MSOA11CD regionName X2021.12.04 X2021.12.11 X2021.12.18 X2021.12.25 All.Ages\n1 E02000002     London          25          48         148         176     7726\n2 E02000003     London          46          58         165         215    11246\n3 E02000004     London          24          44         100         141     6646\n4 E02000005     London          58          97         185         231    10540\n5 E02000007     London          38          94         153         205    10076\n6 E02000008     London          54         101         232         245    12777\n\n\nNow using tidyverse,\n\n\nCode\ndf2 <- read_csv(\"covid.csv\")\nprint(df2, n = 6)\n\n\n# A tibble: 6,789 × 7\n  MSOA11CD  regionName `2021-12-04` `2021-12-11` `2021-12-18` `2021-12-25`\n  <chr>     <chr>             <dbl>        <dbl>        <dbl>        <dbl>\n1 E02000002 London               25           48          148          176\n2 E02000003 London               46           58          165          215\n3 E02000004 London               24           44          100          141\n4 E02000005 London               58           97          185          231\n5 E02000007 London               38           94          153          205\n6 E02000008 London               54          101          232          245\n# … with 6,783 more rows, and 1 more variable: `All Ages` <dbl>\n\n\nThere are already some differences. First, tidyverse has, in this case, handled the names of the variables better. It has also created what is described as a tibble which is “a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” You can find out more about them and how they differ from traditional data frames here."
  },
  {
    "objectID": "tidyverse.html#selecting-and-renaming-variables",
    "href": "tidyverse.html#selecting-and-renaming-variables",
    "title": "Tidyverse",
    "section": "Selecting and renaming variables",
    "text": "Selecting and renaming variables\nWe will now select the regionName, 2021-12-04 and All Ages variables, rename the second of these as cases and the third as population, and look at the data again to check that it has worked.\nIn base R,\n\n\nCode\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[2:3] <- c(\"cases\", \"population\")\nhead(df1)\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\nIn tidyverse,\n\n\nCode\ndf2 <- select(df2, regionName, `2021-12-04`, `All Ages`)\ndf2 <- rename(df2, cases = `2021-12-04`, population = `All Ages`)\nprint(df2, n = 6)\n\n\n# A tibble: 6,789 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n# … with 6,783 more rows\n\n\nComparing the two, the tidyverse code may be more intuitive to understand because of its use of verbs as functions: select(), rename() and so forth."
  },
  {
    "objectID": "tidyverse.html#piping",
    "href": "tidyverse.html#piping",
    "title": "Tidyverse",
    "section": "Piping",
    "text": "Piping\nNow we shall bring the two previous stages together, using what is referred to as a pipe. Without worrying about the detail, which will be returned to presently, here is an example of a pipe, |>, being used in base R:\n\n\nCode\nread.csv(\"covid.csv\") |>\n  (\\(x) x[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")])() -> df1\nnames(df1)[2:3] <- c(\"cases\", \"population\")\ndf1 |>\n  head()\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\n\nThe above will only work if you are using R version 4.1.0 or above. You can check which version you are running by using R.Version()$version.\nNow using tidyverse and a different pipe, %>%,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  print(n = 6)\n\n\n# A tibble: 6,789 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n# … with 6,783 more rows\n\n\nThe obvious difference here is that the tidyverse code is more elegant. But what is the pipe and what is the difference between |> in the base R code and %>% in the tidyverse example?\nA pipe is really just a way of sending (’piping`) something from one line of code to the next, to create a chain of commands (forgive the mixed metaphors). For example,\n\n\nCode\nx <- 0:10\nmean(x)\n\n\n[1] 5\n\n\nCould be calculated as\n\n\nCode\n0:10 |>\n  mean()\n\n\n[1] 5\n\n\nor as\n\n\nCode\n0:10 %>%\n  mean\n\n\n[1] 5\n\n\nA more complicated example is below. It employs the function sapply(), a variant of the function lapply(X, FUN) that takes a list X and applies the function FUN to each part of it. In the example, it is the function mean.\n\n\nCode\nx <- list(0:10, 10:20)  # Creates a list with two parts: the numbers 0 to 10, and 10 to 20\ny <- sapply(x, mean)    # Calculates the mean for each part of the list, which are 5 and 15\nsum(y)                  # Sums together the two means, giving 20\n\n\n[1] 20\n\n\nThe above could instead be written as\n\n\nCode\nlist(0:10, 10:20) |>\n  sapply(mean) |>\n  sum()\n\n\n[1] 20\n\n\nor as\n\n\nCode\nlist(0:10, 10:20) %>%\n  sapply(mean) %>%\n  sum\n\n\n[1] 20\n\n\nAll three arrive at the same answer, which is 20.\nSo far, so good but what is the difference between |> and %>%? The answer is that %>% was developed before |> in the magrittr package, whereas |> is R’s new native pipe. They are often interchangeable but not always.\nAt the moment, the |> pipe is less flexible to use than %>%. Consider the following example. The final two lines of code work fine using %>% to pipe the data frame into the regression model (the function lm() fits a linear model).\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)   # Adds some random noise to the relationship between y and x\ndata.frame(x, y) %>%\n  lm(y ~ x, data = .)\n\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n   -0.07291      2.00212  \n\n\nHowever, it does not work with the pipe, |> because it does not recognise the place holder ., which receives the data frame from the line above and contains the variables for the model.\n\n\nCode\n# The following code does not work\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  lm(y ~ x, data = .)\n\n\nTo solve the problem, the above code can be modified by wrapping the regression part in another function but the end result is rather ‘clunky’.\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  (\\(z) lm(y ~ x, data = z))() \n\n\n\nCall:\nlm(formula = y ~ x, data = z)\n\nCoefficients:\n(Intercept)            x  \n    0.00849      2.00106  \n\n\nOver time, expect |> to be developed and to supersede %>%. For now, however, you are unlikely to encounter errors using %>% as a substitute for |> but you might using |> instead of %>%. In other words, %>% is the safer choice and the one which will be used for these tutorials."
  },
  {
    "objectID": "tidyverse.html#back-to-the-example",
    "href": "tidyverse.html#back-to-the-example",
    "title": "Tidyverse",
    "section": "Back to the example",
    "text": "Back to the example\nAfter that digression into piping, let’s return to our example that is comparing base R and tidyverse to read-in a table of data, select variables and rename one, and, in the following, to calculate the number of COVID-19 cases per English region as a percentage of their estimated populations in the week ending 2021-12-04.\nFirst, in base R:\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)  # Total cases per region\ncases    # This step isn't necessary but is included to show the result of the line above\n\n\n           East Midlands          East of England                   London \n                   25472                    35785                    43060 \n              North East               North West               South East \n                   10796                    31185                    62807 \n              South West            West Midlands Yorkshire and The Humber \n                   33846                    26554                    21079 \n\n\nCode\npopulation <- tapply(df1$population, df1$regionName, sum)   # Total population per region\nrate <- round(cases / population * 100, 3)\nrate\n\n\n           East Midlands          East of England                   London \n                   0.524                    0.571                    0.479 \n              North East               North West               South East \n                   0.403                    0.423                    0.681 \n              South West            West Midlands Yorkshire and The Humber \n                   0.598                    0.445                    0.381 \n\n\nNow using tidyverse,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  group_by(regionName) %>%\n  summarise(across(where(is.numeric), sum)) %>%\n  mutate(rate = round(cases / population * 100, 3)) %>%\n  print(n = Inf)\n\n\n# A tibble: 9 × 4\n  regionName               cases population  rate\n  <chr>                    <dbl>      <dbl> <dbl>\n1 East Midlands            25472    4865583 0.524\n2 East of England          35785    6269161 0.571\n3 London                   43060    8991550 0.479\n4 North East               10796    2680763 0.403\n5 North West               31185    7367456 0.423\n6 South East               62807    9217265 0.681\n7 South West               33846    5656917 0.598\n8 West Midlands            26554    5961929 0.445\n9 Yorkshire and The Humber 21079    5526350 0.381\n\n\nEither way produces the same answers but, again, there is an elegance and consistency to the tidyverse way of doing it that is missing from base R."
  },
  {
    "objectID": "tidyverse.html#plotting",
    "href": "tidyverse.html#plotting",
    "title": "Tidyverse",
    "section": "Plotting",
    "text": "Plotting\nAs a final step for the comparison, we will extend the code to visualise the regional COVID-19 rates in a histogram, with a rug plot included. A rug plot is a way of preserving the individual data values that would otherwise be ‘lost’ within the bins of a histogram.\nAs previously, we begin with base R,\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)\npopulation <- tapply(df1$population, df1$regionName, sum)\nrate <- round(cases / population * 100, 3)\nhist(rate, xlab = \"rate (cases as % of population)\",\n     main = \"Regional COVID-19 rates: week ending 2021-12-04\")\nrug(rate, lwd = 2)\n\n\n\n\n\n…and continue with tidyverse, creating the output in such a way that it mimics the previous plot.\n\n\nCode\nrequire(ggplot2)\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  group_by(regionName) %>%\n  summarise(across(where(is.numeric), sum)) %>%\n  mutate(rate = round(cases / population * 100, 3)) ->\n  df2\n\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05, center = -0.025) +\n    geom_rug(size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nIn this instance, it is the tidyverse code that is the more elaborate. This is partly because there is more customisation of it to mimic the previous plot, which has added the final two lines of code. However, it is also because it is using the package ggplot2 to produce the histogram. We return to ggplot2 more in later sessions. For now it is sufficient to scan the code and observe how it is ‘layering up’ the various components of the graphic, which those components separated by the + in the lines of code.\nI prefer the ggplot2 to the hist() graphics plot but that may be a matter of personal taste. However, ggplot2 can do ‘clever things’ with the visualisation, a hint of which is shown below.\n\n\nCode\nrequire(ggplot2)\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05, center = -0.025) +\n    geom_rug(aes(colour = regionName), size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    scale_colour_discrete(name = \"Region\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank()) \n\n\n\n\n\n Please don’t form that impression that ggplot2 is hard-wired to tidverse, and base R to the base graphics. In practice, they are interchangeable.\nHere is an example of using ggplot2 after a sequence of base R commands.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ndf1$rate <- round(df1$cases / df1$population * 100, 3)\nggplot(df1, aes(x = rate, y = regionName)) +\n  geom_boxplot() +\n  labs(x = \"rate (cases as % of population)\",\n       y = \"region\",\n       title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n  theme_minimal()\n\n\n\n\n\nAnd here is an example of using the base R graphic boxplot() after a chain of tidyverse commands.\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  mutate(rate = round(cases / population * 100, 3)) -> df2\npar(mai=c(0.8,2,0.5,0.5), bty = \"n\", pch = 20)  # See text below\nboxplot(df2$rate ~ df2$regionName, horizontal = TRUE,\n        whisklty = \"solid\", staplelty = 0,\n        col = \"white\", las = 1, cex = 0.9, cex.axis = 0.75,\n        xlab = \"rate (cases as % of population)\", ylab=\"\",\n        main = \"Regional COVID-19 rates: week ending 2021-12-04\")\ntitle(ylab = \"region\", line = 6)\n\n\n\n\n\nI would argue that, in this instance, the base R graphic is as nice as the ggplot2 one but it took more customisation to get it that way and I had to go digging around in the help files, ?boxplot, ?bxp and ?par to find what I needed, which included changing the graphic’s margins (par(mai=...))), moving and changing the size of the text on the vertical axis (the argument cex.axis and the use of the title() function), changing the appearance of the ‘whiskers’ (whisklty = \"solid\" and staplelty = 0), and so forth. Still, it does demonstrate that you can have a lot of control over what is produced, if you have the patience and tenacity to do so."
  },
  {
    "objectID": "tidyverse.html#which-is-better",
    "href": "tidyverse.html#which-is-better",
    "title": "Tidyverse",
    "section": "Which is better?",
    "text": "Which is better?\nHaving provided a very small taste of tidyverse and how it differs from base R, we might be tempted to ask, “which is better?” However, the question is misguided: it is a little like deciding to go to South America and asking whether Spanish or Portuguese is the better language. It depends, of course, on what you intend to do and where you intend to travel.\nI use both base R and tidyverse packages in my work, sometimes drifting between the two in rather haphazard ways. If I can get what I want to work then I am happy. Outcomes worry me more than means so, although I use tidyverse a lot, I am not always as tidy as it would want me to be!"
  },
  {
    "objectID": "tidyverse.html#futher-reading",
    "href": "tidyverse.html#futher-reading",
    "title": "Tidyverse",
    "section": "Futher reading",
    "text": "Futher reading\n\nThere is much more to tidyverse than has been covered here. See here for further information about it and its core packages.\nA full introduction to using tidyverse for Data Science is provided by the book R for Data Science by Hadley Wickham and Garrett Grolemund. There is a free online version of it available."
  },
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "A Cartographic Answer",
    "section": "",
    "text": "Let’s answer the ‘why?’ question with a quick example of R in use. We will not worry about the exact detail of what the code means at this stage but will largely take it as we find it, copying and pasting from this webpage into the R Console.\n\nIf you find that the + sign stays on your screen for a while and isn’t followed by > then you have either forgotten to hit Enter/Return or have not included all of the code. You can always press esc on your keyboard and try again.\n\n\nFirst, we will check that the necessary packages are installed and then ‘require’ them, which means to load them so they are available to use.\n\n\nCode\ninstalled <- installed.packages()[,1]\n\nif(!(\"XML\" %in% installed)) install.packages(\"XML\", dependencies = TRUE)\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\nif(!(\"readxl\" %in% installed)) install.packages(\"readxl\", dependencies = TRUE)\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nif(!(\"ggplot2\" %in% installed)) install.packages(\"ggplot2\", dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"ggspatial\", dependencies = TRUE)\nif(!(\"ggspatial\" %in% installed)) install.packages(\"ggspatial\", dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(readxl)\nrequire(sf)\nrequire(ggplot2)\nrequire(classInt)\nrequire(ggspatial)\n\n\n\n\n\nNext, we will download a data table published by Statistics South Africa which estimates the number of people speaking various languages in the South African Provinces in 2011. These data were downloaded from https://superweb.statssa.gov.za/webapi/. The data are found in an Excel spreadsheet, which is read in and manipulated, converting the counts into percentages.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/table_2022-06-22_17-36-26.xlsx?raw=true\", \"language.xlsx\", quiet = TRUE, mode = \"wb\")\n\nread_xlsx(\"language.xlsx\", sheet = \"Data Sheet 0\", skip = 8) %>%\n  rename(Name = 2) %>%\n  drop_na(Afrikaans) %>%\n  select(-1) %>%\n  mutate(across(where(is.numeric), ~ round(. / Total * 100, 2))) -> languages\n\n\nHere is the top of that data:\n\n\nCode\nhead(languages)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# … with 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\n\n\n\nWhat R allows is the opportunity to map the data without needing to go outside R to use separate software such as GIS. To do so, we will need a ‘blank map’ of the Provinces that can be joined with the data.\nFirst, we will download a pre-existing map, also from https://superweb.statssa.gov.za/webapi/.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/mapview.kmz?raw=true\", \"map.kmz\", quiet = TRUE, mode = \"wb\")\nunzip(\"map.kmz\")\nst_read(\"doc.kml\") %>%\n  select(-Description) -> map\n\n\nHere is the outline of that map:\n\n\nCode\nggplot(data = map) +\n  geom_sf()\n\n\n\n\n\n\n\n\nNow we can link the data table to the map\n\n\nCode\nmap %>%\n  left_join(languages, by = \"Name\") -> map\n\n\nand then plot one of the variables.\n\n\nCode\nbreaks <- classIntervals(map$IsiXhosa, n = 4, style = \"equal\")$brks\nggplot(data = map) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(aes(fill = IsiXhosa), alpha = 0.8) +\n  scale_fill_gradient(breaks = breaks, label = round(breaks, 2),\n                      low = \"white\", high = \"dark blue\") +\n  ggtitle(\"% Population speaking Xhosa\")\n\n\n\n\n\n The really nice thing about this is that it is now very easy to change the appearance of the map with only minor updates to the code.\n\n\nCode\nbreaks <- classIntervals(map$English, n = 4, style = \"equal\")$brks\nggplot(data = map) +\n  annotation_map_tile(type = \"stamenwatercolor\", progress = \"none\") +\n  geom_sf(aes(fill = English), alpha = 0.8) +\n  scale_fill_gradient(breaks = breaks, label = round(breaks, 2),\n                      low = \"white\", high = \"dark red\") +\n  ggtitle(\"% Population speaking English\")\n\n\n\n\n\n\n\nCode\nbreaks <- classIntervals(map$Afrikaans, n = 4, style = \"equal\")$brks\nggplot(data = map) +\n  annotation_map_tile(type = \"thunderforestlandscape\", progress = \"none\") +\n  geom_sf(aes(fill = Afrikaans), alpha = 0.8, col = \"transparent\") +\n  scale_fill_gradient(breaks = breaks, label = round(breaks, 2),\n                      low = \"white\", high = \"dark red\") +\n  annotation_north_arrow(which_north = \"grid\", location = \"topright\") +\n  ggtitle(\"% Population speaking Afrikaans\")\n\n\n\n\n\n\n\n\nFinally, once we are happy with it, we can export the image in a format suitable for a journal publication, for instance or to insert into other documents such as Microsoft Word.\nAs jpeg, to print quality:\n\n\nCode\nggsave(\"mymap.jpg\", device = \"jpeg\", width = 7, height = 6, units = \"in\", dpi = \"print\")\n\n\nAs pdf:\n\n\nCode\nggsave(\"mymap.pdf\", device = \"pdf\", width = 7, height = 6, units = \"in\")\n\n\nAs bmp, to screen quality:\n\n\nCode\nggsave(\"mymap.bmp\", device = \"bmp\", width = 7, height = 6, units = \"in\", dpi = \"screen\")\n\n\nIf we now look in your working directory, they should be there:\n\n\nCode\nlist.files(pattern = \"mymap\")\n\n\n[1] \"mymap.bmp\" \"mymap.jpg\" \"mymap.pdf\""
  },
  {
    "objectID": "why.html#convinced",
    "href": "why.html#convinced",
    "title": "A Cartographic Answer",
    "section": "Convinced?",
    "text": "Convinced?\nOf course, maps can also be produced in open source software such as QGIS and GIS software certainly have their use. R is not automatically better or necessarily a replacement for these. However, what it does offer is an integrated environment for what we might call geographic data science: we can download data from external websites, load and tidy-up those data, fit statistical or other models to them and map the results – all from within R. Our stages of working can be saved as scripts, which are faster to change and modify than using ‘point-and-click’ operations, and we can share our code with other people (even those using different operating systems) facilitating collaborative working and reproducible social-/ science. Finally, there are lots of packages available for reading, visualising, and analysing spatial data. Some of them are summarised here. These are attractive reasons for mapping and modelling within R."
  },
  {
    "objectID": "why.html#alternatives",
    "href": "why.html#alternatives",
    "title": "A Cartographic Answer",
    "section": "Alternatives",
    "text": "Alternatives\n\nAside from software such as QGIS, an interesting area of development is Geographic Data Science with Python. You can learn more about it here."
  },
  {
    "objectID": "why.html#need-more-convincing",
    "href": "why.html#need-more-convincing",
    "title": "A Cartographic Answer",
    "section": "Need more convincing?",
    "text": "Need more convincing?\nIf you have time, have a look at this exercise that we sometimes use with prospective students at University open days. The idea of the exercise is not to teach the students R but to show them how we use R for geographic data science in the School of Geographical Sciences. What the exercise does is take COVID-19 data for English neighbourhoods, fit statistical models to it and map the results – all in R. Again, it is the ability to use R for all the stages shown below that makes it so useful.\n\nSource: R for Data Science"
  }
]