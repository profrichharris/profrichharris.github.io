[
  {
    "objectID": "temp.html",
    "href": "temp.html",
    "title": "temp",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "temp.html#quarto",
    "href": "temp.html#quarto",
    "title": "temp",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "temp.html#running-code",
    "href": "temp.html#running-code",
    "title": "temp",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Getting Started",
    "section": "",
    "text": "For this course we will be using R. R is a free software environment for statistical computing and graphics. To run the code blocks for this course on your own computer you will need to have installed R. This is available for Linux, MacOS and Windows.\n\nIMPORTANT: Mac users, make sure you have installed the right version of R for your laptop’s processor (M1/M2 or Intel)\nAt the time of publishing, I am using R version 4.3.1 (2023-06-16 ucrt)."
  },
  {
    "objectID": "start.html#quarto",
    "href": "start.html#quarto",
    "title": "temp",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "start.html#running-code",
    "href": "start.html#running-code",
    "title": "temp",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "start.html#install-r",
    "href": "start.html#install-r",
    "title": "Getting Started",
    "section": "",
    "text": "For this course we will be using R. R is a free software environment for statistical computing and graphics. To run the code blocks for this course on your own computer you will need to have installed R. This is available for Linux, MacOS and Windows.\n\nIMPORTANT: Mac users, make sure you have installed the right version of R for your laptop’s processor (M1/M2 or Intel)\nAt the time of publishing, I am using R version 4.3.1 (2023-06-16 ucrt)."
  },
  {
    "objectID": "start.html#install-r-studio",
    "href": "start.html#install-r-studio",
    "title": "Getting Started",
    "section": "Install R Studio",
    "text": "Install R Studio\nRStudio is an integrated development environment (IDE) that can make programming and other tasks easier in R. An open source edition is available to download and install.\n\nYou need to install R before you install R Studio."
  },
  {
    "objectID": "start.html#open-r-studio",
    "href": "start.html#open-r-studio",
    "title": "Getting Started",
    "section": "Open R Studio",
    "text": "Open R Studio\nOnce R and R Studio are installed, open R Studio on your computer and type the following in the Console to the left or bottom left of the screen, alongside the prompt, &gt;.\n\n\nCode\n1 + 1\n\n\nthen hit Enter/Return. You should, of course, obtain the answer 2, as below.\n\n\n[1] 2\n\n\nYou will also find that if you move your mouse to over the code block above, an option appears to copy the code to the clipboard, allowing you to then paste it into the R Console window in R Studio."
  },
  {
    "objectID": "start.html#install-additional-librariespackages",
    "href": "start.html#install-additional-librariespackages",
    "title": "Getting Started",
    "section": "Install additional libraries/packages",
    "text": "Install additional libraries/packages\nThe base functions of R are greatly extended by the very many packages/libraries that have been developed for it. At the time of writing, there are 19925 of these on CRAN, which is the main repository for them. Many of these have been grouped into ‘tasks’ and topic areas, which can be viewed here.\nMost of the packages that will be needed for this course will be installed as they are needed. However, some will be used so regularly that we should install them now. Cut and paste the following code chunk into the Console and hit Enter/Return.\n\n\nCode\ninstall.packages(\"proxy\", dependencies = TRUE)\ninstall.packages(\"sf\", dependencies = TRUE)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n\n\n\nRun the code above even if you have the packages installed already so that you also have available all the packages that these depend upon and link to."
  },
  {
    "objectID": "start.html#changing-the-working-directory",
    "href": "start.html#changing-the-working-directory",
    "title": "Getting Started",
    "section": "Changing the working directory",
    "text": "Changing the working directory\nIf you type getwd() into the R Console you will obtain your current working directory – the default location to look for files and to save content to. Mine is,\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/profr/Dropbox/github/MandM\"\n\n\nYou may wish to change this to something else each time you start R. You can do this using the drop-down menus. There is also the function setwd(dir) – type ?setwd in the R Console to learn more."
  },
  {
    "objectID": "start.html#organising-your-files-in-a-project",
    "href": "start.html#organising-your-files-in-a-project",
    "title": "Getting Started",
    "section": "Organising your files in a project",
    "text": "Organising your files in a project\nRather than having to set your working directory each time you restart R, you could also create a new project in R by using File –&gt; New Project… from the dropdown menus and create it either in a new directory (probably most sensible) or an existing one.\nThere is nothing especially magical about a project in R. As stated here, “a project is simply a working directory designated with a .RProj file. When you open a project (using File/Open Project in RStudio or by double–clicking on the .Rproj file outside of R), the working directory will automatically be set to the directory that the .RProj file is located in.” However, it is that which makes it useful: when you open a project you know that you are going to be working in a specific folder on your computer which them becomes the default ‘container’ to save files to or to download them from.”\n\nIt would be a good idea to create a new project now which can then be the folder and working directory for this course and its contents."
  },
  {
    "objectID": "start.html#changing-the-appearance-of-r-studio",
    "href": "start.html#changing-the-appearance-of-r-studio",
    "title": "Getting Started",
    "section": "Changing the appearance of R Studio",
    "text": "Changing the appearance of R Studio\nYou may notice that I prefer a blue to a white screen when working in R. To change it to this, from the drop-down menus use Tools –&gt; Global Options… -&gt; Appearance, and select Solarized Dark as the Editor theme. You may, of course, have your own preference."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "Welcome",
    "section": "About the course",
    "text": "About the course\nThe contents of this course were first developed for a short course at the University of Cape Town (UCT) in August 2022. It also forms part of the MSc Geographic Data Science and Spatial Analytics in the School of Geographical Sciences, University of Bristol.\nThe aims of this course are to teach an introduction to mapping, spatial analysis in R. It is a course in geographic data science with a particular focus on mapping, measuring and modelling spatial patterns in data. The core parts of the course are:\n\nWhy use R for mapping and spatial modelling?\nThe basics of mapping in R\nThe Spatial Variable: from maps towards models\nSpatial clustering and spatial heterogeneity: measuring patterns in data\nHarnessing spatial autocorrelation with geographically weighted statistics\nSpatial regression models\n\n\nThis is a work in progress\nChanges will be made and additional content added over time so check back here for the latest updates."
  },
  {
    "objectID": "index.html#course-text",
    "href": "index.html#course-text",
    "title": "Welcome",
    "section": "Course text",
    "text": "Course text\nThe most relevant texts for this course are:\nFirst, parts II (Chapters 7 to 9) and III (Chapters 10 to 17) of Spatial Data Science with Applications in R. An online version of the book is available here. It takes a deeper dive into the fundamentals of spatial data science than this course does and is more ‘technical’ but is a good resource to extend your knowledge of geographical/spatial data science.\n\nSecond, Chapters 2, 3, 5, 7 and 8 of of Spatial Data Science with Applications in R. An online version of the book is available here."
  },
  {
    "objectID": "index.html#pre-reading",
    "href": "index.html#pre-reading",
    "title": "Welcome",
    "section": "Pre-reading",
    "text": "Pre-reading\nThe following short pre-reading is recommended for the course:\nHarris RJ (2019). Not just nuisance: spatialising social statistics. In A Whitworth (ed.) Towards a Spatial Social Policy: Bridging the Gap Between Geography and Social Policy. Chapter 8. Bristol: Policy Press. Available here (or, if that doesn’t work try here)."
  },
  {
    "objectID": "index.html#other-useful-resources",
    "href": "index.html#other-useful-resources",
    "title": "Welcome",
    "section": "Other useful resources",
    "text": "Other useful resources\nSpatial Regression Models for the Social Sciences covers similar statistical ground to this course, For University of Bristol students, it is available to view as an eBook here.\n\nIn addition, Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow offers an extremely useful reference to have to hand if you are stuck when undertaking geocomputation with R. There is a free online version available."
  },
  {
    "objectID": "index.html#provisional-masters-programme",
    "href": "index.html#provisional-masters-programme",
    "title": "Welcome",
    "section": "Provisional Masters programme",
    "text": "Provisional Masters programme\nFor the 2023-4 iteration of the Masters unit, the teaching schedule is:\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nLecture\nPractical\nContent\n\n\n\n\n1\n-\n-\n-\n-\n\n\n2\nMon Oct 2\n11am - noon\n2 - 4pm\nWhy R and set-up practical\n\n\n3\nMon Oct 9\n-\n1 - 3pm\nFlavours of R\n\n\n4\nMon Oct 16\n11am - noon\n1 - 3pm\nMapping the spatial variable 1\n\n\n5\nMon Oct 23\n-\n1 - 3pm\nMapping the spatial variable 2\n\n\n6\n-\n-\n-\n-\n\n\n7\n-\n-\n-\n-\n\n\n8\nMon Nov 13\n11am - noon\n1 - 3pm\nMeasuring spatial autocorrelation\n\n\n9\nMon Nov 20\n11am - noon\n1 - 3pm\nGeographically Weighted Statistics\n\n\n10\n-\n-\n-\n-\n\n\n11\nMon Dec 4\n11am - noon\n1 - 3pm\nSpatial regression + opportunity to work on assessment\n\n\n12\nMon Dec 11\n11am - noon\n1 - 3pm\nGeographically weighted regression + opportunity to work on assessment"
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Welcome",
    "section": "About the author",
    "text": "About the author\nThis course is authored by Richard Harris, Professor of Quantitative Social Geography at the University of Bristol. You can find out more about me, my research and other interests at https://profrichharris.github.io/. It is taught at the University with the assistance of Dr. Richard Timmerman.\n @profrichharris"
  },
  {
    "objectID": "index.html#copyright-notice",
    "href": "index.html#copyright-notice",
    "title": "Welcome",
    "section": "Copyright notice",
    "text": "Copyright notice\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n \n@GeogBristol #justsaying!"
  },
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "A Cartographic Answer",
    "section": "",
    "text": "Let’s answer the ‘why?’ question with a quick example of R in use. We will not worry about the exact detail of what the code means at this stage or attempt to explain it in full. Instead, we will largely take it as we find it, copying and pasting from this webpage into the R Console. The focus is on some of what R can do from a geographic perspective and not, at this stage, on how it does it.\n\nIf you find that the + sign stays on your screen, in the R Console, for a while and isn’t followed by > then you have either forgotten to hit Enter/Return or have not included all of the code that is needed to complete an operation (to complete a function, for example). You can always press esc on your keyboard and try again.\nIt was suggested in ‘Getting Started’ that you might want to create a new project for this course (a folder in which to save all the files). If you followed that advice, begin by using File –> Open Project… from the dropdown menus. Or, if you didn’t, you could create a new project now (File –> New Project…). Remember, all it really does is create a new folder in which to store all your files but that is useful because it will ensure your working directory is the same as that folder.\n\nMac users: to speed-up the rendering of the maps, it is recommended that you change the Backend Graphics Device to AGG. You can do this through the dropdown menus: Tools -> Global Options…, then click on Graphics (next to Basic). This will matter more in later exercises but you might as well do it now.\n\n\nFirst, we will check that the necessary packages are installed and then require them, which means to load them so they are available to use. The usual way to install a package is with the function, install.packages() so, for example, the graphics package ggplot2 is installed using install.packages(\"ggplot2\"). The code below is a bit more elaborate as it checks which packages have not yet been installed and installs them. However, the two-step process is the same: install and then require,\n\nuse install.packages() to install packages – only needs to be done once on your computer, unless you re-install R / replace it with a more recent version;\nthen use require() to load the desired packages – needs to be done each time R is restarted.\n\n\n\nCode\n# Checks to see which packages are already installed:\ninstalled <- installed.packages()[,1]\n# Creates a character vector of packages that are required:\npkgs <- c(\"XML\", \"tidyverse\", \"readxl\", \"proxy\", \"sf\", \"ggplot2\",\n              \"classInt\", \"ggspatial\")\n# Checks which of the required packages have not yet been installed:\ninstall <- pkgs[!(pkgs %in% installed)]\n# Installs any that have not yet been installed:\nif(length(install)) install.packages(install, dependencies = TRUE)\n# Silently loads (requires) all the packages that are needed:\ninvisible(lapply(pkgs, require, character.only = TRUE))\n\n\n\nThe use of # indicates a comment in the code. It is there just for explanation. It is not executable code (it is ignored not run).\n\n\n\nNext, we will download a data table published by Statistics South Africa that provides estimates of the number of people speaking various languages in the South African Provinces in 2011. These data were downloaded from https://superweb.statssa.gov.za/webapi/. The data are found in an Excel spreadsheet, which is read in and manipulated, converting the counts into percentages.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/table_2022-06-22_17-36-26.xlsx?raw=true\", \"language.xlsx\", quiet = TRUE, mode = \"wb\")\n\nread_xlsx(\"language.xlsx\", sheet = \"Data Sheet 0\", skip = 8) |>\n  rename(Name = 2) |>\n  drop_na(Afrikaans) |>\n  select(-1) |>\n  mutate(across(where(is.numeric), ~ round(. / Total * 100, 2))) -> languages\n\n\nHere is the top of the data, viewed in the R environment:\n\n\nCode\nhead(languages)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# ℹ 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\nThere is often more than one way of achieving something in R. Here we could also use,\n\n\nCode\nslice_head(languages, n = 6)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# ℹ 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\n\n\n\nWhat R allows is the opportunity to map the data without needing to go outside R to use separate software such as GIS. To do so, we will need a ‘blank map’ of the Provinces that can be joined with the data to create a choropleth map (a type of thematic map).\nFirst, we will download a pre-existing map, also from https://superweb.statssa.gov.za/webapi/.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/mapview.kmz?raw=true\", \"map.kmz\", quiet = TRUE, mode = \"wb\")\nunzip(\"map.kmz\")\nst_read(\"doc.kml\") |>\n  select(-Description) -> map\n\n\nHere is the outline of that map:\n\n\nCode\nggplot() +\n  geom_sf(data = map)\n\n\n\n\n\n\n\n\nNow we can link the data table to the map\n\n\nCode\nmap |>\n  left_join(languages, by = \"Name\") -> map\n\n\nand then plot one of the variables.\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = IsiXhosa), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark blue\") +\n  ggtitle(\"% Population speaking Xhosa\")\n\n\n\n\n\n The really nice thing about this is that it is now very easy to change the appearance of the map with only minor updates to the code.\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"cartodark\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = English), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  ggtitle(\"% Population speaking English\")\n\n\n\n\n\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"thunderforestlandscape\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = Afrikaans), alpha = 0.8, col = \"transparent\") +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  annotation_north_arrow(which_north = \"grid\", location = \"topright\") +\n  ggtitle(\"% Population speaking Afrikaans\")\n\n\n\n\n\n\n\n\nFinally, once we are happy with it, we can export the image in a format suitable for a journal publication or to insert into other documents such as Microsoft Word.\nAs jpeg, to print quality:\n\n\nCode\nggsave(\"mymap.jpg\", device = \"jpeg\", width = 7, height = 6, units = \"in\",\n       dpi = \"print\")\n\n\nAs pdf:\n\n\nCode\nggsave(\"mymap.pdf\", device = \"pdf\", width = 7, height = 6, units = \"in\")\n\n\nAs bmp, to screen quality:\n\n\nCode\nggsave(\"mymap.bmp\", device = \"bmp\", width = 7, height = 6, units = \"in\",\n       dpi = \"screen\")\n\n\nIf we now look in your working directory, they should be there:\n\n\nCode\nlist.files(pattern = \"mymap\")\n\n\n[1] \"mymap.bmp\" \"mymap.jpg\" \"mymap.pdf\""
  },
  {
    "objectID": "why.html#a-worked-example",
    "href": "why.html#a-worked-example",
    "title": "A Cartographic Answer",
    "section": "",
    "text": "Let’s answer the ‘why?’ question with a quick example of R in use. We will not worry about the exact detail of what the code means at this stage or attempt to explain it in full. Instead, we will largely take it as we find it, copying and pasting from this webpage into the R Console. The focus is on some of what R can do from a geographic perspective and not, at this stage, on how it does it.\n\nIf you find that the + sign stays on your screen, in the R Console, for a while and isn’t followed by &gt; then you have either forgotten to hit Enter/Return or have not included all of the code that is needed to complete an operation (to complete a function, for example). You can always press esc on your keyboard and try again.\nIt was suggested in ‘Getting Started’ that you might want to create a new project for this course (a folder in which to save all the files). If you followed that advice, begin by using File –&gt; Open Project… from the dropdown menus. Or, if you didn’t, you could create a new project now (File –&gt; New Project…). Remember, all it really does is create a new folder in which to store all your files but that is useful because it will ensure your working directory is the same as that folder.\n\nMac users: to speed-up the rendering of the maps, it is recommended that you change the Backend Graphics Device to AGG. You can do this through the dropdown menus: Tools -&gt; Global Options…, then click on Graphics (next to Basic). This will matter more in later exercises but you might as well do it now.\n\n\nFirst, we will check that the necessary packages are installed and then require them, which means to load them so they are available to use. The usual way to install a package is with the function, install.packages() so, for example, the graphics package ggplot2 is installed using install.packages(\"ggplot2\"). The code below is a bit more elaborate as it checks which packages have not yet been installed and installs them. However, the two-step process is the same: install and then require,\n\nuse install.packages() to install packages – only needs to be done once on your computer, unless you re-install R / replace it with a more recent version;\nthen use require() to load the desired packages – needs to be done each time R is restarted.\n\n\n\nCode\n# Checks to see which packages are already installed:\ninstalled &lt;- installed.packages()[,1]\n# Creates a character vector of packages that are required:\npkgs &lt;- c(\"XML\", \"tidyverse\", \"readxl\", \"proxy\", \"sf\", \"ggplot2\",\n              \"classInt\", \"ggspatial\")\n# Checks which of the required packages have not yet been installed:\ninstall &lt;- pkgs[!(pkgs %in% installed)]\n# Installs any that have not yet been installed:\nif(length(install)) install.packages(install, dependencies = TRUE)\n# Silently loads (requires) all the packages that are needed:\ninvisible(lapply(pkgs, require, character.only = TRUE))\n\n\n\nThe use of # indicates a comment in the code. It is there just for explanation. It is not executable code (it is ignored not run).\n\n\n\nNext, we will download a data table published by Statistics South Africa that provides estimates of the number of people speaking various languages in the South African Provinces in 2011. These data were downloaded from https://superweb.statssa.gov.za/webapi/. The data are found in an Excel spreadsheet, which is read in and manipulated, converting the counts into percentages.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/table_2022-06-22_17-36-26.xlsx?raw=true\", \"language.xlsx\", quiet = TRUE, mode = \"wb\")\n\nread_xlsx(\"language.xlsx\", sheet = \"Data Sheet 0\", skip = 8) |&gt;\n  rename(Name = 2) |&gt;\n  drop_na(Afrikaans) |&gt;\n  select(-1) |&gt;\n  mutate(across(where(is.numeric), ~ round(. / Total * 100, 2))) -&gt; languages\n\n\nHere is the top of the data, viewed in the R environment:\n\n\nCode\nhead(languages)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# ℹ 5 more variables: SiSwati &lt;dbl&gt;, Tshivenda &lt;dbl&gt;, Xitsonga &lt;dbl&gt;,\n#   Other &lt;dbl&gt;, Total &lt;dbl&gt;\n\n\nThere is often more than one way of achieving something in R. Here we could also use,\n\n\nCode\nslice_head(languages, n = 6)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# ℹ 5 more variables: SiSwati &lt;dbl&gt;, Tshivenda &lt;dbl&gt;, Xitsonga &lt;dbl&gt;,\n#   Other &lt;dbl&gt;, Total &lt;dbl&gt;\n\n\n\n\n\nWhat R allows is the opportunity to map the data without needing to go outside R to use separate software such as GIS. To do so, we will need a ‘blank map’ of the Provinces that can be joined with the data to create a choropleth map (a type of thematic map).\nFirst, we will download a pre-existing map, also from https://superweb.statssa.gov.za/webapi/.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/mapview.kmz?raw=true\", \"map.kmz\", quiet = TRUE, mode = \"wb\")\nunzip(\"map.kmz\")\nst_read(\"doc.kml\") |&gt;\n  select(-Description) -&gt; map\n\n\nHere is the outline of that map:\n\n\nCode\nggplot() +\n  geom_sf(data = map)\n\n\n\n\n\n\n\n\nNow we can link the data table to the map\n\n\nCode\nmap |&gt;\n  left_join(languages, by = \"Name\") -&gt; map\n\n\nand then plot one of the variables.\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = IsiXhosa), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark blue\") +\n  ggtitle(\"% Population speaking Xhosa\")\n\n\n\n\n\n The really nice thing about this is that it is now very easy to change the appearance of the map with only minor updates to the code.\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"stamenwatercolor\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = English), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  ggtitle(\"% Population speaking English\")\n\n\n\n\n\n\n\nCode\nggplot() +\n  annotation_map_tile(type = \"thunderforestlandscape\", progress = \"none\") +\n  geom_sf(data = map, aes(fill = Afrikaans), alpha = 0.8, col = \"transparent\") +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  annotation_north_arrow(which_north = \"grid\", location = \"topright\") +\n  ggtitle(\"% Population speaking Afrikaans\")\n\n\n\n\n\n\n\n\nFinally, once we are happy with it, we can export the image in a format suitable for a journal publication or to insert into other documents such as Microsoft Word.\nAs jpeg, to print quality:\n\n\nCode\nggsave(\"mymap.jpg\", device = \"jpeg\", width = 7, height = 6, units = \"in\",\n       dpi = \"print\")\n\n\nAs pdf:\n\n\nCode\nggsave(\"mymap.pdf\", device = \"pdf\", width = 7, height = 6, units = \"in\")\n\n\nAs bmp, to screen quality:\n\n\nCode\nggsave(\"mymap.bmp\", device = \"bmp\", width = 7, height = 6, units = \"in\",\n       dpi = \"screen\")\n\n\nIf we now look in your working directory, they should be there:\n\n\nCode\nlist.files(pattern = \"mymap\")\n\n\n[1] \"mymap.bmp\" \"mymap.jpg\" \"mymap.pdf\""
  },
  {
    "objectID": "why.html#another-example",
    "href": "why.html#another-example",
    "title": "A Cartographic Answer",
    "section": "Another example",
    "text": "Another example\nThe following example is much more complex so please don’t be put off by it. I have included it to make a simple point – it does not take too many lines of code to produce a high quality visual output. It might take a little bit of searching around online to find the code and instruction to produce exactly what you want but I rarely struggle to find an answer fairly quickly.\nI originally developed the following maps in response to the release of the 2021 UK Census data showing the ethnic composition of small area neighbourhoods. The four cities – Birmingham, Leicester, London and Manchester – are the ones that are no longer majority White British (i.e. less than half their population self-identified as White British). A consequence of this demographic change is that the cities are becoming more ethnically diverse, which is what the maps show, using a standardised census geography that I also created in R.\n\n\nCode\n# Read-in the attribute data and the boundary file:\ndf <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/diversity.csv?raw=true\")\nmap <- st_read(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/cities.geojson\", quiet = TRUE)\n\n# Although more complex, at heart what the following code does is\n# join the map to the data and then produce a separate map for\n# each city and time period, using a consistent style\ndf |>\n  pivot_longer(where(is.numeric), values_to = \"index\", names_to = \"year\") %>%\n  mutate(year = paste0(\"20\",substring(year, 3, 4))) %>%\n  left_join(map, ., by = \"OAXXCD\") %>%\n  mutate(group = paste(CITY, year, sep = \" ~ \")) %>%\n  split(.$group) %>%\n\n  lapply(function(x) {\n    \n    ggplot(data = x, aes(fill = index)) +\n      geom_sf(col = \"transparent\") +\n      scale_fill_viridis_c(\"Diversity\",\n                           values = c(0,0.25,0.5,0.7,0.85,0.95,1)) +\n      annotation_north_arrow(location = \"tl\",\n                            style = north_arrow_minimal(text_size = 10),\n                            height = unit(0.6, \"cm\"), width = unit(0.6, \"cm\")) +\n      annotation_scale(location = \"br\", style = \"ticks\", line_width = 0.5,\n                       text_cex = 0.5, tick_height = 0.4,\n                       height = unit(0.15, \"cm\"), text_pad = unit(0.10, \"cm\")) +\n      theme_minimal() +\n      theme(axis.text = element_blank(),\n            axis.ticks = element_blank(),\n            plot.title = element_text(size = 8, hjust = 0.5),\n            legend.title = element_text(size = 7, vjust = 3),\n            legend.text =element_text(size = 6), \n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            plot.margin = margin(t = 0,  \n                                 r = 0,  \n                                 b = 0,\n                                 l = 0)) +\n      labs(title = paste0(x$CITY[1], \": \", x$year[1]))\n  }) -> g\n\n# The cowplot library offers some additional plotting functionality\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\")\nrequire(cowplot)\n\n# The following gets the common legend for the maps\n# and stops it being printed 12 times -- once will be enough!\nlegend <- get_legend(g[[1]])\nlapply(g, function(x) {\n  x + theme(legend.position='none')\n}) -> g\n\n# This brings all the maps together as one\nggdraw(plot_grid(plot_grid(plotlist = g, ncol=3, align='v'),\n                 plot_grid(NULL, legend, ncol=1, scale = 0.5),\n                 rel_widths=c(1, 0.1),\n                 rel_heights=c(1, 0,1))) -> g\n\nprint(g)"
  },
  {
    "objectID": "why.html#convinced",
    "href": "why.html#convinced",
    "title": "A Cartographic Answer",
    "section": "Convinced?",
    "text": "Convinced?\nOf course, maps can also be produced in open source software such as QGIS and GIS software certainly have their use. R is not automatically better or necessarily a replacement for these. However, what it does offer is an integrated environment for what we might call geographic data science: we can download data from external websites, load and tidy-up those data, fit statistical or other models to them and map the results – all from within R. Our stages of working can be saved as scripts, which are faster to change and modify than using ‘point-and-click’ operations, and we can share our code with other people (even those using different operating systems) facilitating collaborative working and reproducible social-/ science. Finally, there are lots of packages available for reading, visualising, and analysing spatial data in R. Some of them are summarised here. These are attractive reasons for mapping and modelling within R."
  },
  {
    "objectID": "why.html#alternatives",
    "href": "why.html#alternatives",
    "title": "A Cartographic Answer",
    "section": "Alternatives",
    "text": "Alternatives\n\nAside from software such as QGIS, an interesting area of development is Geographic Data Science with Python. You can learn more about it here."
  },
  {
    "objectID": "why.html#need-more-convincing",
    "href": "why.html#need-more-convincing",
    "title": "A Cartographic Answer",
    "section": "Need more convincing?",
    "text": "Need more convincing?\nIf you have time, have a look at this exercise that we sometimes use with prospective students at University open days. The idea of the exercise is not to teach the students R but to show them how we use R for geographic data science in the School of Geographical Sciences. What the exercise does is take COVID-19 data for English neighbourhoods, fit statistical models to it and map the results – all in R. Again, it is the ability to use R for all the stages shown below that makes it so useful.\n\nSource: R for Data Science"
  },
  {
    "objectID": "base.html#introduction",
    "href": "base.html#introduction",
    "title": "Base R",
    "section": "Introduction",
    "text": "Introduction\nBase R is what you download from CRAN. You might think of it as classic R. A short introduction of ‘the basics’ is provided below. For a fuller introduction, see the software manual, An Introduction to R. It is worth reading even if you end-up regularly using the Tidyverse variant of R described in the next session as there are some tasks that are (in my opinion) easier to do using Base R or by mixing it up a little."
  },
  {
    "objectID": "base.html#functions",
    "href": "base.html#functions",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\nR is a functional programming language where functions ‘do things’ to objects. What they do is dependent upon the class/type and attributes of the objects that go into the function, and also on the arguments of the function.\nFor example, try typing the following into the R Console, which is the bottom left panel of R Studio. Type it alongside the prompt symbol, > then hit Enter/Return.\n\n\nCode\nround(10.32, digits = 0)\n\n\n[1] 10\n\n\nThis calls the function round(), which is operating on the numeric object, 10.32. The argument digits specifies the number of digits to round to. It is set to zero in the example above.\nBecause digits = 0 is the default value for the function, we could just write\n\n\nCode\nround(10.32)\n\n\n[1] 10\n\n\nand obtain the same answer as before. I know that digits = 0 is the default value because, as I type the name of the function into the R Console, I see the arguments of the function and any default values appear.\n\nWe can also find out more about the function, including some examples of its use, by opening its help file.\n\n\nCode\n?round\n\n\n Should we wish to round 10.32 to one digit then we are no longer rounding to the default of zero decimal places and must therefore specify the argument explicitly (the default is no longer what we want).\n\n\nCode\nround(10.32, digits = 1)\n\n\n[1] 10.3\n\n\nThe following also works because it preserves the order of the arguments in the function.\n\n\nCode\nround(10.32, 1)\n\n\n[1] 10.3\n\n\nIn other words, if we do not specifically state that x = 10.32 (where x is a numeric vector; here, 10.32) and digits = 1 then they will be taken as the first and second arguments of the function. This requires care to make sure they genuinely are in the right order. If you aren’t certain, then define the arguments explicitly because they will then work out of order.\n\n\nCode\nround(digits = 1, x = 10.32)\n\n\n[1] 10.3\n\n\n In the examples above, both the input to and output from the function are a numeric vector of type double. The input is:\n\n\nCode\nclass(10.32)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(10.32)\n\n\n[1] \"double\"\n\n\nThe output is:\n\n\nCode\nclass(round(10.32, digits = 1))\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(round(10.32, digits = 1))\n\n\n[1] \"double\"\n\n\nNote how a function can be wrapped within a function, as in the example above: class(round(...)).\n At the moment we are using x = 10.32, which is a numeric vector of length 1,\n\n\nCode\nlength(10.32)\n\n\n[1] 1\n\n\nHowever, the round() function can operate on numeric vectors of other lengths too.\n\n\nCode\nround(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7))\n\n\n[1] 1 2 3 4 6 7 8\n\n\nHere the combine function, c is used to create a vector of length 7, which is the input into round(). The output is of length 7 too.\n\n\nCode\nlength(round(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7)))\n\n\n[1] 7\n\n\n\nThere are lots of functions for R and I often forget what I need. Fortunately, there is a large user community too and so a quick web search often helps me quickly find what I need. Don’t be afraid to do a Google search for what you need.\n\nWriting a new function\nWe can write our own functions. The following will take a number and report whether it is a prime number or not.\n\n\nCode\nis.prime <- function(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}\n\n\nLet’s try it.\n\n\nCode\nis.prime(2)\n\n\n[1] TRUE\n\n\nCode\nis.prime(10)\n\n\n[1] FALSE\n\n\nCode\nis.prime(13)\n\n\n[1] TRUE\n\n\nCode\nis.prime(3.3)\n\n\n[1] NA\n\n\n There is quite a lot to unpack about the function. It is not all immediately relevant but it is instructive to have an overview of what it is doing. First of all the function takes the form\nf <- function(x) {\n  ...\n}\nwhere x is the input into the function in much the same way that x is the number to be rounded in round(x = ...). It is a ‘place holder’ for the input into the function.\nStatements such as if(x == 2) are logical statements: if(...) is true then do whatever follows. If what is to be done spans over multiple lines, they are enclosed by ‘curly brackets’, {...}.\nThe statement if(x < 2 | x %% floor(x) != 0) in the function is also a logical statement with the inclusion of an or statement, denoted by |. What it is checking is whether x < 2 or if x is a fraction. Had we needed to have both conditions to be met, then an and statement would be used, denoted by & instead of |. Note that ! means not, so != tests for not equal to and is the opposite of ==, which tests for equality.\nWhere it says, 2:(x-1), this is equivalent to the function, seq(from = 2, to = (x-1), by = 1). It generates a sequence of integer numbers from \\(2\\) to \\((x-1)\\).\n\n\nCode\nx <- 10\n2 : (x - 1)\n\n\n[1] 2 3 4 5 6 7 8 9\n\n\nCode\nseq(from = 2, to = (x-1), by = 1)\n\n\n[1] 2 3 4 5 6 7 8 9\n\n\nifelse() is another logical statement. It takes the form, ifelse(condition, a, b): if the condition is met then do a, else do b. In the prime number function it is checking whether dividing \\(x\\) by any of the numbers from \\(2\\) to \\((x-1)\\) generates a whole number.\nFinally, the function return() returns an output from the function; here, a logical vector of length 1 that is TRUE, FALSE or NA dependent upon whether \\(x\\) is or is not a prime number, or if it is not a whole number above \\(1\\).\nNote that in newer versions of R, functions can also take the form,\nf <- \\(x) {\n  ...\n}\nTherefore the following is exactly equivalent to before.\n\n\nCode\nis.prime <- \\(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "base.html#objects-and-classes",
    "href": "base.html#objects-and-classes",
    "title": "Base R",
    "section": "Objects and Classes",
    "text": "Objects and Classes\nOur function that checks for a prime number is stored in the object is.prime.\n\n\nCode\nclass(is.prime)\n\n\n[1] \"function\"\n\n\nThere are other classes of object in R. Some of the most common are listed below.\n\nLogical\nThe output from the is.prime() function is an example of an object of class logical because the answer is TRUE or FALSE (or NA, not applicable).\n\n\nCode\nx <- is.prime(10)\nprint(x)\n\n\n[1] FALSE\n\n\nCode\nclass(x)\n\n\n[1] \"logical\"\n\n\nSome other examples:\n\n\nCode\ny <- 10 > 5\nprint(y)\n\n\n[1] TRUE\n\n\nCode\nclass(y)\n\n\n[1] \"logical\"\n\n\nCode\nz <- 2 == 5   # is 2 equal to 5?\nprint(z)\n\n\n[1] FALSE\n\n\n\n\nNumeric\nWe have already seen that some objects are numeric.\n\n\nCode\nx <- mean(0:100)\nprint(x)\n\n\n[1] 50\n\n\nCode\nclass(x)\n\n\n[1] \"numeric\"\n\n\nThis presently is of type double; i.e. it allows for decimal places even where they are not required.\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nbut it could be converted to class integer (a whole number with no decimal places).\n\n\nCode\nx <- as.integer(x)\nclass(x)\n\n\n[1] \"integer\"\n\n\n\n\nCharacter\nOther classes include character. Note the difference between the length() of a character vector and the number of characters, nchar(), that any element of that vector contains.\n\n\nCode\nx <- \"Mapping and Modelling in R\"\nprint(x)\n\n\n[1] \"Mapping and Modelling in R\"\n\n\nCode\nlength(x)   # There is only one element in this vector\n\n\n[1] 1\n\n\nCode\nnchar(x)    # And that element contains 26 letters\n\n\n[1] 26\n\n\nCode\nclass(x)\n\n\n[1] \"character\"\n\n\nCode\ny <- paste(x, \"with Richard Harris\")\nprint(y)\n\n\n[1] \"Mapping and Modelling in R with Richard Harris\"\n\n\nCode\nlength(y)   # There is still only one element\n\n\n[1] 1\n\n\nCode\nnchar(y)    # But now it contains more letters\n\n\n[1] 46\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nCode\nz <- unlist(strsplit(x, \" \"))\nprint(z)\n\n\n[1] \"Mapping\"   \"and\"       \"Modelling\" \"in\"        \"R\"        \n\n\nCode\nlength(z)   # The initial vectors has been split into 5 parts\n\n\n[1] 5\n\n\nCode\nnchar(z)\n\n\n[1] 7 3 9 2 1\n\n\nCode\nclass(z)\n\n\n[1] \"character\"\n\n\n\nAs the name suggests, print is a function that prints its contents to screen. Often it can be omitted in favour of referencing the object directly. For instance, in the example above, rather than typing print(z) it would be sufficient just to type z. Just occasionally though you will find that an object does not print as you intended when the function is omitted. If this happens, try putting print back in.\n\n\nMatrix\nAn example of a matrix is\n\n\nCode\nx <- matrix(1:9, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nCode\nncol(x)   # Number of columns\n\n\n[1] 3\n\n\nCode\nnrow(x)   # Number of rows\n\n\n[1] 3\n\n\nCode\nclass(x)\n\n\n[1] \"matrix\" \"array\" \n\n\nHere the argument byrow is changed from its default value of FALSE to be TRUE:\n\n\nCode\ny <- matrix(1:9, ncol = 3, byrow = TRUE)\n\n\nThis result is equivalent to the transpose of the original matrix.\n\n\nCode\ny\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nCode\nt(x)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\nData frame\nA data.frame is a table of data, such as,\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nncol(df)    # Number of columns\n\n\n[1] 4\n\n\nCode\nnrow(df)    # Number of rows\n\n\n[1] 7\n\n\nCode\nlength(df)  # The length is also the number of columns\n\n\n[1] 4\n\n\nCode\nnames(df)   # The names of the variables in the data frame\n\n\n[1] \"Day\"   \"Date\"  \"Month\" \"Year\" \n\n\nNote that the length of each column should be equal in the specification of the data frame. The following will generate an error because the Date column is now too short. You might wonder why the Month and Year columns were fine previously when, in fact, they were give only one value, whereas there are 7 days and 7 dates. It is because R recycled them the requisite number of times (i.e. it gave all the rows the same value for Month and Year – it recycled June and 2022 seven times). That option isn’t available for the example below where there are 7 days but 6 dates.\n# This will generate an error\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:25,\n                 Month = \"June\",\n                 Year = 2022)\n\n\nFactors\nEarlier versions of R would, by default, convert character fields in a data frame into factors. The equivalent operation now is,\n\n\nCode\ndf2 <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022, stringsAsFactors = TRUE)\n\n\nTreating character fields as factors was clever but frustrating if you didn’t realise it was happening and wanted the characters to remains as characters. The difference is not immediately obvious,\n\n\nCode\nhead(df, n= 2)    # with stringsAsFactors = FALSE (the current default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nCode\nhead(df2, n = 2)  # with stringsAsFactors = TRUE  (the historic default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nThese appear to be the same but differences begin to be apparent in the following:\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf2$Day\n\n\n[1] Mon   Tues  Wed   Thurs Fri   Sat   Sun  \nLevels: Fri Mon Sat Sun Thurs Tues Wed\n\n\nCode\ndf$Month\n\n\n[1] \"June\" \"June\" \"June\" \"June\" \"June\" \"June\" \"June\"\n\n\nCode\ndf2$Month\n\n\n[1] June June June June June June June\nLevels: June\n\n\nBasically, a factor is a categorical variable: it encodes which groups or categories (which levels) are to be found in the variable. Knowing this, it is possible to count the number of each group, as in,\n\n\nCode\nsummary(df2)\n\n\n    Day         Date       Month        Year     \n Fri  :1   Min.   :20.0   June:7   Min.   :2022  \n Mon  :1   1st Qu.:21.5            1st Qu.:2022  \n Sat  :1   Median :23.0            Median :2022  \n Sun  :1   Mean   :23.0            Mean   :2022  \n Thurs:1   3rd Qu.:24.5            3rd Qu.:2022  \n Tues :1   Max.   :26.0            Max.   :2022  \n Wed  :1                                         \n\n\nbut not\n\n\nCode\nsummary(df)\n\n\n     Day                 Date         Month                Year     \n Length:7           Min.   :20.0   Length:7           Min.   :2022  \n Class :character   1st Qu.:21.5   Class :character   1st Qu.:2022  \n Mode  :character   Median :23.0   Mode  :character   Median :2022  \n                    Mean   :23.0                      Mean   :2022  \n                    3rd Qu.:24.5                      3rd Qu.:2022  \n                    Max.   :26.0                      Max.   :2022  \n\n\n Factors can be useful but do not always behave as you might anticipate. For example,\n\n\nCode\nx <- c(\"2021\", \"2022\")\nas.numeric(x)\n\n\n[1] 2021 2022\n\n\nis different from,\n\n\nCode\nx <- factor(c(\"2021\", \"2022\"))\nas.numeric(x)\n\n\n[1] 1 2\n\n\nThese days the defult is stringsAsFactors = FALSE, which is better when using functions such as read.csv() to read a .csv file into a data.frame in R.\n\n\nLists\nA list is a more flexible class that can hold together other types of object. Without a list, the following only works because the 1:3 are coerced from numbers in x to characters in y – note the \" \" that appear around them, which shows they are now text.\n\n\nCode\nx <- as.integer(1:3)\nclass(x)\n\n\n[1] \"integer\"\n\n\nCode\ny <- c(\"a\", x)\ny\n\n\n[1] \"a\" \"1\" \"2\" \"3\"\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nOn the other hand,\n\n\nCode\ny <- list(\"a\", x)\n\n\ncreates a ragged list of two parts:\n\n\nCode\nclass(y)\n\n\n[1] \"list\"\n\n\nCode\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n\nThe first part has the character \"a\" in it.\n\n\nCode\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\nclass(y[[1]])\n\n\n[1] \"character\"\n\n\nThe second has the numbers 1 to 3 in it.\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\nclass(y[[2]])\n\n\n[1] \"integer\"\n\n\nNote that the length of the list is the length of its parts. Presently it is 2 but the following example has a length of three.\n\n\nCode\ny <- list(\"a\", x, df)\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nlength(y)\n\n\n[1] 3\n\n\n This should not be confused with the length of any one part.\n\n\nCode\nlength(y[[1]])\n\n\n[1] 1\n\n\nCode\nlength(y[[2]])\n\n\n[1] 3\n\n\nCode\nlength(y[[3]])\n\n\n[1] 4"
  },
  {
    "objectID": "base.html#assignments",
    "href": "base.html#assignments",
    "title": "Base R",
    "section": "Assignments",
    "text": "Assignments\nThroughout this document I have used the assignment term <- to store the output of a function, as in x <- as.integer(1:3) and y <- list(\"a\", x, df), and so forth. The <- is used to assign the result of a function to an object. You can, if you prefer use =. For example, all the following make the same assignment, which is to give x the value of 1.\n\n\nCode\nx <- 1\nx = 1\n1 -> x\n\n\nPersonally, I avoid using = as an assignment for the following reasons.  First, not to confuse assignments with arguments,\n\n\nCode\nx <- round(10.32, digits = 1)   # I think this is a bit clearer\nx = round(10.32, digits = 1)    # and this a bit less so\n\n\nSecond, to not confuse assignments with logical statements,\n\n\nCode\nx <- 1\ny <- 2\nz <- x == y   # Again, this is a bit clearer\nz = x == y    # and this not so much\n\n\nThird – but this is pedantic – to avoid the following sort of situation which makes no sense mathematically…\n\n\nCode\nx = 1\ny = 2\nx = y\n\n\n… but does in terms of what it really means:\n\n\nCode\nx <- 1\ny <- 2\nx <- y # Assign the value of y to x, overwriting its previous value\n\n\n Which you use is a matter of personal preference and, of course, = has one less character than <- to worry about. However, this course is written with,\n<- (or ->) is as assignment, as in x <- 1;\n= is the value of an argument, as in round(x, digits = 1); and\n== is a logical test for equality, as in x == y.\n\nIt is important to remember that R is case sensitive. An object called x is different from one called X; y is not the same as Y and so forth."
  },
  {
    "objectID": "base.html#manipulating-objects",
    "href": "base.html#manipulating-objects",
    "title": "Base R",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nIn addition to passing objects to functions such as…\n\n\nCode\nx <- 0:100\nmean(x)\n\n\n[1] 50\n\n\nCode\nsum(x)\n\n\n[1] 5050\n\n\nCode\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      25      50      50      75     100 \n\n\nCode\nmedian(x)\n\n\n[1] 50\n\n\nCode\nquantile(x, probs = c(0, 0.25, 0.5, 0.75, 1))\n\n\n  0%  25%  50%  75% 100% \n   0   25   50   75  100 \n\n\nCode\nhead(sqrt(x)) # The square roots of the first of x\n\n\n[1] 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nCode\ntail(x^2)     # The square roots of the last of x\n\n\n[1]  9025  9216  9409  9604  9801 10000\n\n\nCode\nsd(x)         # The standard deviation of x\n\n\n[1] 29.30017\n\n\n…there are other ways we may wish to interact with objects.\n\nMathematical operations\nMathematical operations generally operate on a pairwise basis between corresponding elements in a vector. For example,\n\n\nCode\nx <- 1\ny <- 3\nx + y\n\n\n[1] 4\n\n\nCode\nx <- 1:5\ny <- 6:10\nx + y\n\n\n[1]  7  9 11 13 15\n\n\nCode\nx * y   # Multiplication\n\n\n[1]  6 14 24 36 50\n\n\nCode\nx / y   # Divisions\n\n\n[1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000\n\n\nIf one vector is shorter that the other, values will be recycled. In the following example the results are \\(1\\times6\\), \\(2\\times7\\), \\(3\\times8\\), \\(4\\times9\\) and then \\(5\\times6\\) as y is recycled.\n\n\nCode\nx <- 1:5  # This is a vector of length 5\ny <- 6:9  # This is a vector of length 4\nx * y     # A vector of length 5 but some of y is recycled\n\n\n[1]  6 14 24 36 30\n\n\n\n\nSubsets of objects\n\nVectors\nIf x is a vector then x[n] is the nth element in the vector (the nth position, the nth item). To illustrate,\n\n\nCode\nx <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nx[1]\n\n\n[1] \"a\"\n\n\nCode\nx[3]\n\n\n[1] \"c\"\n\n\nCode\nx[c(1, 3, 5)]\n\n\n[1] \"a\" \"c\" \"e\"\n\n\nCode\nx[length(x)]\n\n\n[1] \"f\"\n\n\nThe notation -n can be used to exclude elements.\n\n\nCode\nx[-3]   # All of x except the 3rd element\n\n\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n\nCode\nx[c(-1, -3, -5)]    # x without the 1st, 3rd and 5th elements\n\n\n[1] \"b\" \"d\" \"f\"\n\n\n\n\nMatrices\nIf x is a matrix then x[i, j] is the value of the ith row of the jth column:\n\n\nCode\nx <- matrix(1:10, ncol = 2)\nx\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[1, 1]     # row 1, column 1\n\n\n[1] 1\n\n\nCode\nx[2, 1]     # row 2, column 1\n\n\n[1] 2\n\n\nCode\nx[c(3, 5), 2]   # rows 3 and 5 of column 2\n\n\n[1]  8 10\n\n\nCode\nx[nrow(x), ncol(x)]   # the final entry in the matrix\n\n\n[1] 10\n\n\n All of the values in the ith row can be selected using the form x[i, ]\n\n\nCode\nx[1, ]    # row 1\n\n\n[1] 1 6\n\n\nCode\nx[3, ]    # row 3\n\n\n[1] 3 8\n\n\nCode\nx[c(1, 5), ]  # rows 1 and 5\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    5   10\n\n\nCode\nx[c(-1, -3), ]  # All except the 1st and 3rd rows\n\n\n     [,1] [,2]\n[1,]    2    7\n[2,]    4    9\n[3,]    5   10\n\n\nSimilarly, all of the values in the jth column can be selected using the form x[, j]\n\n\nCode\nx[ ,1]    # column 1\n\n\n[1] 1 2 3 4 5\n\n\nCode\nx[ ,2]    # column 2\n\n\n[1]  6  7  8  9 10\n\n\nCode\nx[ , 1:2]   # columns 1 and 2\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[-3 , 1:2]   # columns 1 and 2 except row 3\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    4    9\n[4,]    5   10\n\n\n\n\nData frames\nData frames are not unlike a matrix.\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf[, 1]   # The first column\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, 1]  # The first row of the first column (Day)\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, 2]  # The second row of the second column (Date)\n\n\n[1] 21\n\n\nHowever, you can also reference the variable name directly, through the x$variable style notation,\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ndf$Date[2]\n\n\n[1] 21\n\n\nAlternatively, if you wish, with the square brackets, using the [, \"variable\"] format.\n\n\nCode\ndf[, \"Day\"]\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, \"Day\"]\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, \"Date\"]\n\n\n[1] 21\n\n\n\n\nLists\nWe have already seen the use of double square brackets, [[...]] to refer to a part of a list:\n\n\nCode\nx <- 1:3\ny <- list(\"a\", x, df)\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\ny[[3]]\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nThe extension to this is to be able to refer to a specific element within a part of the list by combining it with the other notation. Some examples are:\n\n\nCode\ny[[1]][1]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]][3]\n\n\n[1] 3\n\n\nCode\ny[[3]]$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ny[[3]]$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ny[[3]][2, \"Date\"]\n\n\n[1] 21\n\n\n\nThe way to remember the difference between [[...]] and [...] is that the double square brackets reference a specific part of a list, for example [[3]], the third part; the single square brackets reference a position or element in a vector, such as [4], the fourth. Combining them, [[3]][4] is the 4th element of a vector where that vector forms the 3rd part of a list."
  },
  {
    "objectID": "base.html#deleting-objects-and-saving-the-workspace",
    "href": "base.html#deleting-objects-and-saving-the-workspace",
    "title": "Base R",
    "section": "Deleting objects and saving the workspace",
    "text": "Deleting objects and saving the workspace\nMy current working directory is,\n\n\nCode\ngetwd()\n\n\n[1] \"/Users/ggrjh/Dropbox/github/MandM\"\n\n\nand it contains the following objects:\n\n\nCode\nls()\n\n\n[1] \"df\"       \"df2\"      \"is.prime\" \"x\"        \"y\"        \"z\"       \n\n\nYours will be different. Remember, it can be useful to create a new project for a new collection of work that you are doing in R and then opening that project each time you start R will ensure that the working directory is that of the project.\nTo delete a specific object, use rm(),\n\n\nCode\nrm(z)\n\n\nOr, more than one,\n\n\nCode\nrm(df, df2, is.prime)\n\n\nTo save the workspace and all the objects it now contains use the save.image() function.\n\n\nCode\nsave.image(\"workspace1.RData\")\n\n\nTo delete all the objects created in your workspace, use\n\n\nCode\nrm(list=ls())\n\n\n\nIt is a good idea to save a workspace with a new filename before deleting too much from your workspace to allow you to recover it if necessary. Be especially careful if you use rm(list=ls()) as there is no undo function. The best you can do is load the workspace as it was the last time that you saved it.\nTo (re)load a workspace, use load().\n\n\nCode\nload(\"workspace1.RData\")"
  },
  {
    "objectID": "base.html#further-reading",
    "href": "base.html#further-reading",
    "title": "Base R",
    "section": "Further reading",
    "text": "Further reading\nThis short introduction to base R has really only scratched the surface. There are many books about R that provide a lot more detail but, to remind you, the manual that comes with the software is worth reading and probably the best place to start – An Introduction to R. It is thorough but also relatively short.\nDon’t worry if not everything makes sense at this stage. The best way to learn R is to put it into practice and that is what we shall be doing in later sessions."
  },
  {
    "objectID": "tidyverse.html#introduction",
    "href": "tidyverse.html#introduction",
    "title": "Tidyverse",
    "section": "Introduction",
    "text": "Introduction\nIf base R is R Classic then tidyverse is a new flavour of R, designed for data science. It consists of a collection of R packages that “share an underlying design philosophy, grammar, and data structures.”\nTidyverse is easier to demonstrate than to pin-down to some basics so let’s work through an example using both base R and tidyverse to illustrate some differences."
  },
  {
    "objectID": "tidyverse.html#to-start",
    "href": "tidyverse.html#to-start",
    "title": "Tidyverse",
    "section": "To Start",
    "text": "To Start\nIf, as suggested in ‘Getting Started’, you have created an R Project to contain all the files you create and download for this course then open it now by using File –> Open Project… from the dropdown menus in R Studio. If you have not created one then now might be a good time!\nWe will begin by downloading a data file to use. It will be downloaded to your working directory, which is the folder associated with your R Project if you are using one. You can check the working directory by using getwd() and change it using Session –> Set Working Directory or with the function setwd(dir) where dir is the chosen directory. If you have created a Project then the working directory is that of the Project.\nThe data are an extract of the Covid Data Dashboard for England in December 2021. Some prior manipulation and adjustments to those data have been undertaken for another project so treat them as indicative only. The actual reported numbers may have been changed slightly from their originals although only marginally so.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_extract.csv\", \"covid.csv\", mode = \"wb\", quiet = TRUE) \n\n\nWe also need to require(tidyverse) ready for use.\n\n\nCode\nrequire(tidyverse)\n\n\n\nIf you get a warning message saying there is no package called tidyverse then you need to install it: install.packages(\"tidyverse\", dependencies = TRUE). You will find that some people prefer to use library() instead of require(). The difference between them is subtle but you can find an argument in favour of using library() here even though I usually don’t."
  },
  {
    "objectID": "tidyverse.html#reading-in-the-data",
    "href": "tidyverse.html#reading-in-the-data",
    "title": "Tidyverse",
    "section": "Reading-in the data",
    "text": "Reading-in the data\nLet’s read-in and take a look at the data. First in base R.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\nhead(df1)\n\n\n   MSOA11CD regionName X2021.12.04 X2021.12.11 X2021.12.18 X2021.12.25 All.Ages\n1 E02000002     London          25          48         148         176     7726\n2 E02000003     London          46          58         165         215    11246\n3 E02000004     London          24          44         100         141     6646\n4 E02000005     London          58          97         185         231    10540\n5 E02000007     London          38          94         153         205    10076\n6 E02000008     London          54         101         232         245    12777\n\n\nNow using tidyverse,\n\n\nCode\ndf2 <- read_csv(\"covid.csv\")\nslice_head(df2, n = 6)\n\n\n# A tibble: 6 × 7\n  MSOA11CD  regionName `2021-12-04` `2021-12-11` `2021-12-18` `2021-12-25`\n  <chr>     <chr>             <dbl>        <dbl>        <dbl>        <dbl>\n1 E02000002 London               25           48          148          176\n2 E02000003 London               46           58          165          215\n3 E02000004 London               24           44          100          141\n4 E02000005 London               58           97          185          231\n5 E02000007 London               38           94          153          205\n6 E02000008 London               54          101          232          245\n# ℹ 1 more variable: `All Ages` <dbl>\n\n\nThere are some similarities – for example the function read.csv reads-in a file of comma separated variables, as does read_csv. However, the output from these functions differ. First, tidyverse has, in this case, handled the names of the variables better. It has also created what is described as a tibble which is “a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” You can find out more about them and how they differ from traditional data frames here. Basically, they are a form of data frame that fit into tidyverse’s philosophy to try and keep ‘things’ tidy through a shared underlying design philosophy, grammar and data structures."
  },
  {
    "objectID": "tidyverse.html#selecting-and-renaming-variables",
    "href": "tidyverse.html#selecting-and-renaming-variables",
    "title": "Tidyverse",
    "section": "Selecting and renaming variables",
    "text": "Selecting and renaming variables\nWe will now: - select the regionName, 2021-12-04 and All Ages variables; - rename the second of these as cases and the third as population; - and look at the data again to check that it has worked.\nIn base R,\n\n\nCode\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[2:3] <- c(\"cases\", \"population\")\nhead(df1)\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\nIn tidyverse,\n\n\nCode\ndf2 <- select(df2, regionName, `2021-12-04`, `All Ages`)\ndf2 <- rename(df2, cases = `2021-12-04`, population = `All Ages`)\nslice_head(df2, n = 6)\n\n\n# A tibble: 6 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n\n\nComparing the two, the tidyverse code may be more intuitive to understand because of its use of verbs as functions: select(), rename() and so forth."
  },
  {
    "objectID": "tidyverse.html#piping",
    "href": "tidyverse.html#piping",
    "title": "Tidyverse",
    "section": "Piping",
    "text": "Piping\nNow we shall bring the two previous stages together, using what is referred to as a pipe. Without worrying about the detail, which we will return to presently, here is an example of a pipe, |> being used in base R:\n\n\nCode\nread.csv(\"covid.csv\") |>\n  (\\(x) x[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")])() -> df1\nnames(df1)[2:3] <- c(\"cases\", \"population\")\ndf1 |>\n  head()\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\n\nThe above will only work if you are using R version 4.1.0 or above. You can check which version you are running by using R.Version()$version.\nHere is the same process using tidyverse and a different pipe, %>%,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  slice_head(n = 6)\n\n\n# A tibble: 6 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n\n\nThe obvious difference here is that the tidyverse code is more elegant. But what is the pipe and what is the difference between |> in the base R code and %>% in the tidyverse example?\nA pipe is really just a way of sending (’piping`) something from one line of code to the next, to create a chain of commands (forgive the mixed metaphors). For example,\n\n\nCode\nx <- 0:10\nmean(x)\n\n\n[1] 5\n\n\nCould be calculated as\n\n\nCode\n0:10 |>\n  mean()\n\n\n[1] 5\n\n\nAs\n\n\nCode\n0:10 %>%\n  mean()\n\n\n[1] 5\n\n\nor, if you want to save on a few characters of code,\n\n\nCode\n0:10 %>%\n  mean\n\n\n[1] 5\n\n\nHowever, this won’t work:\n\n\nCode\n0:10 |>\n  mean\n\n\nThis is confusing but it is because of the different pipes, one (|>) a more recent development than the other (%>%).\nA more complicated example of piping is below. It employs the function sapply(), a variant of the function lapply(X, FUN) that takes a list X and applies the function FUN to each part of it. In the example, it is the function mean.\nHere it is without any pipes:\n\n\nCode\nx <- list(0:10, 10:20)\n  # Creates a list with two parts: the numbers 0 to 10, and 10 to 20\ny <- sapply(x, mean)\n  # Calculates the mean for each part of the list, which are 5 and 15\nsum(y)\n\n\n[1] 20\n\n\nCode\n  # Sums together the two means, giving 20\n\n\nWith pipes, the above could instead be written as\n\n\nCode\nlist(0:10, 10:20) |>\n  sapply(mean) |>\n  sum()\n\n\n[1] 20\n\n\nor as\n\n\nCode\nlist(0:10, 10:20) %>%\n  sapply(mean) %>%\n  sum()\n\n\n[1] 20\n\n\nAll three arrive at the same answer, which is 20.\nSo far, so good but what is the difference between |> and %>%? The answer is that %>% was developed before |> in the magrittr package, whereas |> is R’s new native pipe. They are often interchangeable but not always.\nAt the moment, the |> pipe is less flexible to use than %>%. Consider the following example. The final two lines of code work fine using %>% to pipe the data frame into the regression model, which is a line of best fit between the x and y values (the function lm() fits a linear model which can be used to predict a y value from a value of x).\n\n\nCode\n1:100 %>%\n  data.frame(x = ., y = 2*. + rnorm(100)) %>%\n  lm(y ~ x, data = .)\n\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n     0.3165       1.9939  \n\n\n(note: the output you get will likely differ from mine because the function rnorm() adds some random variations to the data)\nHowever, it does not work with the pipe, |> because it does not recognise the place holder . that we had previously used to represent what was flowing through the pipe.\n\n\nCode\n# The following code does not work\n1:100 |>\n  data.frame(x = ., y = 2*. + rnorm(100)) |>\n  lm(y ~ x, data = .)\n\n\nTo solve the problem, the above code can be modified by wrapping the regression part in another function but the end result is rather ‘clunky’.\n\n\nCode\n1:100 |>\n  (\\(z) data.frame(x = z, y = 2*z + rnorm(100)))() |>\n  (\\(z) lm(y ~ x, data = z))() \n\n\n\nCall:\nlm(formula = y ~ x, data = z)\n\nCoefficients:\n(Intercept)            x  \n    -0.2703       2.0040  \n\n\nOver time, expect |> to be developed and to supersede %>%. For now, you are unlikely to encounter errors using %>% as a substitute for |> but you might using |> instead of %>%. In other words, %>% is the safer choice if you are unsure, although the |> is faster:\n\n\nCode\ninstall.packages(\"microbenchmark\", dependencies = TRUE)\nrequire(\"microbenchmark\")\nmicrobenchmark(\n  1:100 %>%\n    data.frame(x = ., y = 2*. + rnorm(100)) %>%\n    lm(y ~ x, data = .),\n  1:100 |>\n    (\\(z) data.frame(x = z, y = 2*z + rnorm(100)))() |>\n    (\\(z) lm(y ~ x, data = z))(),\n  times = 100\n)\n\n\n\n\nUnit: microseconds\n                                                                                                   expr\n                       1:100 %>% data.frame(x = ., y = 2 * . + rnorm(100)) %>% lm(y ~      x, data = .)\n (function(z) lm(y ~ x, data = z))((function(z) data.frame(x = z,      y = 2 * z + rnorm(100)))(1:100))\n     min       lq     mean   median       uq      max neval cld\n 244.278 253.0110 301.5222 266.7870 280.2965 3069.629   100   a\n 242.269 253.1955 274.9644 261.8055 278.9435  495.280   100   a"
  },
  {
    "objectID": "tidyverse.html#back-to-the-example",
    "href": "tidyverse.html#back-to-the-example",
    "title": "Tidyverse",
    "section": "Back to the example",
    "text": "Back to the example\nAfter that digression into piping, let’s return to our example that is comparing base R and tidyverse to read-in a table of data, select variables, rename one and, in the following, to calculate the number of COVID-19 cases per English region as a percentage of their estimated populations in the week ending 2021-12-04.\nFirst, in base R:\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)  # Total cases per region\ncases\n\n\n           East Midlands          East of England                   London \n                   25472                    35785                    43060 \n              North East               North West               South East \n                   10796                    31185                    62807 \n              South West            West Midlands Yorkshire and The Humber \n                   33846                    26554                    21079 \n\n\nCode\n  # This step isn't necessary but is included\n  # to show the result of the line above\npopulation <- tapply(df1$population, df1$regionName, sum)\n  # Total population per region\nrate <- round(cases / population * 100, 3)\nrate\n\n\n           East Midlands          East of England                   London \n                   0.524                    0.571                    0.479 \n              North East               North West               South East \n                   0.403                    0.423                    0.681 \n              South West            West Midlands Yorkshire and The Humber \n                   0.598                    0.445                    0.381 \n\n\nNow using tidyverse,\n\n\nCode\nread_csv(\"covid.csv\") |>\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  group_by(regionName) |>\n  summarise(across(where(is.numeric), sum)) |>\n  mutate(rate = round(cases / population * 100, 3)) |>\n  print(n = Inf)\n\n\n# A tibble: 9 × 4\n  regionName               cases population  rate\n  <chr>                    <dbl>      <dbl> <dbl>\n1 East Midlands            25472    4865583 0.524\n2 East of England          35785    6269161 0.571\n3 London                   43060    8991550 0.479\n4 North East               10796    2680763 0.403\n5 North West               31185    7367456 0.423\n6 South East               62807    9217265 0.681\n7 South West               33846    5656917 0.598\n8 West Midlands            26554    5961929 0.445\n9 Yorkshire and The Humber 21079    5526350 0.381\n\n\nEither way produces the same answers but, again, there is an elegance and consistency to the tidyverse way of doing it (which works just fine with the |> pipe) that is, perhaps, missing from base R."
  },
  {
    "objectID": "tidyverse.html#plotting",
    "href": "tidyverse.html#plotting",
    "title": "Tidyverse",
    "section": "Plotting",
    "text": "Plotting\nAs a final step for the comparison, we will extend the code to visualise the regional COVID-19 rates in a histogram, with a rug plot included. A rug plot is a way of preserving the individual data values that would otherwise be ‘lost’ within the bins of a histogram.\nAs previously, we begin with base R,\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)\npopulation <- tapply(df1$population, df1$regionName, sum)\nrate <- round(cases / population * 100, 3)\nhist(rate, xlab = \"rate (cases as % of population)\",\n     main = \"Regional COVID-19 rates: week ending 2021-12-04\")\nrug(rate, lwd = 2)\n\n\n\n\n\n…and continue with tidyverse, creating the output in such a way that it mimics the previous plot.\n\n\nCode\nrequire(ggplot2)\nread_csv(\"covid.csv\") |>\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  group_by(regionName) |>\n  summarise(across(where(is.numeric), sum)) |>\n  mutate(rate = round(cases / population * 100, 3)) -> df2\n\ndf2 |>\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05,\n                   center = -0.025) +\n    geom_rug(linewidth = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nIn this instance, it is the tidyverse code that is the more elaborate. This is partly because there is more customisation of it to mimic the base R plot. However, it is also because it is using the package ggplot2 to produce the histogram. We return to ggplot2 more in later sessions. For now it is sufficient to scan the code and observe how it is ‘layering up’ the various components of the graphic, which those components separated by the + in the lines of code.\n\nThe use of the + notation in ggplot2 operates a little like a pipe in that the outcome of one operation is handed on to the next to modify the graphic being produced. It doesn’t use the pipe because the package’s origins are somewhat older but just think of the + as layering-up – adding to – the graphic.\nI prefer the ggplot2 to the hist() graphics plot but that may be a matter of personal taste. However, ggplot2 can do ‘clever things’ with the visualisation, a hint of which is shown below.\n\n\nCode\ndf2 |>\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05,\n                   center = -0.025) +\n    geom_rug(aes(colour = regionName), size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    scale_colour_discrete(name = \"Region\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank()) \n\n\n\n\n\n Please don’t form that impression that ggplot2 is hard-wired to tidverse and base R to the base graphics. In practice, they are interchangeable.\nHere is an example of using ggplot2 after a sequence of base R commands.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ndf1$rate <- round(df1$cases / df1$population * 100, 3)\nggplot(df1, aes(x = rate, y = regionName)) +\n  geom_boxplot() +\n  labs(x = \"rate (cases as % of population)\",\n       y = \"region\",\n       title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n  theme_minimal()\n\n\n\n\n\nAnd here is an example of using the base R graphic boxplot() after a chain of tidyverse commands.\n\n\nCode\nread_csv(\"covid.csv\") |>\n  select(regionName, `2021-12-04`, `All Ages`) |>\n  rename(cases = `2021-12-04`, population = `All Ages`) |>\n  mutate(rate = round(cases / population * 100, 3)) -> df2\npar(mai=c(0.8,2,0.5,0.5), bty = \"n\", pch = 20)  # See text below\nboxplot(df2$rate ~ df2$regionName, horizontal = TRUE,\n        whisklty = \"solid\", staplelty = 0,\n        col = \"white\", las = 1, cex = 0.9, cex.axis = 0.75,\n        xlab = \"rate (cases as % of population)\", ylab=\"\",\n        main = \"Regional COVID-19 rates: week ending 2021-12-04\")\ntitle(ylab = \"region\", line = 6)\n\n\n\n\n\nI would argue that, in this instance, the base R graphic is as nice as the ggplot2 one but it took more customisation to get it that way and I had to go digging around in the help files, ?boxplot, ?bxp and ?par to find what I needed, which included changing the graphic’s margins (par(mai=...))), moving and changing the size of the text on the vertical axis (the argument cex.axis and the use of the title function), changing the appearance of the ‘whiskers’ (whisklty = \"solid\" and staplelty = 0), and so forth. Still, it does demonstrate that you can have a lot of control over what is produced, if you have the patience and tenacity to do so."
  },
  {
    "objectID": "tidyverse.html#which-is-better",
    "href": "tidyverse.html#which-is-better",
    "title": "Tidyverse",
    "section": "Which is better?",
    "text": "Which is better?\nHaving provided a very small taste of tidyverse and how it differs from base R, we might ask, “which is better?” However, the question is misguided: it is a little like deciding to go to South America and asking whether Spanish or Portuguese is the better language to use. It depends, of course, on what you intend to do and where you intend to travel.\nI use both base R and tidyverse packages in my work, sometimes drifting between the two in rather haphazard ways. If I can get what I want to work then I am happy. Outcomes worry me more than means so, although I use tidyverse a lot, I am not always as tidy as it would want me to be!"
  },
  {
    "objectID": "tidyverse.html#futher-reading",
    "href": "tidyverse.html#futher-reading",
    "title": "Tidyverse",
    "section": "Futher reading",
    "text": "Futher reading\n\nThere is much more to tidyverse than has been covered here. See here for further information about it and its core packages.\nA full introduction to using tidyverse for Data Science is provided by the book R for Data Science (2nd edition) by Hadley Wickham and Garrett Grolemund. There is a free online version."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "program.html#programming-in-r",
    "href": "program.html#programming-in-r",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "program.html#scripts",
    "href": "program.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is essentially the same code that was used previously when answering the question “why R?”, producing maps of ethnic diversity in the English cities of Birmingham, Leicester, London and Manchester. The main difference is that the maps are output to create a .pdf file rather than to screen.\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the document maps.pdf in your Working Directory.\nBe patient whilst the code takes a few moments to run.\nYou can check the document is there with,\n\n\nCode\nfile.exists(\"maps.pdf\")\n\n\n[1] TRUE\n\n\nand you should be able to open it with,\n\n\nCode\nsystem(\"open maps.pdf\")\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety."
  },
  {
    "objectID": "program.html#r-markdown",
    "href": "program.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs, then to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -&gt; New File -&gt; R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet."
  },
  {
    "objectID": "program.html#the-source-code-for-this-document",
    "href": "program.html#the-source-code-for-this-document",
    "title": "Programming",
    "section": "The source code for this document",
    "text": "The source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "program.html#summary",
    "href": "program.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "program.html#further-reading",
    "href": "program.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/. It is a bit advanced for this stage of the course but it is worth noting that there is a cheatsheet available."
  },
  {
    "objectID": "exercise1.html",
    "href": "exercise1.html",
    "title": "Follow-up exercise",
    "section": "",
    "text": "For this short follow-up exercise, create a simple R markdown file (I suggest in HTML output format) that, when knitted, includes the code and output for the following:\n\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/diversity2.csv. The file is in .csv format. It contains headers (i.e. the top row of the data is the variable names).\nTo draw a scatterplot using the following variables in the data: E.01 as the x-axis and E.21 as the y-axis. In the plot, name the x-axis, “Diversity (2001)” and the y-axis “Diversity (2021)”. For your information, E.01 and E.21 measure the ethnic diversity of various places in England and Wales in 2001 and 2021, respectively. The measure ranges from 0 (no diversity) to 1 (‘maximum diversity’).\nTo add a reference line to the plot, with a gradient of 1 and a y-intercept of 0, showing where E.01 = E.21 (no change in diversity),\nTo add a rug plot to the plot on both axes.\n\nRun through the stages above twice in your markdown file, first using base R and the base R graphics, then second using tidyverse and ggplot**. Then add a very brief textual comment beneath your code and output interpreting the graphs in terms of whether places are becoming more ethnically diverse or not. This only needs to be a sentence or two of text, nothing longer."
  },
  {
    "objectID": "exercise1.html#to-do",
    "href": "exercise1.html#to-do",
    "title": "Follow-up exercise",
    "section": "",
    "text": "For this short follow-up exercise, create a simple R markdown file (I suggest in HTML output format) that, when knitted, includes the code and output for the following:\n\nTo read the following file into R: https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/diversity2.csv. The file is in .csv format. It contains headers (i.e. the top row of the data is the variable names).\nTo draw a scatterplot using the following variables in the data: E.01 as the x-axis and E.21 as the y-axis. In the plot, name the x-axis, “Diversity (2001)” and the y-axis “Diversity (2021)”. For your information, E.01 and E.21 measure the ethnic diversity of various places in England and Wales in 2001 and 2021, respectively. The measure ranges from 0 (no diversity) to 1 (‘maximum diversity’).\nTo add a reference line to the plot, with a gradient of 1 and a y-intercept of 0, showing where E.01 = E.21 (no change in diversity),\nTo add a rug plot to the plot on both axes.\n\nRun through the stages above twice in your markdown file, first using base R and the base R graphics, then second using tidyverse and ggplot**. Then add a very brief textual comment beneath your code and output interpreting the graphs in terms of whether places are becoming more ethnically diverse or not. This only needs to be a sentence or two of text, nothing longer."
  },
  {
    "objectID": "exercise1.html#to-help",
    "href": "exercise1.html#to-help",
    "title": "Follow-up exercise",
    "section": "To help",
    "text": "To help\nMost of the code you need to complete this task is included in the ‘Flavours of R’ section of this course. However, you may also find it useful to look at the help files for ?read.csv, ?read_csv, ?plot,?abline and ?rug, the example ggplot2 code available here, here and here, and perhaps also the ggplot2 cheatsheet. You might also look at this introduction to R Markdown but it covers more about R Markdown than you need at this stage. Simply creating a new document using File -&gt; New File -&gt; R Markdown… should be sufficient to get you started."
  },
  {
    "objectID": "themap.html",
    "href": "themap.html",
    "title": "The Spatial Variable",
    "section": "",
    "text": "When we look at a map such as the following, which is a choropleth (or thematic) map showing the percentage of the population with no experience of schooling in each of the South African municipalities in 2011, one thing should be immediately obvious: the areas are shaded in a range of colours; they are not all the same. This is because the values that the colours represent vary across the country with some places having a greater percentage of their population without schooling than others. In this way, the map reveals and also visually represents the spatial (geographic) variation in the variable of interest. The map portrays the geographic pattern. Knowing something about the pattern might provide information about the processes that generated the pattern. At a minimum, it can reveal socio-spatial inequalities in an outcome of interest across a study region.\n\nIt is not surprising to find spatial variation. It is improbable that all the values would be the same. It is nearly always possible to find that some places have lower or higher values than others and to colour the map accordingly. Nevertheless, three characteristics of the spatial variation appear evident in the map.\n\nSpatial heterogeneity. This is the idea that the values typical in one part of the map are not typical in another. To put it simply, some parts of the map are shaded in blue whereas others are in red and those parts seem neither randomly nor regularly distributed because of…\nSpatial clustering. This is the idea that values found in one part of the map tend to be surrounded by similar values in neighbouring parts of the map. In other words, there are patches of blue and patches of red coloured areas on the map – blue tends be near blue and red tends to be near red. Another name for this is positive spatial autocorrelation: values tend to be more similar to nearby other values than they are to distant ones.\n\nEvidence of spatial clustering supports Waldo Tobler’s much cited ‘first law’ of geography: “everything is related to everything else, but near things are more related than distant things.” However, it isn’t really a law because it is by no means always true. If we look at the map, we can also see,\n\nSpatial discontinuities (negative spatial autocorrelation) because sometimes neighbouring places can have very different characteristics – there there can be sharp changes across borders (red next to blue).\n\nNevertheless, Tobler’s ‘law’ does suggest that places tend to be situated within broader spatial contexts such that the processes that both generate and are generated by those contexts have a spatial expression and root\nTaken together, these characteristics of spatial variation indicate spatial dependencies, whereby the measured attributes of one place are not independent of other places. This dependence has statistical consequences if assumptions of independence are violated. Of more substantive geographic interest is how they have arisen – which processes are they caused by or associated with? Why are places not all the same? Why is there a geographical pattern? Complicating the answers to these questions is that what we see in the map is not just a function of underlying social or other processes but also the ways the data are collected and the map constructed. For example, the geographic scale of the data and where the boundaries are drawn between places. This is the Modifiable Areal Unit Problem (MAUP)."
  },
  {
    "objectID": "themap.html#introduction",
    "href": "themap.html#introduction",
    "title": "The Spatial Variable",
    "section": "",
    "text": "When we look at a map such as the following, which is a choropleth (or thematic) map showing the percentage of the population with no experience of schooling in each of the South African municipalities in 2011, one thing should be immediately obvious: the areas are shaded in a range of colours; they are not all the same. This is because the values that the colours represent vary across the country with some places having a greater percentage of their population without schooling than others. In this way, the map reveals and also visually represents the spatial (geographic) variation in the variable of interest. The map portrays the geographic pattern. Knowing something about the pattern might provide information about the processes that generated the pattern. At a minimum, it can reveal socio-spatial inequalities in an outcome of interest across a study region.\n\nIt is not surprising to find spatial variation. It is improbable that all the values would be the same. It is nearly always possible to find that some places have lower or higher values than others and to colour the map accordingly. Nevertheless, three characteristics of the spatial variation appear evident in the map.\n\nSpatial heterogeneity. This is the idea that the values typical in one part of the map are not typical in another. To put it simply, some parts of the map are shaded in blue whereas others are in red and those parts seem neither randomly nor regularly distributed because of…\nSpatial clustering. This is the idea that values found in one part of the map tend to be surrounded by similar values in neighbouring parts of the map. In other words, there are patches of blue and patches of red coloured areas on the map – blue tends be near blue and red tends to be near red. Another name for this is positive spatial autocorrelation: values tend to be more similar to nearby other values than they are to distant ones.\n\nEvidence of spatial clustering supports Waldo Tobler’s much cited ‘first law’ of geography: “everything is related to everything else, but near things are more related than distant things.” However, it isn’t really a law because it is by no means always true. If we look at the map, we can also see,\n\nSpatial discontinuities (negative spatial autocorrelation) because sometimes neighbouring places can have very different characteristics – there there can be sharp changes across borders (red next to blue).\n\nNevertheless, Tobler’s ‘law’ does suggest that places tend to be situated within broader spatial contexts such that the processes that both generate and are generated by those contexts have a spatial expression and root\nTaken together, these characteristics of spatial variation indicate spatial dependencies, whereby the measured attributes of one place are not independent of other places. This dependence has statistical consequences if assumptions of independence are violated. Of more substantive geographic interest is how they have arisen – which processes are they caused by or associated with? Why are places not all the same? Why is there a geographical pattern? Complicating the answers to these questions is that what we see in the map is not just a function of underlying social or other processes but also the ways the data are collected and the map constructed. For example, the geographic scale of the data and where the boundaries are drawn between places. This is the Modifiable Areal Unit Problem (MAUP)."
  },
  {
    "objectID": "themap.html#from-the-map-towards-models",
    "href": "themap.html#from-the-map-towards-models",
    "title": "The Spatial Variable",
    "section": "From the map towards models",
    "text": "From the map towards models\nWith the above questions in mind, we might imagine the map as a first stage in a process of geographical enquiry where what we do is look for and then quantify some of the geographical patterns in the data before beginning to model them and to look for correlates, associations and causes. Here the map is not simply a tool for visualising and communicating data, it is also a tool for exploring data and thinking geographically about them.\n\n\n\n\n\n\n In practice, the process of analysis is likely to involve greater cycling between the various stages. Nevertheless, there is a good argument for starting with the map.\n\n\n\n\n\n\nThe point is that the map serves both as a representation of ‘the spatial variable’ and as a tool for understanding it."
  },
  {
    "objectID": "themap.html#further-reading",
    "href": "themap.html#further-reading",
    "title": "The Spatial Variable",
    "section": "Further Reading",
    "text": "Further Reading\nThe Spatial Variable was the name of the inaugural lecture given by Ron Johnston, one of the most influential geographers of recent times, on his appointment as Professor at the University of Sheffield. A transcript and brief commentary on that lecture is available here and is highly recommended reading."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture 1. Introduction to the course\nLecture 2. Thematic maps in R\n\nThis is a work in progress\nAdditional lectures will be added as they become available."
  },
  {
    "objectID": "thematicmaps.html",
    "href": "thematicmaps.html",
    "title": "Thematic maps in R",
    "section": "",
    "text": "There are lots of ways to produce maps in R. But, however, they are drawn, two things are usually needed to produce a choropleth map of the sort seen in previous sessions:\n\nfirst, some data;\nsecond, a map to join the data to.\n\nThe join is usually made possible by the data and the map containing the same variable; for example, using the same ID codes for all the places in the data as in the map. This implies that what is contained in the data set are measurements of the places shown in the map. Sometimes maps will come pre-bundled with the data of interest so, in effect, the join has already been made.\nOnce we have the data ready to map then R offers plenty of options to produce quick or publication quality maps, which may have either static or dynamic content. The two packages we shall focus on are:\n\nggplot2 (mainly in Part 1) and\ntmap (mainly in Part 2).\n\nHowever, there are others.\nAnother package we shall be using in this session is sf, which provides “support for simple features, a standardized way to encode spatial vector data.” What we are mapping in this exercise are vector data – data for which the geography is ultimately defined by a series of points (two points define a line, a series of lines define a boundary, a boundary demarcates an area). The other common geographic representation used in Geographic Information Systems is raster, which is a grid representation of a study region, which each cell in the grid given one or more values to represent what is found there. For raster data, the stars and raster packages are useful."
  },
  {
    "objectID": "thematicmaps.html#introduction",
    "href": "thematicmaps.html#introduction",
    "title": "Thematic maps in R",
    "section": "",
    "text": "There are lots of ways to produce maps in R. But, however, they are drawn, two things are usually needed to produce a choropleth map of the sort seen in previous sessions:\n\nfirst, some data;\nsecond, a map to join the data to.\n\nThe join is usually made possible by the data and the map containing the same variable; for example, using the same ID codes for all the places in the data as in the map. This implies that what is contained in the data set are measurements of the places shown in the map. Sometimes maps will come pre-bundled with the data of interest so, in effect, the join has already been made.\nOnce we have the data ready to map then R offers plenty of options to produce quick or publication quality maps, which may have either static or dynamic content. The two packages we shall focus on are:\n\nggplot2 (mainly in Part 1) and\ntmap (mainly in Part 2).\n\nHowever, there are others.\nAnother package we shall be using in this session is sf, which provides “support for simple features, a standardized way to encode spatial vector data.” What we are mapping in this exercise are vector data – data for which the geography is ultimately defined by a series of points (two points define a line, a series of lines define a boundary, a boundary demarcates an area). The other common geographic representation used in Geographic Information Systems is raster, which is a grid representation of a study region, which each cell in the grid given one or more values to represent what is found there. For raster data, the stars and raster packages are useful."
  },
  {
    "objectID": "thematicmaps.html#getting-started",
    "href": "thematicmaps.html#getting-started",
    "title": "Thematic maps in R",
    "section": "Getting Started",
    "text": "Getting Started\nAs in previous sessions, if you are keeping all the files and outputs from these exercises together in an R Project (which is a good idea) then open that Project now.\n\nLoad the data\nLet’s begin with the easy bit and load the data, which are from http://superweb.statssa.gov.za. These includes the variable No_schooling which is the percentage of the population without schooling per South African municipality in 2011.\n\n\nCode\n# A quick check to see if the Tidyverse packages are installed...\ninstalled &lt;- installed.packages()[,1]\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\",\n                                                   dependencies = TRUE)\nrequire(tidyverse)\n\n# Read-in the data directly from a web location. The data are in .csv format\neducation &lt;- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/education.csv\")\n# Quick check of the data by looking at the top three rows\nslice_head(education, n = 3)\n\n\n# A tibble: 3 × 8\n  LocalMunicipalityCode LocalMunicipalityName No_schooling Some_primary\n  &lt;chr&gt;                 &lt;chr&gt;                        &lt;dbl&gt;        &lt;dbl&gt;\n1 EC101                 Camdeboo                      13.4         34.5\n2 EC102                 Blue Crane Route              16.0         36.2\n3 EC103                 Ikwezi                        18.4         35.4\n# ℹ 4 more variables: Complete_primary &lt;dbl&gt;, Some_secondary &lt;dbl&gt;,\n#   Grade_12_Std_10 &lt;dbl&gt;, Higher &lt;dbl&gt;\n\n\n\n\nLoading the map\nNext we need a ‘blank map’ of the same South African municipalities that are included in the data above. It is read-in below in geoJSON format but it would not have been unusual if it had been in .shp (shapefile) or .kml format, instead. The source of the data is https://dataportal-mdb-sa.opendata.arcgis.com/. There are several ways of reading this file into R but it is better to use the sf package because older options such as maptools::readShapePoly() (which was for reading shapefiles) or rgdal::readOGR are either deprecated already or in the process of being retired.\n\n\nCode\n# Another quick check to make sure that various libraries have been installed\nif(!(\"proxy\" %in% installed)) install.packages(\"proxy\")\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nrequire(sf)\n\n# Use the read_sf() function from the sf library to read in a digital map of the study area\nmunicipal &lt;- read_sf(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/MDB_Local_Municipal_Boundary_2011.geojson\")\n\n\n If we now look at the top of the municipal object then we find it is of class sf, which is short for simple features. It has a vector geometry (it is of type multipolygon) and has its coordinate reference system (CRS) set as WGS 84. It also contains some attribute data, although not the schooling data we are looking to map.\n\n\nCode\nslice_head(municipal, n = 1)\n\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 27.15761 ymin: -33.28488 xmax: 28.0811 ymax: -32.67573\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 11\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;                 &lt;chr&gt;                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n# ℹ 6 more variables: DistrictMunicipalityCode &lt;chr&gt;,\n#   DistrictMunicipalityName &lt;chr&gt;, Year &lt;int&gt;, Shape__Area &lt;dbl&gt;,\n#   Shape__Length &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nHere are just the attribute data\n\n\nCode\nst_drop_geometry(municipal) |&gt;\n  slice_head(n = 5)\n\n\n# A tibble: 5 × 10\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;                 &lt;chr&gt;                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n2        2 EC           Eastern Cape EC101                 Camdeboo             \n3        3 EC           Eastern Cape EC102                 Blue Crane Route     \n4        4 EC           Eastern Cape EC103                 Ikwezi               \n5        5 EC           Eastern Cape EC104                 Makana               \n# ℹ 5 more variables: DistrictMunicipalityCode &lt;chr&gt;,\n#   DistrictMunicipalityName &lt;chr&gt;, Year &lt;int&gt;, Shape__Area &lt;dbl&gt;,\n#   Shape__Length &lt;dbl&gt;\n\n\nAnd here is the ‘blank’ map, drawn using plot{sf}, which is the ‘base’ way of plotting sf objects.\n\n\nCode\npar(mai=c(0, 0, 0, 0))  # Removes the plot margins\nmunicipal |&gt;\n  st_geometry() |&gt;\n  plot()\n\n\n\n\n\nHad it been necessary to set the coordinate reference system (CRS) then the function st_set_crs() would be used. Instead, and just for fun, we will change the existing CRS: here is the map transformed on to a ‘south up’ coordinate reference system, achieved by changing its EPSG code to 2050 with the function st_transform().\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nmunicipal |&gt;\n  st_transform(2050) |&gt;\n  st_geometry() |&gt;\n  plot()\n\n\n\n\n\n\nNote how functions with the sf library tend to start with st_. Personally, I find this slightly confusing and I am not sure it doesn’t make it harder to find what I looking for in the package’s help pages but it is consistent for the functions and methods that operate on spatial data and is, I believe, short for spatial type.\n\n\nsf and sp\nAt the risk of over-simplification, sf (simple features) can be viewed as a successor to the earlier sp (spatial) and related packages, which are well documented in the book Applied Spatial Data Analysis with R. Sometimes other packages are still reliant on sp and so the spatial objects need to be changed into sp’s native format prior to use.\n\n\nCode\n# From sf to sp\nmunicipal_sp &lt;- as(municipal, \"Spatial\")\nclass(municipal_sp)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nCode\n# From sp to sf\nmunicipal_sf &lt;- st_as_sf(municipal_sp)\nclass(municipal_sf)\n\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "href": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "title": "Thematic maps in R",
    "section": "Joining the attribute data to the map",
    "text": "Joining the attribute data to the map\nIf we look again at the map and schooling data, we find that they have two variables in common which suggests a means to join them together based on a common variable.\n\n\nCode\n# The variables that they appear to have in common...\nintersect(names(municipal), names(education))\n\n\n[1] \"LocalMunicipalityCode\" \"LocalMunicipalityName\"\n\n\nThis is encouraging but, in this example, we need to be careful using the municipal names because not all of those in the map are in the education data or vice versa. Although the variable LocalMunicipalityName is present in both the map and education data, the variable is not actually the same in both because of different ways of spelling the names of places.\n\n\nCode\n# anti_join() returns all rows from x without a match in y\nanti_join(municipal, education, by = \"LocalMunicipalityName\")\n\n\nSimple feature collection with 7 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 27.42423 ymin: -32.05935 xmax: 32.05014 ymax: -24.96652\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 11\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;                 &lt;chr&gt;                \n1       34 EC           Eastern Cape EC157                 King Sabata Dalindye \n2       74 KZN          KwaZulu-Nat… KZN214                uMuziwabantu         \n3       75 KZN          KwaZulu-Nat… KZN215                Ezinqoleni           \n4       97 KZN          KwaZulu-Nat… KZN262                uPhongolo            \n5      146 MP           Mpumalanga   MP301                 Chief Albert Luthuli \n6      149 MP           Mpumalanga   MP304                 Dr Pixley Ka Isaka S \n7      165 NW           North West   NW372                 Local Municipality o \n# ℹ 6 more variables: DistrictMunicipalityCode &lt;chr&gt;,\n#   DistrictMunicipalityName &lt;chr&gt;, Year &lt;int&gt;, Shape__Area &lt;dbl&gt;,\n#   Shape__Length &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nCode\nanti_join(education, municipal, by = \"LocalMunicipalityName\")\n\n\n# A tibble: 7 × 8\n  LocalMunicipalityCode LocalMunicipalityName  No_schooling Some_primary\n  &lt;chr&gt;                 &lt;chr&gt;                         &lt;dbl&gt;        &lt;dbl&gt;\n1 EC157                 King Sabata Dalindyebo         21.7         33.2\n2 KZN214                UMuziwabantu                   20.7         42.1\n3 KZN215                Ezingoleni                     21.9         40.1\n4 KZN262                UPhongolo                      22.5         35.1\n5 MP301                 Albert Luthuli                 23.8         31.1\n6 MP304                 Pixley Ka Seme                 24.0         31.9\n7 NW372                 Madibeng                       12.8         27.1\n# ℹ 4 more variables: Complete_primary &lt;dbl&gt;, Some_secondary &lt;dbl&gt;,\n#   Grade_12_Std_10 &lt;dbl&gt;, Higher &lt;dbl&gt;\n\n\nFortunately, the municipal codes are consistent even where the names are not, which is why place codes and IDs are generally better than using names when joining data.\n\n\nCode\n# The municipality codes are consistent in the map and data; there are none that do not match.=\nanti_join(municipal, education, by = \"LocalMunicipalityCode\")\n\n\nSimple feature collection with 0 features and 10 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 11\n# ℹ 11 variables: OBJECTID &lt;int&gt;, ProvinceCode &lt;chr&gt;, ProvinceName &lt;chr&gt;,\n#   LocalMunicipalityCode &lt;chr&gt;, LocalMunicipalityName &lt;chr&gt;,\n#   DistrictMunicipalityCode &lt;chr&gt;, DistrictMunicipalityName &lt;chr&gt;, Year &lt;int&gt;,\n#   Shape__Area &lt;dbl&gt;, Shape__Length &lt;dbl&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nCode\nanti_join(education, municipal, by = \"LocalMunicipalityCode\")\n\n\n# A tibble: 0 × 8\n# ℹ 8 variables: LocalMunicipalityCode &lt;chr&gt;, LocalMunicipalityName &lt;chr&gt;,\n#   No_schooling &lt;dbl&gt;, Some_primary &lt;dbl&gt;, Complete_primary &lt;dbl&gt;,\n#   Some_secondary &lt;dbl&gt;, Grade_12_Std_10 &lt;dbl&gt;, Higher &lt;dbl&gt;\n\n\nWe therefore join the data to the map using the variable LocalMunicipalityCode and check that the schooling data are now attached to the map.\n\n\nCode\nmunicipal &lt;- left_join(municipal, education, by = \"LocalMunicipalityCode\")\nnames(municipal)\n\n\n [1] \"OBJECTID\"                 \"ProvinceCode\"            \n [3] \"ProvinceName\"             \"LocalMunicipalityCode\"   \n [5] \"LocalMunicipalityName.x\"  \"DistrictMunicipalityCode\"\n [7] \"DistrictMunicipalityName\" \"Year\"                    \n [9] \"Shape__Area\"              \"Shape__Length\"           \n[11] \"geometry\"                 \"LocalMunicipalityName.y\" \n[13] \"No_schooling\"             \"Some_primary\"            \n[15] \"Complete_primary\"         \"Some_secondary\"          \n[17] \"Grade_12_Std_10\"          \"Higher\"                  \n\n\n\nNote that the variables LocalMunicipalityName.x and LocalMunicipalityName.y have been created in the process of the join. This is because we did not use LocalMunicipalityName for the join but that variable name is present in both the map and the data and is therefore duplicated when they are joined together – municipal$LocalMunicipalityName becomes municipal$LocalMunicipalityName.x and education$LocalMunicipalityName creates municipal$LocalMunicipalityName.y."
  },
  {
    "objectID": "thematicmaps.html#mapping-the-data",
    "href": "thematicmaps.html#mapping-the-data",
    "title": "Thematic maps in R",
    "section": "Mapping the data",
    "text": "Mapping the data\n\nUsing plot{sf}\nThe ‘one line’ way of plotting the data is to use the in-built plot() function for sf.\n\n\nCode\nplot(municipal[\"No_schooling\"])\n\n\n\n\n\nAs a ‘rough and ready’ way to check for spatial variation and patterns in the data, it is quick and easy. It is important to specify the variable(s) you wish to include in the plot or else it will plot them all up to the value specified by the argument max.plot, which has a default of nine.\n\n\nCode\nplot(municipal)\n\n\n\n\n\n The map can be customised. For example,\n\n\nCode\nif(!(\"RColorBrewer\" %in% installed)) install.packages(\"RColorBrewer\",\n                                                      dependencies = TRUE)\nrequire(RColorBrewer)\n\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"jenks\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\nHave a read through the documentation at ?sf::plot to get a sense of what the various arguments do. Try changing them and see if you can produce a map with six equal interval breaks, for example.\nNote the use of the RColorBrewer package and its brewer.pal() function. RColorBrewer provides colour palettes based on https://colorbrewer2.org/ and has been used to create a diverging red-yellow-blue colour palette that is reversed using the function rev() so that red is assigned to the highest values, not lowest. A ‘natural breaks’ (jenks) classification with 7 colours has been used (breaks = \"jenks\", nbreaks = 7).\n\n\nThinking about the map classes\nHere is the same underlying map but with equal interval breaks instead:\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"equal\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\n… and here with quantile breaks:\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"quantile\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"% of Population with No Schooling\")\n\n\n\n\n\nClearly the maps above do not appear exactly the same. This because the geographical patterns and therefore the geographical information that we view in the map are a function of how the map is constructed, including the number, colouring and widths (ranges) of the map classes. Ideally, these should be set to reflect the distribution of the data and what is being look for in it.\nThe following histograms show the break points in the distributions used in the various maps. The code works by creating a list of plots (specifically, a list of ggplots, see below) – one plot each for the jenks, equal and quantile styles – and then, using a package called gridExtra, to arrange those plots into a single grid. However, the code matters less than what it reveals, which is that Jenks or other ‘natural breaks’ classifications are reasonably good for identifying break points that reflect the distribution of the data in the absence of the user having cause to set those break points in some other way.\nThinking about the distribution of the data is important to avoid creating maps that give the wrong impression of the prevalence or otherwise of values in the data. For example, if you use a quantile classification with, say, 4 breaks, it will create four map classes, each containing approximately one quarter of the data, even if some of the values in those classes are rarely found and unusual when compared to others in the same class. An equal interval classification will not have this problem.\n\n\nCode\nif(!(\"gridExtra\" %in% installed)) install.packages(\"gridExtra\",\n                                                   dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"classInt\",\n                                                  dependencies = TRUE)\n\nrequire(gridExtra)\nrequire(classInt)\n\nstyles &lt;- c(\"jenks\", \"equal\", \"quantile\")\ng &lt;- lapply(styles, \\(x) {\n        ggplot(municipal, aes(x = No_schooling)) +\n        geom_histogram(fill = \"light grey\") +\n        xlab(\"% of Population with No Schooling\") +\n        geom_vline(xintercept = classIntervals(municipal$No_schooling,\n                                               n = 7, style = x)$brks,\n                   col = \"dark red\") +\n        geom_rug() +\n        theme_minimal() +\n        ggtitle(paste(x,\"classification\"))\n    })\n# The step below brings together, in a grid, the list of three different plots\n# created above\ngrid.arrange(grobs = g)\n\n\n\n\n\n For further information on using plot{sf} see here and look at the help menu, ?sf::plot."
  },
  {
    "objectID": "thematicmaps.html#using-ggplot2",
    "href": "thematicmaps.html#using-ggplot2",
    "title": "Thematic maps in R",
    "section": "Using ggplot2",
    "text": "Using ggplot2\nWhilst the plot() function for sf objects is useful for producing quick maps, I tend to prefer ggplot2 for better quality ones that I am wanting to customise or annotate in particular ways. We already have seen examples of ggplot2 output in earlier sessions and also in the histograms above.\nggplot2 is based on The Grammar of Graphics. I find it easiest to think of it, initially, in four stages:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\nAs an example, in the code chunk below, those four stages are applied to a boxplot showing the distribution of the no schooling variable by South African Provinces.\nFirst, the data = municipal. Second, consulting with the ggplot2 cheatsheet, I find that the aesthetics, aes(), for the boxplot, require a discrete x and a continuous y, which are provided by ProvinceName and No_schooling, respectively. ProvinceName has also been used to assign a fill colour to each box. Third, the optional changes arise from me preferring theme_minimal() to the default style, although I have then modified it to remove the legend, change the angle of the text on the x-axis, remove the x-axis label and change the y-axis label.\n\n\nCode\nrequire(ggplot2)\nggplot(data = municipal, aes(x = ProvinceName, y = No_schooling,\n                             fill = ProvinceName)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) +\n  xlab(element_blank()) +\n  ylab(\"% No schooling within municipalities\")\n\n\n\n\n\n Let’s now take that process and apply it to create a map, using the same RColorBrewer colour palette as previously and adding the map using geom_sf (for a full list of geoms available for ggplot2 see here). The line scale_fill_distiller is an easy way to shade the map using the colour palette from RColorBrewer andlabs() adds labelling.\n\n\nCode\nggplot(municipal, aes(fill = No_schooling)) +\n  geom_sf() +\n  scale_fill_distiller(\"%\", palette = \"RdYlBu\") +\n  theme_minimal() +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )  \n\n\n\n\n\n Presently the map has a continuous shading scheme – you can see it in the map’s legend. This can be changed to discrete map classes and colours by converting the continuous municipal$No_schooling variable to a factor, using the cut() function, here with break points found using ClassIntervals(style = \"jenks\"). Because we have then changed from continuous to discrete (categorised) data but still want to use an RColorBrewer palette, so scale_fill_brewer() replaces scale_fill_distiller(), wherein the argument direction = -1 reverses the RdYlBu palette so that the highest values are coloured red. Adding guides(fill = guide_legend(reverse = TRUE)) reverses the legend so that the highest values are on top in the legend, which is another preference of mine.\n\n\nCode\n# Find the break points in the distribution using a Jenks classification\nbrks &lt;- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\n\n# Factor the No_schooling variable using those break points\nmunicipal$No_schooling_gp &lt;- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\n\nggplot(municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nAt this point, we may note that the four stages of the map production that I referred to earlier was an over-simplification and we can add a fifth:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart – geom_sf for mapping;\n‘Scale’ the data – in the above examples, link the mapped variables to map classes and colour codes. The scaling is what scale_fill_... was doing;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\n\nAnnotating the map with ggspatial\nHaving created the basic map using ggplot2, we can add some additional map elements using ggspatial, which provides some extra cartographic functions. The following code chunk adds a backdrop to the map. Different backgrounds (alternative map tiles) can be chosen from the list at rosm::osm.types(); see here for what they look like.\nBefore running the code, we may note a change from the previous code chunk (above) which is in addition to installing and requiring ggspatial and adding the map tile as a backdrop. Specifically, if you look at the previous code chunk you will find that the data, municipal are handed-to ggplot in the top line ggplot(municipal, ...) where the first argument is the data one. In other words, ggplot(municipal, ...) is the same as, ggplot(data = municipal, ...) (check the help file, ?ggplot2::ggplot to confirm this). By specifying the data in the top line, in essence this sets data = municipal as a global parameter for the plot: it is where ggplot will, by default, now look for the variable called for in aes(fill = No_schooling_gp) and where it will look for other variables too. Whilst this would work just fine in the code immediately below, a little later I introduce a second geom_sf into the plot and no longer want municipal to be the default choice for all the aesthetics of the chart, just some of them. To pre-empt any problems that might otherwise arise, I no longer specify municipal as the default dataset in my ggplot() but, instead, specifically name municipal where I want to use it – as the fill data, in the line geom_sf(data = municipal, aes(fill = No_schooling_gp)) – which will leave me free to associate other data with different aesthetics in due course. More simply, if you set the data = argument and also any aesthetics, mapping = aes() then you are basically saying “use these for everything that follows” unless you override them by saying what you want for each specific geom.\n\n\nCode\nif(!(\"ggspatial\" %in% installed)) install.packages(\"ggspatial\",\n                                                   dependencies = TRUE)\nrequire(ggspatial)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n A north arrow and a scale bar can also be added, although including the scale bar generates a warning because the map to true life distance ratio is not actually constant across the map but varies with longitude and latitude. The argument, location = \"tl\" is short for top left; location = \"br\" for bottom right. See ?annotation_north_arrow and ?annotation_scale for further details and options. Note also the use of the last_plot() function to more easily add content to the last ggplot.\n\n\nCode\nlast_plot() +\n  annotation_north_arrow(location = \"tl\",\n                         style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\n\n\n\n\n\n\nAdding point symbols to the map\nIn the next example, the locations of South African cities are added to the map, with a symbol drawn in proportion to their population size. The source of the data is a shapefile from https://data.humdata.org/dataset/hotosm_zaf_populated_places. The symbol shape that is specified by pch = 3 has the same numeric coding as those in ?graphics::points (i.e. 0 is a square, 1, is a circle, 2 is a triangle, and so forth).\n\nThe function scales::label_comma() forces decimal display of numbers to avoid displaying scientific notation.\n\nThe notation [pagkage_name]::[function] is a way of saying in which library/package a specific function is found - the label_comma() function is in the scales library. Sometimes this can be useful to run a function from a package/library without first requiring (loading) it. It can also be useful to avoid the problem when two packages contain a function with the same name. For example, there is a select() function in both the ’raster and dplyr libraries. If you require (load) both of these libraries then the select function in whichever is loaded second will mask the select function in the one loaded first, potentially causing errors or confusions. A way around the problem is to use the :: notation to be specific about which package’s select you wish to use, i.e. raster::select() or dplyr::select().\n\n\nCode\nif(!(\"scales\" %in% installed)) install.packages(\"scales\", dependencies = TRUE)\n\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\nunzip(\"cities.zip\")\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |&gt;\n  filter(place == \"city\") |&gt;\n  mutate(population = as.numeric(population)) -&gt;\n  cities\n\nlast_plot() +\n  geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma())\n\n\n\n\n\n\nSlightly confusingly, a shapefile actually consists of at least three files, one with the extension .shp (the coordinate/shape data), one .shx (an index file) and one .dbf (the attribute data). If you use a shapefile you need to make sure you download all of them and keep them together in the same folder.\n\n\nLabelling using ggsflabel\nNice labelling of the cities is provided by ggsflabel, in this example using a pipe |&gt; to filter and only label cities with over a million population. The function geom_sf_label_repel() is designed to stop labels from being placed over each other. We could use last_plot() again to create this map but, instead, here is the code in full:\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n    geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma()) +\n  geom_sf_label_repel(data = cities |&gt; filter(population &gt; 1e6),\n                    aes(label = name), alpha = 0.7, size = 3) +\n  theme_minimal() +\n  # The line below removes some annoying axis titles that otherwise appear\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n\n\nSaving the map as a graphic file\nHaving created the map, it can now be saved as a graphic. If you are not using an R Project to save your files into, you may wish to change your working directory before saving the graphic, using setwd(dir) and substituting dir with the pathname to the preferred directory, or by using Session -&gt; Set Working Directory -&gt; Choose Directory from the dropdown menus. Once you have done so, the last_plot() is easily saved using the function ggsave(). For example, in .pdf format, to a print quality,\n\n\nCode\nggsave(\"no_schooling.pdf\", device = \"pdf\", width = 6, units = \"in\",\n       dpi = \"print\")\n\n\nAlternatively, we can write directly to a graphics device, using one of the functions bmp(), jpeg(), png(), tiff() or pdf(). For instance,\n\n\nCode\njpeg(\"no_schooling.jpg\", res = 72)\nlast_plot()\ndev.off()"
  },
  {
    "objectID": "thematicmaps.html#creating-an-interactive-map-using-ggiraph",
    "href": "thematicmaps.html#creating-an-interactive-map-using-ggiraph",
    "title": "Thematic maps in R",
    "section": "Creating an interactive map using ggiraph",
    "text": "Creating an interactive map using ggiraph\nSo far all the maps we have created have been static. This is obviously better for anything that will be printed but, for a website or similar, we may wish to include more ‘interaction’. The package ggiraph package creates dynamic ggplot2 graphs and we can use it to create an interactive map where information about the areas appears as we brush over those areas on the map with the mouse pointer. This is achieved by replacing, in the code, geom_sf() with the geom_sf_interactive() function from ggiraph, specifying the text to show with the tooltip (the example below pastes a number of character elements together without a space between them, hence paste0() but does include a carriage return, \\n) and rendering the resulting ggplot2 object with girafe().\n\n\nCode\nif(!(\"ggiraph\" %in% installed)) install.packages(\"ggiraph\", dependencies = TRUE)\nrequire(ggiraph)\n\ng &lt;- ggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf_interactive(data = municipal,\n                      aes(tooltip = paste0(LocalMunicipalityName.x, \"\\n\",\n                                           round(No_schooling,1), \"%\"),\n                             fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) +\n  annotation_north_arrow(location = \"tl\",\n                         style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\ngirafe(ggobj = g)\n\n\n\n\n\n\n\nSave your workspace\nThis is the end of Part 1 and may be a good place to stop or, at least, take a break. If you won’t be continuing immediately to Part 2 then don’t forget to save your workspace. For example, by using,\n\n\nCode\nsave.image(\"making_maps.RData\")"
  },
  {
    "objectID": "thematicmaps.html#bivarite-mapping-with-ggplot2",
    "href": "thematicmaps.html#bivarite-mapping-with-ggplot2",
    "title": "Thematic maps in R",
    "section": "Bivarite Mapping with ggplot2",
    "text": "Bivarite Mapping with ggplot2\nSometimes we want to the colours on the map to reflect the combination of two variables’ values, not just one. We used to be able to use the bivariate package to do this. Unfortunately, this does not appear to be available any more and whilst it is possible to download achieved versions, the most recent was not working for me and could create conflicts with newer packages. We can, however, undertake the process ‘by hand’.\nTo illustrate, let’s use the two variables, municipal$No_schooling and municipal$Higher. One measures the percentage of the population without schooling per South African municipality in 2011 and we shall use it to create a new variable, the percentage with schooling; the other is the percentage with higher education. The following code creates the bivariate map. The process is broadly that described in this tutorial but I have changed and simplified it a little. It may not look especially simple but there is not much to it that we have not done already. The main difference is that we are cutting (categorising) the data along two variables and then creating a colour scheme and a legend to represent the resulting groups.\n\n\nCode\n# First, split the municipalities into three groups (categories) based on the % Schooling\ntertiles_schooling &lt;- municipal |&gt;\n  # Create the new schooling variable from no_schooling:\n  mutate(Schooling = 100 - No_schooling) |&gt;\n  # Take the Schooling variable and cut it into the groups:\n  pull(Schooling) %&gt;%\n  cut(., breaks = quantile(., probs = seq(0, 1, length.out = 4)),\n                           labels = FALSE,\n                           include.lowest = TRUE)\n\n# Second, split the municipalities into three groups based on the % higher education\ntertiles_higher &lt;- municipal |&gt;\n  pull(Higher) %&gt;%\n  cut(., breaks = quantile(., probs = seq(0, 1, length.out = 4)),\n                           labels = FALSE,\n                           include.lowest = TRUE)\n\n# Third, add those groups to the map's attribute data and also combine them\nmunicipal$schooling_gp  &lt;- tertiles_schooling\nmunicipal$higher_gp  &lt;- tertiles_higher\nmunicipal$combined_gp &lt;- paste(tertiles_schooling, tertiles_higher, sep = \" - \")\n\n# Here is what has been created\nmunicipal |&gt; \n  st_drop_geometry() |&gt;\n  select(schooling_gp, higher_gp, combined_gp) |&gt;\n  slice_head(n = 5)\n\n\n# A tibble: 5 × 3\n  schooling_gp higher_gp combined_gp\n         &lt;int&gt;     &lt;int&gt; &lt;chr&gt;      \n1            3         3 3 - 3      \n2            3         3 3 - 3      \n3            2         2 2 - 2      \n4            2         2 2 - 2      \n5            3         3 3 - 3      \n\n\nCode\n# Fourth, define a colour scheme, here using hex colour codes\n# https://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/ is helpful\ncols &lt;- c(\n  \"3 - 3\" = \"#e8e8e8\", # highest primary and secondary\n  \"2 - 3\" = \"#cbb8d7\",\n  \"1 - 3\" = \"#9972af\", # lowest primary, highest secondary\n  \"3 - 2\" = \"#e4d9ac\",\n  \"2 - 2\" = \"#c8ada0\", # medium primary, medium secondary\n  \"1 - 2\" = \"#976b82\",\n  \"3 - 1\" = \"#c8b35a\", # highest primary, lowest secondary\n  \"2 - 1\" = \"#8f8e53\",\n  \"1 - 1\" = \"#804d36\" # low primary, lowest secondary\n)\n\n# Now create the map, omitting the legend\nmap &lt;- ggplot() +\n  geom_sf(data = municipal, aes(fill = combined_gp)) +\n  scale_fill_manual(values = cols, guide = FALSE) +\n  geom_sf(data = cities, pch = 21, bg = \"light grey\") +\n  geom_sf_label_repel(data = cities |&gt; filter(population &gt; 0.25e6),\n                    aes(label = name), alpha = 0.7, size = 3) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Levels of education in South African municipalities\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) +\n  annotation_north_arrow(location = \"tl\",\n                         style = north_arrow_minimal(text_size = 14))\n\n# Create the map's legend\n# It should be more obvious what this does once it is plotted\nlegend &lt;- ggplot() +\n  geom_tile(\n    data = municipal,\n    mapping = aes(\n      x = schooling_gp,\n      y = higher_gp,\n      fill = combined_gp)) +\n  scale_fill_manual(values = cols, guide = FALSE) +\n  labs(x = \"← ← Lower % Schooling\",\n       y = \"← ← Lower % HE\") +\n  coord_fixed() +\n  theme(axis.title = element_text(size = 7.5),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_blank())\n\nif(!(\"cowplot\" %in% installed)) install.packages(\"ggiraph\", dependencies = TRUE)\nrequire(cowplot)\n\n# Use the ggdraw() function in cowplot to combine the map and legend together\nggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.75, 0.1, 0.25, 0.25)"
  },
  {
    "objectID": "thematicmaps.html#using-tmap",
    "href": "thematicmaps.html#using-tmap",
    "title": "Thematic maps in R",
    "section": "Using tmap",
    "text": "Using tmap\nClearly there is a lot of scope to produce high quality maps using ggplot2 and various associated packages. However, it has a rival, in tmap, which is arguably easier to use. Like ggplot2, tmap adopts the Grammar of Graphics but approaches it in a slightly different way that uses layers: it builds-up the layers of the graphic by first specifying a spatial object or background, then doing things to the map based on it, then specifying another spatial object and/or other map elements to do things with, and so forth. The types of layers available can be viewed here and here. A brief introduction to tmap is available here.\nThe code chunk a little further below builds-up the layers of the map to produce one quite like that previously created in ggplot2. First, however, there is a error in the geometry of the underlying municipal map file to deal with:\n\n\nCode\nall(st_is_valid(municipal))\n\n\n[1] FALSE\n\n\nThe problem lies in the 128th area, where one edge crosses another,\n\n\nCode\nst_is_valid(municipal[128,], reason = TRUE)\n\n\n[1] \"Edge 25 crosses edge 27\"\n\n\ntmap is less forgiving of this error than ggplot2 is. A temporary ‘fix’ – more of a side-step really – is achieved by changing the coordinate reference system, which presently is using EPSG: 4326 (you can see this with st_crs(municipal)), to EPSG: 3857.\n\n\nCode\nmunicipal &lt;- st_transform(municipal, 3857)\nall(st_is_valid(municipal))\n\n\n[1] TRUE\n\n\nNow we can produce the plot, to give an output similar to one of the earlier ggplots:\n\n\nCode\nif(!(\"tmap\" %in% installed)) install.packages(\"tmap\", dependencies = TRUE)\nrequire(tmap)\n\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal, is.master = TRUE) +\n  tm_fill(\"No_schooling\", palette = \"-RdYlBu\", title = \"%\", style = \"jenks\",\n          n = 7) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities |&gt; filter(population &gt; 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\n\n\n\n\n The map looks pretty good and can be saved using the function tmap_save. For example,\n\n\nCode\ntmap_save(tmap_last(), \"no_schooling2.jpg\", width = 7, units = \"in\")\n\n\nHowever, there is a cartographic/mathematical irritation that might bug a reviewer if you were to submit the map as part of an academic journal (or a marker if you were to submit the map for assessment!). If you look at the map classes, they are non-unique: e.g. 5.64 to 9.87, 9.87 to 13.38, 13.38 to 17.19, and so forth. Which category would a value of 9.87 (or 13.38, etc.) fall into?\nTo solve this problem, we can do what we did for the ggplots, which is to create a factor from the municipal$No_schooling variable, which is what the first two lines of code below do. The third line reverses the order of the factors, so that the highest and not lowest valued group is treated as the first level, and so forth. The reason I have added this is because of my preference for the highest values to appear top in the legend.\n\n\nCode\nbrks &lt;- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp &lt;- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\nmunicipal$No_schooling_gp &lt;- factor(municipal$No_schooling_gp,\n                                levels = rev(levels(municipal$No_schooling_gp)))\n\ntm_graticules(col = \"lightgrey\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %&gt;% filter(population &gt; 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\n\n\n\n\n Where tmap really excels is in rendering interactive maps to leaflet by changing the tmap_mode from tmap_mode(\"plot\")to tmap_mode(\"view\"). The following allows panning and can be zoomed in and out of. Unfortunately, it also reveals that the earlier ‘fix’ to the municipal object doesn’t work here so I have omitted the problem area, although that isn’t much of a solution.\n\n\nCode\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap.HOT\") +\n  tm_shape(municipal[-128,], name = \"municipalities\") +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\") +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\")\n\n\n\n\n\n\n\nThe following version adds further functionality. It allows different map layers to be displayed (move your mouse cursor over the map layers icon to do so) and, if you right click on any of the areas shown, will bring-up information about them.\n\n\nCode\ntm_basemap(c(OSM = \"OpenStreetMap\",\n             OSMHot = \"OpenStreetMap.HOT\",\n             Carto = \"CartoDB\")) +\n  tm_shape(municipal[-128,], name = \"municipalities\") +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\",\n          id = \"LocalMunicipalityName.x\",\n          popup.vars = c(\"% No schooling:\" = \"No_schooling\",\n                         \"Province: \" = \"ProvinceName\"),\n          popup.format = list(digits = 1)) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\",\n          id = \"name\",\n          popup.vars = c(\"Population: \" = \"population\")) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_view(view.legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n \nDifferent layers are available dependent upon the tmap_mode. For example, there is no straightforward way of adding a maptile (tm_basemap) as the backdrop to a map in tmap’s \"plot\" mode, or a compass (tm_compass) or a scale bar (tm_scale_bar) in \"view\" mode.\n We can also have some fun! Here is an animated map where the animation is produced from a combination of the tm_facets(along = \"ProvinceName\", free.coords = FALSE) layer and the use of the tmap_animation() function. Notice how I add municipal to the map twice, as two separate layers. The first is to provide a general backdrop to the map with all the municipalities shaded grey. They second is linked to the animation with the municipalities shaded by the percentage of their population without schooling.\n\n\nCode\nif(!(\"gifski\" %in% installed)) install.packages(\"gifski\", dependencies = TRUE)\n\ntmap_mode(\"plot\")\n\nt &lt;- tm_graticules(col = \"light grey\") +\n  tm_shape(municipal) +\n  tm_polygons(col = \"grey\", border.col = \"black\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"white\") +\n  tm_facets(along = \"ProvinceName\", free.coords = FALSE) +\n  tm_legend(title = \"Percentage of Population with No Schooling\",\n            bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\",\n             bg.color = \"white\")\n\ntmap_animation(t, delay = 100)\n\n\n  \nThe animation may be saved as a .gif file by including the argument filename (see ?tmap_animation).\n Faceting can also be used on static maps, as in the following example, where tm_facets(by = \"ProvinceName\", free.coords = TRUE) creates a choropleth map for each Province, with a common legend, positioned outside each provincial map through the use of tm_layout(legend.outside.position = \"bottom\").\n\n\nCode\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\",\n          title = \"% Population with No Schooling\",\n          legend.is.portrait = FALSE) +\n  tm_borders(col = \"white\") +\n  tm_facets(by = \"ProvinceName\", free.coords = TRUE) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.outside.position = \"bottom\")"
  },
  {
    "objectID": "thematicmaps.html#geofacets",
    "href": "thematicmaps.html#geofacets",
    "title": "Thematic maps in R",
    "section": "Geofacets",
    "text": "Geofacets\nTo this point of the session we have been using maps to represent the spatial distribution of one or more variables whose values are plotted as an aesthetic of the map such as colour or shape size. A different approach is offered by the geofacet package which uses geography as a ‘placeholder’ to position graphical summaries of data for different parts of the map. The idea is to flexibly visualise data for different geographical regions by providing a ggplot2 faceting function facet_geo() that works just like ggplot2’s built-in faceting, except that the resulting arrangement of panels follows a grid that mimics the original geographic topology as closely as possible.\nHere is an example of it in use, showing the distribution of municipalities within South African provinces in terms of the percentage of the population with higher education (university) qualifications.\n\n\nCode\nif(!(\"geofacet\" %in% installed)) install.packages(\"geofacet\")\nrequire(geofacet)\n\n# Define a grid that mimics the geographical distribution of the provinces\nmygrid &lt;- data.frame(\n  code = c(\"LIM\", \"GT\", \"NW\", \"MP\", \"NC\", \"FS\", \"KZN\", \"EC\", \"WC\"),\n  name = c(\"Limpopo\", \"Gauteng\", \"North West\", \"Mpumalanga\", \"Northern Cape\",\n           \"Free State\", \"KwaZulu-Natal\", \"Eastern Cape\", \"Western Cape\"),\n  row = c(1, 2, 2, 2, 3, 3, 3, 4, 4),\n  col = c(3, 3, 2, 4, 1, 2, 3, 2, 1),\n  stringsAsFactors = FALSE\n)\n\n# Plot the data with the geofaceting\nggplot(municipal, aes(Higher)) +\n  geom_boxplot(col = \"dark grey\") +\n  geom_density() +\n  geom_rug() +\n  facet_geo(~ ProvinceName, grid = mygrid) +\n  scale_y_continuous(breaks = c(0, 0.2, 0.4)) +\n  theme_bw() +\n  labs(\n    title = \"Percentage of Population with higher education\",\n    caption = \"Source: 2011 Census / Statistics South Africa\"\n  ) +\n  xlab(\"% per municipality\")"
  },
  {
    "objectID": "thematicmaps.html#saving-the-map-and-attribute-data",
    "href": "thematicmaps.html#saving-the-map-and-attribute-data",
    "title": "Thematic maps in R",
    "section": "Saving the map and attribute data",
    "text": "Saving the map and attribute data\nThat’s almost it for now! However, before finishing, we will save the map with the joined attribute data to the working directory as an R object.\n\n\nCode\nsave(municipal, file = \"municipal.RData\")"
  },
  {
    "objectID": "thematicmaps.html#summary",
    "href": "thematicmaps.html#summary",
    "title": "Thematic maps in R",
    "section": "Summary",
    "text": "Summary\nThis session has demonstrated that R is a powerful tool for drawing publication quality maps. The native plot functions for sf objects are useful as a quick way to draw a map and both ggplot2 and tmap offer a range of functionality to customise their cartographic outputs to produce really nice looking maps. I tend to use ggplot2 but that is really more out of habit than anything else as tmap might actually be the easier to use. It depends a bit on whether I am drawing static maps (usually in ggplot2) or interactive ones (probably better in tmap). There are other packages available, too, including mapview, which the following code chunk uses (see also, here), and Leaflet to R. Perhaps the key take-home point is that these maps can look at lot better than those produced by some conventional GIS and have the advantage that they can be linked to other analytically processes in R, as future sessions will demonstrate.\n\n\nCode\nif(!(\"mapview\" %in% installed)) install.packages(\"mapview\", dependencies = TRUE)\nrequire(mapview)\n\nmapview(municipal %&gt;% mutate(No_schooling = round(No_schooling, 1)), \n        zcol = \"No_schooling\",\n        layer.name = \"% No Schooling\",\n        map.types = \"OpenStreetMap.HOT\",\n        col.regions = colorRampPalette(rev(brewer.pal(9, \"RdYlBu\"))))"
  },
  {
    "objectID": "thematicmaps.html#further-reading",
    "href": "thematicmaps.html#further-reading",
    "title": "Thematic maps in R",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 2 on Spatial data and R packages for mapping from Geospatial Health Data by Paula Morga.\n\nChapter 9 on Making maps with R from Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow.\n\nChapter 8 on Plotting spatial data from Spatial Data Science with Applications in R by Edzer Pebesma and Roger Bivand.\nSee also: Elegant and informative maps with tmap, which is a work in progress by Martijn Tennekes and Jakub Nowosad."
  },
  {
    "objectID": "thematicmaps.html#maps-in-shiny",
    "href": "thematicmaps.html#maps-in-shiny",
    "title": "Thematic maps in R",
    "section": "Maps in Shiny",
    "text": "Maps in Shiny\nShiny is described as “an open source R package that provides an elegant and powerful web framework for building web applications using R” [or Python]. At the time of writing, there is a nice example of a mapping application on the Shiny homepage.\nTeaching Shiny in detail is beyond the scope of this course but there are Getting Started guides (for both R and Python) and a gallery of applications which can be viewed here.\nThe following code creates an app that allows variables from the South African municipality data to be selected and mapped, with various choices for the maps classes and colour palette. Don’t worry if not all of the code makes sense to you but hopefully you will recognise some parts from this session and will see that the basis of the app is to define a user interface and then to define tasks on the server side that will take some inputs from selections made by the user in the interface.\n\n\nCode\n# This checks that the required packages are installed and then loaded\ninstalled &lt;- installed.packages()[,1]\npkgs &lt;- c(\"shiny\", \"sf\", \"tidyverse\", \"ggplot2\", \"sf\", \"classInt\", \"RColorBrewer\",\n          \"ggspatial\")\ninstall &lt;- pkgs[!(pkgs %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE)\ninvisible(lapply(pkgs, require, character.only = TRUE))\n\n# This downloads the map and the data and merges them\nmap &lt;- read_sf(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/MDB_Local_Municipal_Boundary_2011.geojson\")\nmapping_data &lt;- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/education.csv\")\nmap &lt;- left_join(map, mapping_data, by = \"LocalMunicipalityCode\")\n\n# This selects out from the data the variables of interest\ndf &lt;- map |&gt;\n  st_drop_geometry() |&gt;\n  select(where(is.double), -starts_with(\"Shape\"))\n\n# This defines the user interface\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Educational geographies for South African municipalities\"),\n\n    # Sidebar with various types of input\n    sidebarLayout(\n        sidebarPanel(\n            varSelectInput(\"var\", \"Mapping variable\", df),\n            sliderInput(\"brks\", \"Classes\", min = 3, max = 8, value = 5, step = 1),\n            selectInput(\"type\", \"Classification\", c(\"equal\", \"quantile\", \"jenks\")),\n            selectInput(\"pal\", \"Palette\", rownames(brewer.pal.info)),\n            checkboxInput(\"rev\", \"Reverse palette\"),\n            checkboxInput(\"north\", \"North arrow\")\n          ),\n\n        # The main panel with contain the map plot\n        mainPanel(\n          plotOutput(\"map\")\n        )\n    )\n)\n\n# This defines the server side of the app, taking various inputs\n# from the user interface\nserver &lt;- function(input, output, session) {\n  \n  output$map &lt;- renderPlot({\n    \n    vals &lt;- map |&gt;\n      pull(!!input$var)\n    \n    brks &lt;- classIntervals(vals, n = input$brks, style = input$type)$brks\n    map$gp &lt;- cut(vals, brks, include.lowest = TRUE)\n    \n    \n    p &lt;- ggplot(map, aes(fill = gp)) +\n      geom_sf() +\n      scale_fill_brewer(\"%\", palette = input$pal, direction = ifelse(input$rev, 1, -1)) +\n      theme_minimal() +\n      guides(fill = guide_legend(reverse = TRUE)) +\n      labs(caption = \"Source: 2011 Census, Statistics South Africa\")\n    \n    if(input$north) p &lt;- p + annotation_north_arrow(location = \"tl\",\n                                              style = north_arrow_minimal(text_size = 14))\n  \n    p\n  \n  }, res = 100)}\n\n# This loads and runs the app\nshinyApp(ui, server)\n\n\nYou can play with this app to see how changing the design of the map can easily affect your interpretation of the geography of what it shows. The classic book about this is Mark Monmonier’s How to Lie with Maps."
  },
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "Measuring spatial autocorrelation",
    "section": "",
    "text": "We begin by recreating one of the maps from the previous session. You may wish to change your working directory to be the same as previously if it is not already (it should be if you are saving all your files in a Project and begin this session by opening that Project).\n\n\nCode\ninstalled &lt;- installed.packages()[,1]\npkgs &lt;- c(\"tidyverse\", \"sf\", \"RColorBrewer\", \"classInt\", \"ggplot2\")\ninstall &lt;- pkgs[!(pkgs %in% installed)]\ninvisible(lapply(pkgs, require, character.only = TRUE))\n\nif(!file.exists(\"municipal.RData\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/municipal.RData?raw=true\", \"municipal.RData\", mode = \"wb\")\nload(\"municipal.RData\")\n\nbrks &lt;- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp &lt;- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\n\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population with no schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities, creating clusters of red and of blue values (and of yellow too). However, simply ‘eye-balling’ the map to look for patterns isn’t very scientific and it can be very deceptive. You can probably see patterns in the following map, too, but they arise from an entirely random permutation of the previous map’s data – all I have done to generate the map is randomly ‘shuffle’ the data around the locations on the map."
  },
  {
    "objectID": "autocorrelation.html#introduction",
    "href": "autocorrelation.html#introduction",
    "title": "Measuring spatial autocorrelation",
    "section": "",
    "text": "We begin by recreating one of the maps from the previous session. You may wish to change your working directory to be the same as previously if it is not already (it should be if you are saving all your files in a Project and begin this session by opening that Project).\n\n\nCode\ninstalled &lt;- installed.packages()[,1]\npkgs &lt;- c(\"tidyverse\", \"sf\", \"RColorBrewer\", \"classInt\", \"ggplot2\")\ninstall &lt;- pkgs[!(pkgs %in% installed)]\ninvisible(lapply(pkgs, require, character.only = TRUE))\n\nif(!file.exists(\"municipal.RData\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/municipal.RData?raw=true\", \"municipal.RData\", mode = \"wb\")\nload(\"municipal.RData\")\n\nbrks &lt;- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp &lt;- cut(municipal$No_schooling, brks,\n                                 include.lowest = TRUE)\n\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population with no schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities, creating clusters of red and of blue values (and of yellow too). However, simply ‘eye-balling’ the map to look for patterns isn’t very scientific and it can be very deceptive. You can probably see patterns in the following map, too, but they arise from an entirely random permutation of the previous map’s data – all I have done to generate the map is randomly ‘shuffle’ the data around the locations on the map."
  },
  {
    "objectID": "autocorrelation.html#cols4all",
    "href": "autocorrelation.html#cols4all",
    "title": "Measuring spatial autocorrelation",
    "section": "cols4all",
    "text": "cols4all\nBefore we get to the main focus of today’s session, which is about assessing whether the patterns we think we see in a map might be random or whether there are clusters of ‘hot spots’ or ‘cold spots’ within it, it is worth introducing a new package which was published, on CRAN earlier this year. It is called cols4all and is introduced, with a very useful vignette, here. It is described as containing “a large collection of palettes (to be precise 436 at the time of writing), but with the central question: which palettes are good and why?”\n\n\nCode\nif(!(\"colorspace\" %in% installed)) install.packages(\"colorspace\",\n                                                  dependencies = TRUE)\nif(!(\"cols4all\" %in% installed)) install.packages(\"cols4all\",\n                                                  dependencies = TRUE)\nrequire(cols4all)\n\n\nIf we now activate its dashboard, we can learn a lot about different colour palettes and whether, for example, they are colour-blind friendly (try opening in browser if the dashboard does not load properly but that option appears).\n\n\nCode\nc4a_gui()\n\n\n\nWe can discover, for example, that the palette blue_red3 in the hcl series goes from blue to red and is colour-blind friendly. Let’s update the previous map, using cols4all to select the palette and keeping in mind that the fill colour is determined from the variable municipal$No_schooling_gp, which is a discrete (categorical) factor (use class(municipal$No_schooling_gp) to confirm this). You may need to close the dashboard, above, before you can proceed by hitting the esc key after clicking in the R Console. Note the use of the scale_fill_discrete_c4a_cat() function in the ggplot code below – scale the fill colours of the map using their discrete categories and a cols4all (c4a) colour category, which is the palette, \"hcl.blue_red3\".\n\n\nCode\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"hcl.blue_red3\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#morans-test",
    "href": "autocorrelation.html#morans-test",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran’s test",
    "text": "Moran’s test\nThe classic way of quantifying how similar places are to their neighbours is to calculate the Moran’s statistic. It is a measure of spatial autocorrelation – of how much the values of a variable exhibit spatial clustering of alike values (positive spatial autocorrelation) or of ‘opposite’ values (negative spatial autocorrelation: like the black-white of a chess board, for example). We can calculate this statistic in R using the spdep package. You may recall that that is a problem with the object (the map) municipal because of an invalid geometry (see previous session and try which(!st_is_valid(municipal)) to confirm). We were able to partially ‘fix’ this by changing its coordinate reference system.\n\n\nCode\nif(!(\"spdep\" %in% installed)) install.packages(\"spdep\", dependencies = TRUE)\nrequire(spdep)\n\nmunicipal &lt;- st_transform(municipal, 3857)\n\n\n\nCreating a neighbours list\nIf the purpose of a Moran’s test is to quantify how similar places are to their neighbours, then the first step is to define neighbours. How to do so isn’t necessarily obvious. Think about if I asked you to identify your neighbours. Would it just be people who lived next door to you? Or within a certain distance of your house? Or those you interact with most? Or…?\nIn the following example, neighbours are places that share a border (places that are contiguous). Presently it is sufficient for them to meet at a single point, so, if two places happened to be triangular in shape, it would be sufficient for the corners of those triangles to touch to count as neighbours. If the requirement is that they share an edge, not merely a corner, then change the default argument from queen = TRUE to queen = FALSE (see ?poly2nb for details).\n\n\nCode\nneighbours &lt;- poly2nb(municipal)\n\n\n\nThe function poly2nb() includes the argument snap, which is the threshold distance boundary points can be apart to still be considered contiguous. It has a default value of sqrt(.Machine$double.eps) which, on my present computer, is equal to 1.4901161^{-8}and is tiny. The problem is it can sometimes be too tiny, not capturing neighbourhood relationships when, for example, there are ‘slivers’ or slight gaps between places. To avoid this, it can be helpful to replace the default value with a small but still larger distance, e.g.poly2nb(municipal, snap = 1)\\. Consider doing this when, for example, you obtain places that appear to have no contiguous neighbours but you know that really they do.\nThe summary of neighbours reveals that, on average, each South African municipality has 5.2 neighbours but it can range from 1 (the 183rd region in the municipal data) to 10 (region 69). The most frequent number is 6.\n\n\nCode\nsummary(neighbours)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1216 \nPercentage nonzero weights: 2.220761 \nAverage number of links: 5.196581 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 24 42 52 69 31  3  4  1 \n1 least connected region:\n183 with 1 link\n1 most connected region:\n69 with 10 links\n\n\n\nIt’s pretty common for the average number of contiguous neighbours to be about 5 and the most frequent number of contiguous neighbours to be 5 or 6. It suggests that the spatial configuration of the places is loosely approximating a hexagonal tessellation, although places on the edge of a study region will typically have fewer contiguous neighbours then those at the centre. But, of course, it isn’t guaranteed. A raster grid of square cells has 4 contiguous neighbours across the grid, except at the edges.\nThe neighbours of region 69 are,\n\n\nCode\nneighbours[[69]]\n\n\n [1]  60  64  68 139 153 154 157 158 164 165\n\n\nIt is instructive to write the list of neighbours to an external neighbours file,\n\n\nCode\nwrite.nb.gal(neighbours, \"neighbours.gal\")\n\n\n… which can then be viewed by using file.edit(\"neighbours.gal\"). The file will look like the below and has a very simple format. The top line say there are 234 regions. Going down, the first of these has 4 neighbours, which are regions 13, 14, 15 and 16. The second has 6 neighbours, which are 3, 4, 8, 18, 189, 234, and so forth. The same information could be encoded in a \\(234\\times234\\) matrix, where cell \\((i, j)\\) is given a value of one if \\(i\\) and \\(j\\) are considered neighbours, else zero. The problem with that approach is most of the matrix is sparse (contains nothing but zeros) because most regions are not neighbours. It is quicker and more memory efficient to state which regions are neighbours. The rest, by definition, are not.\n\nThese neighbourhood relationships can be viewed as a graph by extracting the coordinate points (st_coordinates()), of the centroids (st_centroid()), of the polygons that represent each municipality, and by using the plot functions for sf (simple features) and nb (neighbours list) objects. Centroids are the centre points of geometric objects. The argument of_largest_polygon = TRUE returns the centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the whole MULTIPOLYGON, a multipolygon being when one place is represented by multiple polygons such as a mainland and an offshore island (so of_largest_polygon = TRUE would give the point at the centre of the mainland). Setting this argument to true should help the centroid to lie within the boundary of the place, rather than, say, in the sea between the mainland and an island, although it isn’t guaranteed.\n\nImagine an area that for some reason is roughly C shaped. Where would the centroid of that be and would it be within the area’s own boundary?\n\n\nCode\ncoords &lt;- st_centroid(municipal, of_largest_polygon = TRUE)\npts &lt;- st_coordinates(coords)\n\npar(mai = c(0, 0, 0, 0))  # Remove the margins and white space around the plot\nplot(st_geometry(municipal), border = \"grey\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\n\nCreating spatial weights\nThe neighbourhood list simply defines which places are neighbours. The spatial weights goes a step further and gives a weight to each neighbourhood link. One motivation for doing this is to stop any statistic that is based on a sum across neighbourhood links to be dominated by those neighbourhoods with most neighbours. Moran is one such statistic. Hence, if a region has six neighbours, each of those is given a weight of \\(1/6\\). If it has four, \\(1/4\\), and so forth. This is called row-standardisation and is the default style in the conversion of a neighbourhood to a spatial weights list with the function nb2listw(). See ?nb2listw for alternative specifications.\n\n\nCode\nspweight &lt;- nb2listw(neighbours)\n# Here are the neighbours of and weights for the first region:\nspweight$neighbours[[1]]\n\n\n[1] 13 14 15 16\n\n\nCode\nspweight$weights[[1]]\n\n\n[1] 0.25 0.25 0.25 0.25\n\n\n\nDealing with places that don’t have any neighbours\nNot all places have neighbours. Islands, for example, can be separated from the mainland by the sea. If you attempt to create spatial weights using the nb2listw() function with a neighbours list that includes places without neighbours, then you will get an error message:\nError in nb2listw(neighbours) : Empty neighbour sets found.\nThere are four ways you can address this problem.\n\nChange the snap distance in the function poly2nb() to include, as neighbours, places that appear not to actually share a border but are within a certain threshold distance apart. As noted previously, this can be helpful to deal with digitisation errors – when, for example, there are small gaps between places that do really share a boundary.\nChange the way you are defining the neighbourhood relationships – for example, by identifying the \\(k\\) nearest neighbours of each location, regardless of whether they share a border or not (see below). Everywhere has somewhere that is closest to it.\nSave the .gal file using write.nb.gal() and then manually edit it to make any connections you wish to between places. You can read the revised version back into R using the function read.gal() – see ?read.gal().\nLeave it as it, with some places not having any neighbours, but specify the argument zero.policy = TRUE in nb2listw so it allows empty sets. For example, spweight &lt;- nb2listw(neighbours, zero.policy = TRUE). Other related functions will also need zero.policy = TRUE to be included, such as moran.test(), moran.plot() and localmoran().\n\nPresently, however, this is not an issue because none of the South African municipalities are without a contiguous neighbour.\n\n\n\nCalculating the Moran’s value\nTo recap: we began with a definition of neighbours based on contiguity. We then row-standardised those weights to generate the spatial weights. With those we can now run a Moran’s test to measure the strength of spatial autocorrelation between neighbours in the municipal$No_schooling variable.\n\n\nCode\nmoran &lt;- moran.test(municipal$No_schooling, spweight)\nmoran\n\n\n\n    Moran I test under randomisation\n\ndata:  municipal$No_schooling  \nweights: spweight    \n\nMoran I statistic standard deviate = 14.014, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.576702139      -0.004291845       0.001718747 \n\n\nThe Moran statistic is 0.577 and the 95% confidence interval is,\n\n\nCode\nz &lt;- c(-1.96, 1.96)\nround(moran$estimate[1]  + z * sqrt(moran$estimate[3]), 3)\n\n\n[1] 0.495 0.658\n\n\nSince the confidence interval does not include the expected value of -0.004, we can conclude that there is statistically significant positive autocorrelation in the variables – municipalities with higher percentages of no schooling tend to be surrounded by other municipalities with the same. Similarly, low values tend to be surrounded by other ones that are low.\n\nThe expected value is very close to zero so what we are almost saying is that because the confidence interval does not span zero so there is evidence of positive spatial autocorrelation. Although this is quite close to being true, to actually be correct would require the expected value of the statistic to be zero when there is no spatial autocorrelation. It isn’t. It is presently -0.004 and approaches zero as the number of observations increases: \\(E(I) = -1 / (n - 1)\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "autocorrelation.html#moran-plot-and-local-moran-values",
    "href": "autocorrelation.html#moran-plot-and-local-moran-values",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran plot and local Moran values",
    "text": "Moran plot and local Moran values\nWhilst there is positive spatial autocorrelation in the values overall, not everywhere is surrounded by similar values. The following plot has 4 quadrants marked upon it. The top right indicates places where both they and their average neighbour have above average values of municipal$No_schooling. We can describe these as high-high clusters on the map. The bottom left indicates places where they and their average neighbour have below average values. These are low-low clusters. Both the high-high and the low-low contribute to positive spatial autocorrelation because, for these, the places and their neighbours display similar values. The two other quadrants do not. In the top left are low-high clusters. In the bottom right are high-low. There reveal clusters of dissimilar values (negative spatial autocorrelation). In the chart, the high-high and low-low places are more plentiful than the low-high and high-low ones, hence the upwards sloping line of best fit and the positive Moran statistic.\n\n\nCode\n# Return the plot margins to their default values in R:\npar(mai=c(1.02,0.82,0.82,0.42))\nmoran.plot(municipal$No_schooling, spweight)\n\n\n\n\n\nIt is straightforward to map which quadrant each place belongs to. First, we calculate the local Moran statistics proposed by Anselin (1995). A local statistic is one that applies to a subspace of the map, whereas a global statistic is a summary measure for the whole map. Anselin showed that the (global) Moran statistic can be decomposed into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours. There are 234 municipalities in the data so there will be 234 local Moran values too.\n\n\nCode\nlocalm &lt;- localmoran(municipal$No_schooling, spweight)\n\n\nUsefully, if we look at the attributes of localm, we discover an attribute named quadr which contains what we want and which can be mapped.\n\n\nCode\nnames(attributes(localm))\n\n\n[1] \"dim\"      \"dimnames\" \"call\"     \"class\"    \"quadr\"   \n\n\nCode\nhead(attr(localm, \"quadr\"))\n\n\n      mean   median    pysal\n1 Low-High Low-High Low-High\n2  Low-Low  Low-Low  Low-Low\n3  Low-Low  Low-Low  Low-Low\n4 High-Low High-Low High-Low\n5  Low-Low  Low-Low  Low-Low\n6  Low-Low  Low-Low  Low-Low\n\n\nIn fact, it includes three different versions of what we might want, the differences being due to which average “low” and “high” are defined by (see the text below the section Value in ?localmoran for more details). Let’s now map one of them, still using the cols4all package to select a colour palette.\n\n\nCode\nquadr &lt;- attr(localm, \"quadr\")\nggplot(data = municipal, aes(fill = quadr$median)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"parks.charmonix\") +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nUnfortunately, the resulting map is somewhat misleading because not all of the local Moran values are statistically significant and some of the various high-high, low-low, etc. pairings my be only trivially alike or dissimilar. Looking at the top of the local Moran data suggests a way of isolating those that are statistically significant and is adopted in the following map.\n\n\nCode\nhead(localm, n = 3)   # The p-values are in column 5\n\n\n          Ii          E.Ii     Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.1260406 -0.0055341954 0.31779548 -0.2137648      0.8307305\n2  0.1064341 -0.0020006564 0.07619128  0.3928401      0.6944376\n3  0.1402600 -0.0003160278 0.01205565  1.2803122      0.2004354\n\n\nCode\nquadr[localm[,5] &gt; 0.05 | is.na(localm[,5]), ] &lt;- NA\n\nggplot(data = municipal, aes(fill = quadr$median)) +\n  geom_sf() +\n  scale_fill_discrete_c4a_cat(name = \"%\", palette = \"parks.charmonix\",\n                              na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nArguably, this new map may still not apply a strict enough definition of statistical significance because the issue of repeat testing has not been tackled. Remember, there are 234 places, 234 local Moran values and therefore 234 tests of significance. Essentially the issue is if we test often enough, then it is hardly surprising if some values emerge as ‘significant’. It’s a bit like a fishing trip where we keep casting until we finally catch something.\nThe p-values can be adjusted for this using R’s p.adjust() function. The following example uses a false discovery rate method (method = fdr) but other alternatives include method = bonferroni. A question is whether this is now too strict given that there are not 234 independent tests. Rather, the data have overlapping geographies (places share neighbours) as well as spatial dependencies.\n\n\nCode\nquadr[p.adjust(localm[,5], method = \"fdr\") &gt; 0.05 | is.na(localm[,5]), ] &lt;- NA\n\n# There is now a problem in that only 2 of the 4 categories remain,\n# which are Low-Low and High-High:\nunique(quadr)\n\n\n        mean    median     pysal\n1       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n68   Low-Low   Low-Low   Low-Low\n92 High-High High-High High-High\n\n\nCode\n# For compatibility with the previous map, I extract its palette\npal &lt;- c4a(\"parks.charmonix\", 4)\npal\n\n\n[1] \"#008FF8\" \"#B6AA0D\" \"#E2C2A2\" \"#E23B0E\"\n\n\nCode\n# and then only use the first and fourth of those colours in the new map:\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(name = \"%\", type = pal[c(1,4)], na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant (p-adjusted) local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#issues-with-the-moran-statistic",
    "href": "autocorrelation.html#issues-with-the-moran-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Issues with the Moran statistic",
    "text": "Issues with the Moran statistic\n\nHow to define neighbours?\n\nn-order contiguity\nAny statistic that includes spatial weights is dependent upon how those weights are defined. How, then, to decide which places are neighbours. The calculations above use first order contiguity – places that share a boundary – but we could extend that definition to include neighbours of neighbours, or more:\n\n\nCode\nneighbours &lt;- nblag(neighbours, maxlag = 2)\n\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))   # Plot graphics in a 1 row by 2 column grid\nplot(st_geometry(municipal), border = \"grey\", main = \"First order contiguity\")\nplot(neighbours[[1]], pts, add = T)\nplot(st_geometry(municipal), border = \"grey\", main = \"Second order contiguity\")\nplot(neighbours[[2]], pts, add = T)\n\n\n\n\n\nChanging the definition of neighbours does, of course, change the value of the Moran statistic and it will change the local Moran values too.\n\n\nCode\nlapply(neighbours, \\(x) {\n  nb2listw(x) %&gt;%\n    moran.test(municipal$No_schooling, .) -&gt;\n    moran\n  moran$estimate[1]\n})\n\n\n[[1]]\nMoran I statistic \n        0.5767021 \n\n[[2]]\nMoran I statistic \n        0.3890747 \n\n\nIn passing, if you remember the discussion around piping and the differences between %&gt;% and |&gt; back when we were looking at ‘Flavours of R’ and `Tidyverse’ then you may recall how the following code is equivalent to the above but is more ‘clunky’:\n\n\nCode\nlapply(neighbours, \\(x) {\n  nb2listw(x) |&gt;\n    (\\(y) moran.test(municipal$No_schooling, y))() -&gt;\n    moran\n  moran$estimate[1]\n})\n\n\n[[1]]\nMoran I statistic \n        0.5767021 \n\n[[2]]\nMoran I statistic \n        0.3890747 \n\n\n\n\nk nearest neighbours\nThere is no particular reason to stick with a contiguity-based definition of neighbours. We could, for example, look for the \\(k\\) nearest neighbours (or, more precisely, the \\(k\\) nearest centroids to the centroid of each place), as in the two examples below.\n\n\nCode\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))\n\ncoords &lt;- st_centroid(municipal, of_largest_polygon = TRUE)\n\nneighbours &lt;- knn2nb(knearneigh(coords, k = 5))\nplot(st_geometry(municipal), border = \"grey\", main = \"Five nearest neighbours\")\nplot(neighbours, pts, add = T)\n\nneighbours &lt;- knn2nb(knearneigh(coords, k = 10))\nplot(st_geometry(municipal), border = \"grey\", main = \"Ten nearest neighbours\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\nThe function knearneigh() contains the default argument, longlat = NULL. It should be ok not to change this here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. If you suspect it isn’t or if coords is simply a matrix of point coordinates, not an explicitly spatial object, change the default argument to longlat = TRUE.\nWe can, if we wish, run through all the possible values of \\(k\\), from \\(1\\) to \\(k_{max} = (n - 1)\\) where \\(n\\) is the number of municipalities, and consider the Moran statistics that they generate. The resulting chart shows that the statistic is highly dependent on the scale of the analysis: as \\(k\\) increases, neighbourhood relationships extend over an increasing portion of the map, and the statistic tends to decline. It declines because nearby places tend to have similar values whereas those that are further away introduce more variation because of the spatial heterogeneity across the map. This is an empirical example of Tobler’s first ‘law’ of geography in action: “everything is related to everything else, but near things are more related than distant things.”\n\nThe code would generate a lot of warning messages, warning you that \\(k\\) has become a large subset of all \\(n\\). However, I have suppressed them by wrapping knearneigh() in the suppressWarnings() function.\n\n\nCode\nn &lt;- nrow(municipal)\nI &lt;- sapply(1: (n-1), \\(k) {\n  suppressWarnings(knearneigh(coords, k)) |&gt;\n    knn2nb() |&gt;       # I am mixing my\n    nb2listw() %&gt;%    # pipes here (deliberately!)\n    moran.test(municipal$No_schooling, .) -&gt;\n    moran\n  moran$estimate[1]\n})\n\nggplot(data.frame(k = 1:(n-1), I = I), aes(x = k, y = I)) +\n  geom_line() +\n  ylab(\"Moran statistic\") -&gt; g\ng\n\n\n\n\n\nIn principle the chart could be used to identify an ‘optimal’ value of \\(k\\). Unfortunately, the present case offers few clues as to what that optimal value should be. We could use the inflection point of the curve, based on this tutorial:\n\n\nCode\nif(!(\"inflection\" %in% installed)) install.packages(\"inflection\",\n                                                    dependencies = TRUE)\nrequire(inflection)\nk &lt;- 1: (n-1)\ncheck_curve(k, I)\n\n\n$ctype\n[1] \"convex_concave\"\n\n$index\n[1] 0\n\n\nCode\ninfl_pt &lt;- bese(k, I, index = 0)$iplast\ncat(\"\\nThe inflection point is at k = \", infl_pt, \"\\n\")\n\n\n\nThe inflection point is at k =  142 \n\n\nCode\ng + geom_vline(xintercept = infl_pt, linetype = \"dotted\")\n\n\n\n\n\nThe result has correctly identified where the curve turns and you could reasonably argue that this is a good value of \\(k\\) to avoid given the Moran’s statistic is higher for values of \\(k\\) either side of it. However, eliminating one value from the choice set still leaves a lot of others to choose from!\nWe could also try selecting based on seeking to avoid a sharp decrease in the Moran statistic for an increase in \\(k\\) by one. The sharpest fall comes by changing from \\(k = 3\\) to \\(k = 4\\) (the function diff() in the code chunk below calculates the difference between the value of one number in a vector and the next number to its right in that same vector):\n\n\nCode\nslope &lt;- diff(I)\n  # It's really diff(I) / diff(k) that should be calculated but diff(k) = 1 in all cases here\nmax_fall &lt;- k[which.min(slope)]\ncat(\"\\nThe greatest fall in the Moran value is between k = \",\n    max_fall, \"and k = \", max_fall + 1, \"\\n\")\n\n\n\nThe greatest fall in the Moran value is between k =  3 and k =  4 \n\n\nCode\ng + geom_vline(xintercept = max_fall, linetype = \"dashed\")\n\n\n\n\n\nThis shows that the ‘cost’ in increasing from \\(k = 3\\) to \\(k = 4\\) is a relatively big drop in the similarity of the neighbours to each other (it’s still not actually that large though; it’s a drop of 0.032.\nWe could look at the drop in the Moran statistic over a larger increase in \\(k\\) than one. In the following code chunk the differences are calculated over a span of ten.\n\n\nCode\nslope &lt;- diff(I, lag = 10)\nmax_fall &lt;- k[which.min(slope)]\ncat(\"\\nNow the greatest fall in the Moran value is between k = \",\n    max_fall, \"and k = \", max_fall + 10, \"\\n\")\n\n\n\nNow the greatest fall in the Moran value is between k =  1 and k =  11 \n\n\nCode\ng + geom_vline(xintercept = c(max_fall, max_fall + 10), linetype = \"dotdash\")\n\n\n\n\n\nIn either case, what it favours is a small value of \\(k\\). That is hardly surprising because near neighbours do tend to be more similar – again, that’s Tobler’s first ‘law’ of geography, which, although not a law at all and with plenty of exceptions, still has a ring of truth to it. Similarities tend to drop away most quickly at fairly short distances and then ‘flatten out’ at greater distances, which is what the curve in the charts above is showing.\nSo, then, the ‘optimal’ value of \\(k\\) is a small number of nearest neighbours? Not necessarily! When calculating the Moran statistic it is based on the correlation of each location with their neighbours. Imagine there is error in the data; some rogue results. Alternatively, imagine some places are just very unusual. Those errors or outliers will have more influence in the calculation if they are pooled with only a small number of other neighbours than if more neighbours are included so that their unusualness has more chance of being ‘averaged out’. But, if we do choose to include lots of neighbours then we are including ones that are further away and probably less like the location we are correlating them against. We will return to this trade-off when we look at geographically weighted statistics.\nHow else could we decide on the value of \\(k\\) to select? The p-value for the Moran statistics is not shown in the charts but is least when \\(k = 200\\). That might be a justification, of sorts, for choosing \\(k = 200\\) but it is splitting hairs somewhat when all but one of the p-values is tiny and well below \\(p = 0.001\\).\nPerhaps the best way is simply to decide on a threshold value for the Moran statistic and use that but quite how you decide on that threshold…? Here is the maximum value of \\(k\\) for a purely arbitrary threshold of \\(I &gt; 0.4\\).\n\n\nCode\nthreshold_k &lt;- max(which(I &gt; 0.4))\ncat(\"\\nThe maximum value of k for a Moran value greater than 0.4 is \",\n    threshold_k, \"\\n\")\n\n\n\nThe maximum value of k for a Moran value greater than 0.4 is  21 \n\n\nCode\ng + geom_vline(xintercept = threshold_k, linetype = \"dashed\") +\n    geom_hline(yintercept = 0.4, linetype = \"dotted\")\n\n\n\n\n\n\n\nDistance bounds\nOther ways of defining neighbours include whether they lie within a lower or upper distance bound of each other: see ?dnearneigh and this vignette on Creating Neighbours.\n\n\n\nHow to interpet the I statistic\nConceptually, the Moran statistic is easy to understand: it is a (global) measure of the correlation between the values of a variable at a set of locations, and the value of the same variable for those locations’ neighbours.\nMore simply, it can be thought of as the correlation – the strength of association – between locations and their average neighbour. Keep in mind that it is a measure of correlation, not the more commonly used Pearson correlation. The difference is evident in the following comparison, with \\(k = 21\\), where the Moran value is less than two thirds of the Pearson correlation between locations and their average neighbour, where the value for the average neighbour is calculated using last.listw().\n\n\nCode\nspweight &lt;- nb2listw(knn2nb(knearneigh(coords, 21)))\npearson &lt;- cor(lag.listw(spweight, municipal$No_schooling),\n               municipal$No_schooling)\nmoran &lt;- moran.test(municipal$No_schooling, spweight)$estimate[1]\ndata.frame(person = pearson, moran = moran, row.names = \"correlation\")\n\n\n               person     moran\ncorrelation 0.6643967 0.4013773\n\n\nAs Brunsdon and Comber note, the value of Moran’s I is not constrained to be in the range from -1 to +1 but changes with the spatial weights matrix. Following, Jong et al. (1984, p. 20), the extremes for the current spatial weights are,\n\n\nCode\nlistw2mat(spweight) %&gt;%\n  range(eigen((. + t(.)) / 2))\n\n\n[1] -0.420394  1.031139\n\n\nIf this is correct then not only is this not in the range -1 to 1, it is not symmetric around the expected (null) value of -0.004.\nGiven this, it might, perhaps, be regarded as preferable to use the more readily interpreted Pearson correlation between locations and their average neighbour. That correlation, with \\(k = 21\\) nearest neighbours, was calculated above and is 0.664. Whether that figure arose by chance can be assessed using a permutation approach. Given the geography of the South African municipalities, their neighbourhood relations and given the data values, then permuting those values randomly 1000 times allows us to look whether the value of 0.664 is unusual compared to what could arise by chance. Specifically, we look at whether the value is outside of the ‘middle 95%’ of values that have been generated through randomisation. If it is, then it suggests a less than 5-in-100 probability that it arose by chance. (A permutation approach is also available for the global and local Moran statistics: see ?moran.mc and ?localmoran_perm.)\n\n\nCode\nn &lt;- length(municipal$No_schooling)\nsapply(1:1000, \\(x) {\n  sample(municipal$No_schooling, n) %&gt;% \n    cor(lag.listw(spweight, .))\n}) %&gt;%\n  quantile(prob = c(0.025, 0.975))\n\n\n      2.5%      97.5% \n-0.2195203  0.1232592 \n\n\nNote, however, that there is a subtle but important difference between Moran’s I and the Pearson correlation of the values and their average neighbour. Moran’s I can be used to determine whether, for example, high values of a measurement tend to be surrounded by average neighbours that also have a high value, where the measurement scale is the same for the locations and their average neighbour. What the Pearson correlation is assessing is whether values that are high, relatively speaking, for all the locations are surrounded by average neighbours that have a high value relatively speaking, for all the average neighbours. What they are measuring the correlation of is therefore not the same. The difference can be seen in the formulae below:\n\\(I_{xy} = \\dfrac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\)\n\\(r_{xy} = \\dfrac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\)\nwhere \\(I_{xy}\\) is the Moran value, \\(r_{xy}\\) is the Pearson value, \\(x\\) is the measured value at each location and \\(y\\) is the (spatially lagged) value for the average neighbour."
  },
  {
    "objectID": "autocorrelation.html#getis-and-ord-g-statistic",
    "href": "autocorrelation.html#getis-and-ord-g-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Getis and Ord G-Statistic",
    "text": "Getis and Ord G-Statistic\nA different method for identifying ‘hot’ or ‘cold spots’ of a variable is provided by the G-statistic. As Brunsdon and Comber observe, the statistic – which is a local statistic, calculated for each location in turn – is based on the proportion of the total sum of standardised attribute values that are within a threshold distance, \\(d\\), of each area centroid. Looking at the map below, which is of South African wards (simplified slightly from this source), we may suspect there are clusters of places with greater percentages of their populations having relatively high incomes and that these are spatial anomalies. Because the percentages are extremely skewed, they have been plotted with a square root transformation applied to the scale of the map (trans = \"sqrt\") – notice how the values 0 to 4% (which are the original values, not the square roots) occupy more of the legend than the values 4 to 8 do, and so forth. Note that we are also using a col4all diverging colour palette for a continuous variable.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nggplot(data = wards, aes(fill = High_income)) +\n  geom_sf(colour = \"transparent\") +\n  scale_fill_continuous_c4a_div(name = \"%\", palette = \"carto.yel_or\",\n                                trans = \"sqrt\", mid = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Percentage of Population with income R614401 or greater\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )\n\n\n\n\n\nThe G-statistic requires a binary spatial weights (style = \"B\", below), whereby locations either are or are not within the threshold distance of each other. The following has a threshold distance of 20km. Not all of the ward centroids will be within 20km of another which creates situations of zero neighbours. That is tolerated by setting zero.policy = TRUE in the conversion from a neighbours to spatial list.\n\nAs with the function knearneigh(), dnearneigh() has the default argument, longlat = NULL, which should not need changing here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. However, don’t take this for granted.\n\n\nCode\ncoords &lt;- st_centroid(wards, of_largest_polygon = TRUE)\nneighbours &lt;- dnearneigh(coords, 0, 20)\nspweight &lt;- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG &lt;- localG(wards$High_income, spweight)\n\n\nHaving calculated the local G values, we can map them, using a manually generated fill palette. The argument na.translate = F removes the NA values from the legend but not from the map, wherein they are shaded white (na.value = \"white\"). The G values are also standardised z-values, hence values of 1.96 or greater are relatively rate if the assumption that the statistic is Normally distributed is warranted. (If it isn’t, consider using localG_perm.)\n\n\nCode\nbrks &lt;- c(min(wards$localG, na.rm = TRUE), -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp &lt;- cut(wards$localG, brks, include.lowest = TRUE)\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  theme_minimal() +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 20km threshold\"\n  )\n\n\n\n\n\nLet’s now add a refinement to the map and, based on what we learned from the previous session, label the cities that appear to contain a hot spot of higher percentages of higher earners. Note the use of st_join() which is a spatial join: it identifies, from geography, which ward the cities are located in and appends the ward data, including the G statistics, to the cities, retaining only those that are in the most significant hot spots. Note that this definition of ‘most significant’ doesn’t address the problem of repeat testing which we also saw with the local Moran statistics.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n  \nread_sf(\"hotosm_zaf_populated_places_points.shp\") |&gt;\n  filter(place == \"city\") |&gt;\n  st_join(wards) |&gt;\n  filter(localG &gt; 3.29) -&gt;\n  cities\n\nlast_plot() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)\n\n\n\n\n\nThe results are dependent on the distance threshold defining which places are or are not neighbours. Here are the results with a distance threshold of 100km.\n\n\nCode\nneighbours &lt;- dnearneigh(coords, 0, 100)\nspweight &lt;- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG &lt;- localG(wards$High_income, spweight)\n\nbrks &lt;- c(min(wards$localG, na.rm = TRUE),\n          -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp &lt;- cut(wards$localG, brks, include.lowest = TRUE)\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 100km threshold\"\n  ) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)"
  },
  {
    "objectID": "autocorrelation.html#summary",
    "href": "autocorrelation.html#summary",
    "title": "Measuring spatial autocorrelation",
    "section": "Summary",
    "text": "Summary\nThis session has been about identifying and quantifying spatial clustering in data, using a combination of ‘global’ (whole map) and local statistics; specifically, Moran’s I, its local equivalent, the local Moran values, and the Getis-Ord G statistic. These statistics are dependent on the spatial weights matrix used in their calculation, which does, unfortunately, create something of a vicious circle: ideally that matrix would be calibrated to the spatial patterns evident in the data but those patterns are only measured and quantified once the weights matrix has been specified. For example, we could, in principle, use Moran’s I to measure the spatial autocorrelation and select the weights matrix accordingly. However, we need the spatial weights matrix to calculate Moran’s I. One option is to take the view that looking at the patterns at a range of scales is itself revealing so choose lots of bandwidths, not just one, and treat it as a multiscale analysis, as in the following.\n\n\nCode\n# Sets the threshold distance from 20 to 180km in intervals of 20\n# I chose 9 distances because they can be shown in a 3 x 3 grid of maps\nd &lt;- seq(from = 20, by = 20, length.out = 9)\n\n# Calculate the G statistics for each distance and save them\n# as a list of sf features (maps)\ncoords &lt;- st_centroid(wards, of_largest_polygon = TRUE)\nmaps &lt;- suppressWarnings(\n    lapply(d, \\(x) {\n    neighbours &lt;- dnearneigh(coords, 0, x)\n    spweight &lt;- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\n    wards$localG &lt;- localG(wards$High_income, spweight)\n    wards$distance &lt;- x\n    return(wards)\n  })\n)\n\n# Bind the maps together into a single object to be used\n# for the faceting in ggplot2\nmaps &lt;- do.call(rbind, maps)\n\nbrks &lt;- c(min(maps$localG, na.rm = TRUE),\n          -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(maps$localG, na.rm = TRUE))\nmaps$localG_gp &lt;- cut(maps$localG, brks, include.lowest = TRUE)\n\n# This function is used with the labeller argument in the\n# facet_wrap function to customise the title of each sub-plot\nfacet_labels &lt;- \\(x) paste0(x, \"km radius\")\n\nggplot(data = maps, aes(fill = localG_gp)) +\n  geom_sf(colour = NA) +\n  scale_fill_discrete_c4a_div(name = \"G\", palette = \"brewer.rd_yl_bu\",\n                              reverse = TRUE, na.translate = FALSE) +\n  facet_wrap(~ distance, labeller = labeller(distance = facet_labels)) +\n  theme_light() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank(),\n        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),\n        axis.text.x = element_blank(), axis.text.y = element_blank(),\n        legend.position = \"bottom\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "autocorrelation.html#further-reading",
    "href": "autocorrelation.html#further-reading",
    "title": "Measuring spatial autocorrelation",
    "section": "Further Reading",
    "text": "Further Reading\n\nChapters 7 and 8 of Spatial Data Science with Applications in R by Paula Moraga.\n\nChapter 2 of Spatial Regression Models for the Social Sciences by Guangqing Chi and Jun Zhu is recommended. For University of Bristol students, it is available to view as an eBook here.\n\nChapter 8 on Localised Spatial Analysis in An Introduction to R for Spatial Analysis & Mapping by Chris Brunsdon and Lex Comber."
  },
  {
    "objectID": "thematicmaps.html#part-1",
    "href": "thematicmaps.html#part-1",
    "title": "Thematic maps in R",
    "section": "Part 1",
    "text": "Part 1"
  },
  {
    "objectID": "thematicmaps.html#part-2",
    "href": "thematicmaps.html#part-2",
    "title": "Thematic maps in R",
    "section": "Part 2",
    "text": "Part 2\nTo begin, you may need to open your Project then load the workspace from Part 1. For example,\n\n\nCode\nload(\"making_maps.RData\")\n\n\nYou may also need to reload all the previously used packages/libraries. I think this includes all the ones that you still need:\n\n\nCode\n# Just a way of requiring more than one package at the same time:\npkgs &lt;- c(\"tidyverse\", \"sf\", \"ggplot2\", \"RColorBrewer\", \"ggspatial\", \"ggsflabel\")\ninvisible(lapply(pkgs, require, character.only = TRUE))"
  },
  {
    "objectID": "gwstats.html",
    "href": "gwstats.html",
    "title": "Geographically Weighted Statistics",
    "section": "",
    "text": "In the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by ‘pooling’ the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in GWmodel – an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. These are geographically weighted statistics."
  },
  {
    "objectID": "gwstats.html#introduction",
    "href": "gwstats.html#introduction",
    "title": "Geographically Weighted Statistics",
    "section": "",
    "text": "In the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by ‘pooling’ the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in GWmodel – an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. These are geographically weighted statistics."
  },
  {
    "objectID": "gwstats.html#geographical-weighted-statistics",
    "href": "gwstats.html#geographical-weighted-statistics",
    "title": "Geographically Weighted Statistics",
    "section": "Geographical Weighted Statistics",
    "text": "Geographical Weighted Statistics\nThe idea behind geographically weighted statistics is simple. Instead of calculating the (global) mean average, for example, for the all the data and the whole map, a series of (local) averages are calculated for various sub-spaces within the map.\nImagine a point location, \\(i\\), at position \\((u_i, v_i)\\) on the map. To calculate the local statistic, first find either the \\(k\\) nearest neighbours to \\(i\\) or all of those within a fixed distance, \\(d\\), from it. Second, to add the geographical weighting in the name of the statistics, apply a weighting scheme whereby the neighbours nearest to \\(i\\) have most weight in the subsequent calculations and the weights decrease with increasing distance from \\(i\\), becoming zero at the \\(k\\)th nearest neighbour or at the distance threshold, \\(d\\). Third, calculate, for the point and its neighbours, the weighted mean value (or some other summary statistic) of a variable, using the inverse distance weighting in the calculation. Fourth, repeat the process for other points on the map. This means that if there are \\(n\\) points of calculation then there will be \\(n\\) geographically weighted mean values calculated across the map. These can be then be compared to look for spatial variation.\nLet’s see this in action, beginning by ensuring the necessary packages are installed and required. As with previous sessions, you might start by opening the R Project that you created for these classes.\n\n\nCode\ninstalled &lt;- installed.packages()[,1]\nrequired &lt;- c(\"colorspace\", \"cols4all\", \"GWmodel\", \"proxy\", \"sf\", \"sp\",\n              \"tidyverse\", \"tmap\")\ninstall &lt;- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE,\n                                     repos = \"https://cloud.r-project.org\")\n\nrequire(cols4all)\nrequire(GWmodel)\nrequire(sf)\nrequire(tidyverse)\nrequire(tmap)\n\n\nWe will use the same data as in the previous session but confine the analysis to the Western Cape of South Africa. This is largely to reduce run times but there is another reason that I shall return to presently.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nwards |&gt;\n  filter(ProvinceNa == \"Western Cape\") -&gt;\n  wcape_wards\n\n\nThe calculated points for the geographically weighted statistics will be the ward centroids. A slight complication here is that GWmodel presently is built around the elder sp not sf formats for handling spatial data in R so the wards need to be converted from the one format to the other.\n\n\nCode\nwcape_wards_sp &lt;- as_Spatial(wcape_wards)\n\n\nWe can see that wcape_wards is of class sf,\n\n\nCode\nclass(wcape_wards)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nwhereas wcape_wards_sp is now of class SpatialPolygonsDataFrame, which is what we want.\n\n\nCode\nclass(wcape_wards_sp)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nHaving made the conversion we are ready to calculate some geographically weighted statistics, doing so for the High_income variable – the percentage of the population with income R614401 or greater in 2011 – in the example below.\n\n\nCode\ngwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\n\n\nMost warning messages are intended to be helpful and the one generated from the code above is not an exception. It is related to a change in how coordinate reference systems (CRS) are represented but does not affect the results of the analyses here.\nThe results are contained in the spatial data frame ($SDF),\n\n\nCode\nhead(gwstats$SDF)\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 6 \nextent      : 18.30766, 18.64221, -34.12601, -33.82513  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs \nvariables   : 5\nnames       :    High_income_LM,    High_income_LSD,    High_income_LVar,   High_income_LSKe,  High_income_LCV \nmin values  : 0.101829034109578, 0.0561016907606905, 0.00314739970620815, -0.283824535877161, 0.44823543363985 \nmax values  :  5.88774318923676,   2.92991675455044,    8.58441218859538,    1.7142999351663, 1.45252681718161 \n\n\nAs well as the local means, the local standard deviations, variances, skews and coefficients of variation are included (the coefficient of variation is the ratio of the standard deviation to the mean). The local medians, interquartile ranges and quantile imbalances could also be added by including the argument quantile = TRUE in gwss() – see ?gwss.\nThe results are dependent on the data (of course) but also\n\nthe kernel (i.e. the shape of the weighting – the distance decay – around each point), and\nthe bandwidth (i.e. the maximum number of neighbours to include, \\(k\\) or the distance threshold, \\(d\\)).\n\nThe kernel matters…\n\nSource: GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models\n… but it (usually) matters much less than the bandwidth, which controls the amount of spatial smoothing: the larger it is, the more neighbours are being averaged over. The trade-off is between bias and precision. A smaller bandwidth is less likely to average-out geographical detail in the data and should create a geographically weighted average, for example, that is representative of the location at the centre of the kernel but it is also dependent on a small number of observations, some or more of which could be in error or unsual outliers for the vicinity.\nThe following maps compare a bandwidth of \\(bw = 10\\) nearest neighbours to \\(bw = 100\\). If you zoom into the areas around the north of Cape Town you will see some of the differences. Note that the argument adaptive = TRUE sets the bandwidth to be in terms of nearest neighbours, else it would indicate a fixed distance of 10 metres. The advantage of using nearest neighbours is it allows for varying population densities. Otherwise, using a fixed distance, more rural areas will tend to have fewer neighbours than urban ones because those rural areas are larger and more spaced apart.\n\n\nCode\ngwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw10 &lt;- gwstats$SDF$High_income_LM\n\ngwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwcape_wards$bw100 &lt;- gwstats$SDF$High_income_LM\n\nwards2 &lt;- pivot_longer(wcape_wards, cols = starts_with(\"bw\"), names_to = \"bw\",\n             values_to = \"GWmean\")\n\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(wards2, names = \"wards\") +\n  tm_fill(\"GWmean\", palette = \"Reds\", title = \"%\",\n          alpha = 0.7,\n          id = \"District_1\",\n          popup.vars = c(\"GW mean:\" = \"GWmean\",\n                         \"Ward ID:\" = \"WardID\"),\n          popup.format = list(digits = 1)) +\n  tm_borders() +\n  tm_facets(by = \"bw\") +\n  tm_legend(title =\n        \"Geographically weighted % population with income R614401 or greater\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA nice feature of the facetting (tm_facets) in tmap is it has created two dynamically linked maps.\n\nA slight tangent\nYou may note the use of the pivot_longer function from the tidyverse packages in the code above. To see what this does, take a look at,\n\n\nCode\nwcape_wards |&gt;\n  st_drop_geometry() |&gt;\n  select(WardID, bw10, bw100) |&gt;\n  arrange(WardID) |&gt;\n  head(n = 3)\n\n\n    WardID      bw10     bw100\n1 10101001 0.3868500 0.4349990\n2 10101002 0.4108465 0.4435761\n3 10101003 0.3742776 0.4254601\n\n\nNow compare it with,\n\n\nCode\nwcape_wards |&gt;\n  st_drop_geometry() |&gt;\n  select(WardID, bw10, bw100) |&gt;\n  pivot_longer(cols = starts_with(\"bw\"), names_to = \"bw\",\n          values_to = \"GWmean\") |&gt;\n  arrange(WardID) |&gt;\n  head(n = 6)\n\n\n# A tibble: 6 × 3\n  WardID   bw    GWmean\n  &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;\n1 10101001 bw10   0.387\n2 10101001 bw100  0.435\n3 10101002 bw10   0.411\n4 10101002 bw100  0.444\n5 10101003 bw10   0.374\n6 10101003 bw100  0.425\n\n\nWhat you can see is that the two columns, bw10 and bw100 from the first table have been stacked into rows in the second. Doing this allows us to create the two linked plots by faceting on the bandwidth variable, bw, using tm_facets(by = \"bw\"). The reverse operation to pivot_longer() is pivot_wider() – see ?pivot_wider.\n\n\n‘Pre-calculating’ the distance matrix\nIn the calculations above, the distances between the ward centroids that are used in the geographical weighting are calculated twice. First in gwss(wcape_wards_sp, vars = \"High_income\", bw = 10, ...) and then again in gwss(wcape_wards_sp, vars = \"High_income\", bw = 100, ...). Since those distances don’t actually change (the centroids are fixed so therefore are the distances between them) so we might have saved a little computational time by calculating the distance matrix in advance and then using it in the geographically weighted statistics. Curiously, though, it actually takes longer. I assume this is because the data set isn’t large enough to justify the extra step of saving the distances in a matrix and then passing that matrix to the gwss() function.\n\n\nCode\n# Time to do the calculations without pre-calculating the distance matrix\nsystem.time({\n  gwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw10 &lt;- gwstats$SDF$High_income_LM\n\n  gwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wcape_wards$bw100 &lt;- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n   0.11    0.00    0.11 \n\n\nCode\n# Time to do the calculations with the pre-calculated distance matrix\nsystem.time({\n  coords &lt;- st_coordinates(st_centroid(wcape_wards))\n  dmatrix &lt;- gw.dist(coords, longlat = T)\n  gwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 10,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw10 &lt;- gwstats$SDF$High_income_LM\n\n  gwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = 100,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wcape_wards$bw100 &lt;- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n   0.91    0.00    0.88"
  },
  {
    "objectID": "gwstats.html#selecting-the-bandwidth",
    "href": "gwstats.html#selecting-the-bandwidth",
    "title": "Geographically Weighted Statistics",
    "section": "Selecting the bandwidth",
    "text": "Selecting the bandwidth\nAs observed in the maps above, the geographically weighted statistics are a function of the geographical weighting that largely is controlled by the bandwidth. This raises the question of which is the correct bandwidth to use? Unfortunately, the most honest answer is that there is no correct answer, although an automatic bandwidth selection might be tried by calibrating the statistics around the local means.\nThe following uses a cross-validation approach,\n\n\nCode\nbw &lt;- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\n# The selected number of nearest neighbours:\nbw\n\n\n[1] 17\n\n\nwhereas the following uses an AIC corrected approach.\n\n\nCode\nbw &lt;- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\")\n\n\nAdaptive bandwidth (number of nearest neighbours): 256 AICc value: 1480.449 \nAdaptive bandwidth (number of nearest neighbours): 166 AICc value: 1473.901 \nAdaptive bandwidth (number of nearest neighbours): 110 AICc value: 1442.05 \nAdaptive bandwidth (number of nearest neighbours): 75 AICc value: 1398.784 \nAdaptive bandwidth (number of nearest neighbours): 54 AICc value: 1370.417 \nAdaptive bandwidth (number of nearest neighbours): 40 AICc value: 1345.457 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 1335.979 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1320.554 \nAdaptive bandwidth (number of nearest neighbours): 23 AICc value: 1314.985 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 1306.969 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \n\n\nCode\nbw\n\n\n[1] 20\n\n\nBoth use a golden-section search method and, if you look at the output, both iterate to a very similar solution in a very similar set of steps. Bandwidths of 17 and 20 have been suggested but, in practice, there is probably little between them so we may as well use the larger.\nThat automatic bandwidth selection applies only for the Western Cape wards, however. Recall earlier that the data were filtered (wards |&gt; filter(ProvinceNa == \"Western Cape\") -&gt; wcape_wards) with the partial explanation for doing so being to reduce run times. Another explanation is that there is no reason to assume that the spatial autocorrelation that is quantified by the bandwidth selection will be the same everywhere across the map. In fact, it varies from province to province:\n\n\nCode\nbandwidths &lt;- sapply(unique(wards$ProvinceNa), \\(x) {\n  wards |&gt;\n    filter(ProvinceNa == x) |&gt;\n    as_Spatial() %&gt;%\n    bw.gwr(High_income ~ 1, data = .,\n          adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n          approach = \"AIC\") %&gt;%\n    paste0(\"Bandwidth = \", .)\n})\n\n\n\n\nCode\nbandwidths\n\n\n       Free State           Limpopo        Mpumalanga      Western Cape \n\"Bandwidth = 126\"  \"Bandwidth = 25\"  \"Bandwidth = 95\"  \"Bandwidth = 20\" \n    KwaZulu-Natal      Eastern Cape        North West           Gauteng \n \"Bandwidth = 21\"  \"Bandwidth = 24\"  \"Bandwidth = 19\"  \"Bandwidth = 19\" \n    Northern Cape \n\"Bandwidth = 201\" \n\n\nThis suggests that fitting geographically weighted statistics to too large a study region is not desirable because there is little reason to presume that the same bandwidth should apply throughout it."
  },
  {
    "objectID": "gwstats.html#changing-the-interpolation-calculation-points",
    "href": "gwstats.html#changing-the-interpolation-calculation-points",
    "title": "Geographically Weighted Statistics",
    "section": "Changing the interpolation (calculation) points",
    "text": "Changing the interpolation (calculation) points\nSo far we have been using the ward centroids as the points for which the geographically weighted statistics are calculated. There is no requirement to do so as they could be interpolated at any point within this study region. To demonstrate this, let’s reselect the wards in the Western Cape and convert them into a raster grid using the stars package. Stars is an abbreviation of spatial-temporal arrays and can be used for handling raster (gridded) data, as in the following example, which is taken from this introduction to the package.\n\n\nCode\nif(!(\"stars\" %in% installed)) install.packages(\"stars\", dependencies = TRUE)\nrequire(stars)\nsat_image &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nsat_image &lt;- read_stars(sat_image)\nplot(sat_image, axes = TRUE)\n\n\n\n\n\nWe will use the st_rasterize function in stars to convert the geography of South Africa’s Western Cape into a regular grid.\n\n\nCode\nwcape_wards |&gt;\n  st_rasterize(nx = 100, ny = 100) |&gt;\n  st_as_sf() -&gt;\n  gridded\n\npar(mai=c(0,0,0,0))\ngridded |&gt;\n  st_geometry() |&gt;\n  plot()\n\n\n\n\n\nWe can now calculate the geographically weighted mean for each raster cell, with a bandwidth of 20 nearest neighbours.\n\n\nCode\ngridded_sp &lt;- as_Spatial(gridded)\n\ngwstats &lt;- gwss(wcape_wards_sp, gridded_sp, vars = \"High_income\",\n                bw = 20, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\ngridded$GWmean &lt;- gwstats$SDF$High_income_LM\n\nggplot(gridded, aes(fill = GWmean)) +\n  geom_sf(col = \"light grey\", size = 0) + \n                      # size is the width of the raster cell border\n  scale_fill_continuous_c4a_seq(palette = \"scico.lajolla\") +\n  theme_minimal()\n\n\n\n\n\n\nUsing geography to interpolate missing values\nThis ability to interpolate at any point within the study region provides a means to deal with missing values in the data. Contained in the wcape_wards data is a variable giving the average age of the population in each ward in 2011 but it contains 15 missing values:\n\n\nCode\nsummary(wcape_wards$age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.90   27.95   29.80   30.91   33.00   51.30      15 \n\n\nThe same observations also record NAs for the No_schooling variable.\n\n\nCode\nwhich(is.na(wcape_wards$age))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nCode\nwhich(is.na(wcape_wards$No_schooling))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nMissing value are a problem when fitting a regression model, for example. Usually they are simply omitted, as in the following case, where the output reports “(15 observations deleted due to missingness)”.\n\n\nCode\nols1 &lt;- lm(No_schooling ~ age, data = wcape_wards)\nsummary(ols1)\n\n\n\nCall:\nlm(formula = No_schooling ~ age, data = wcape_wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1998 -0.5384  1.7687 13.2843 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.69367    1.14121   17.26   &lt;2e-16 ***\nage         -0.38835    0.03657  -10.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.064 on 385 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.2265,    Adjusted R-squared:  0.2245 \nF-statistic: 112.8 on 1 and 385 DF,  p-value: &lt; 2.2e-16\n\n\n An alternative approach is to replace the missing values with a ‘safe’ alternative such as the mean age for the values that are not missing and the same for the percentages of the populations without schooling. However, we could also replace each missing value with a locally interpolated mean which fits with the geographical context.\nHere are the wards with missing age and no schooling values. These are the points that need to be interpolated.\n\n\nCode\nwcape_wards |&gt;\n  filter(is.na(age)) |&gt;\n  as_Spatial() -&gt;\n  missing\n\n\nThese are the wards with the values that serve as the data points.\n\n\nCode\nwcape_wards |&gt;\n  filter(!is.na(age)) |&gt;\n  as_Spatial() -&gt;\n  present\n\n\nFortunately, the missing values seem to be fairly randomly distributed across the study region. It would be a problem if they were all geographically clustered together because interpolating their values from their neighbours would not be successful if their neighbours’ values were also missing!\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nplot(present, border = \"light grey\")\nplot(missing, col = \"red\", add = T)\n\n\n\n\n\nWe can, then, interpolate the missing values from the present ones and match them into the data using base R’s match() function, matching on their WardID. Note that I have done this twice, once for the age variable and once for No_schooling. This is to allow for the possibility of them having differently sized bandwidths from each other (which they are when using approach = \"AIC\", although not, as it happens, with the default approach = \"CV\").\n\n\nCode\nbw &lt;- bw.gwr(age ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T,\n             approach = \"AIC\")\n\ngwstats &lt;- gwss(present, missing, vars = \"age\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch &lt;- match(missing$WardID, wcape_wards$WardID)\nwcape_wards$age[mch] &lt;- gwstats$SDF$age_LM\n\nbw &lt;- bw.gwr(No_schooling ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\ngwstats &lt;- gwss(present, missing, vars = \"No_schooling\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = T)\n\nwcape_wards$No_schooling[mch] &lt;- gwstats$SDF$No_schooling_LM\n\n\nIt is useful to keep a note of which values are interpolated, so…\n\n\nCode\nwcape_wards$interpolated &lt;- FALSE\nwcape_wards$interpolated[mch] &lt;- TRUE\n\n\nThere should be 15 of them.\n\n\nCode\ntable(wcape_wards$interpolated)\n\n\n\nFALSE  TRUE \n  387    15 \n\n\nNow returning to our regression model, there are, of course, no longer any missing values and, reassuringly, no evidence that the interpolated values are significantly different from the rest in either their mean No_schooling value or in their effect of age upon No_schooling.\n\n\nCode\nols2 &lt;- update(ols1, . ~ . + interpolated*age)\nsummary(ols2)\n\n\n\nCall:\nlm(formula = No_schooling ~ age + interpolated + age:interpolated, \n    data = wcape_wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1709 -0.5446  1.7484 13.2843 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          19.69367    1.12855  17.450   &lt;2e-16 ***\nage                  -0.38835    0.03617 -10.738   &lt;2e-16 ***\ninterpolatedTRUE     -1.72125   10.95102  -0.157    0.875    \nage:interpolatedTRUE  0.02817    0.34961   0.081    0.936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.03 on 398 degrees of freedom\nMultiple R-squared:  0.2285,    Adjusted R-squared:  0.2226 \nF-statistic: 39.28 on 3 and 398 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "gwstats.html#the-geographically-weighted-mean-as-a-low-pass-filer",
    "href": "gwstats.html#the-geographically-weighted-mean-as-a-low-pass-filer",
    "title": "Geographically Weighted Statistics",
    "section": "The geographically weighted mean as a low pass filer",
    "text": "The geographically weighted mean as a low pass filer\nThe geographically weighted mean acts as a smoothing (low pass) filter. We can see this if we apply it to a part of the satellite image from earlier. (If you try it on the whole image, you will be waiting a long time for the bandwidth to be calculated).\nFirst we shall extract band 1 from the image, create a blank raster using the raster package and assign it the same values as from the image.\n\n\nCode\nsat_image |&gt;\n  slice(band, 1) |&gt;\n  pull() -&gt; vals\n\nif(!(\"raster\" %in% installed)) install.packages(\"raster\", dependencies = TRUE)\n\ndim(sat_image)\n\n\n   x    y band \n 349  352    6 \n\n\nCode\nst_bbox(sat_image)\n\n\n     xmin      ymin      xmax      ymax \n 288776.3 9110728.8  298722.8 9120760.8 \n\n\nCode\nr &lt;- raster::raster(nrows = 352, ncol = 349,\n                      xmn = 288776.3, xmx = 298722.8,\n                      ymn = 9110728.8, ymx = 9120760.8)\nraster::crs(r) &lt;- \"EPSG:31985\"\n\n# The image is 349 (row) by 352 (col) whereas the raster is\n# 352 (col) by 349 (row) which is why the values are transposed, t(vals)\nr &lt;- raster::setValues(r, t(vals))\n\n\nNext we will crop out the top-left hand corner of the image, convert the coordinates and the attributes of those raster cells into a SpatialPointsDataFrame for use with the GWmodel functions and calculate the geographically weighted statistics:\n\n\nCode\nr1 &lt;- raster::crop(r, raster::extent(r, 1, 50, 1, 50))\npts &lt;- as(r1, \"SpatialPointsDataFrame\")\n\nbw &lt;- bw.gwr(layer ~ 1, data = pts,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F,\n             approach = \"AIC\")\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth (number of nearest neighbours): 1552 AICc value: 17665.9 \nAdaptive bandwidth (number of nearest neighbours): 967 AICc value: 17492.74 \nAdaptive bandwidth (number of nearest neighbours): 604 AICc value: 17293.36 \nAdaptive bandwidth (number of nearest neighbours): 381 AICc value: 17083.12 \nAdaptive bandwidth (number of nearest neighbours): 242 AICc value: 16875.38 \nAdaptive bandwidth (number of nearest neighbours): 157 AICc value: 16641.81 \nAdaptive bandwidth (number of nearest neighbours): 103 AICc value: 16366.74 \nAdaptive bandwidth (number of nearest neighbours): 71 AICc value: 16052.66 \nAdaptive bandwidth (number of nearest neighbours): 50 AICc value: 15616.22 \nAdaptive bandwidth (number of nearest neighbours): 38 AICc value: 15240.6 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 14739.11 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 14546.43 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 14003.78 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 13950.83 \nAdaptive bandwidth (number of nearest neighbours): 18 AICc value: 13851.75 \nAdaptive bandwidth (number of nearest neighbours): 18 AICc value: 13851.75 \n\n\nCode\ngwstats &lt;- gwss(pts, vars = \"layer\", bw = bw, kernel = \"bisquare\",\n             adaptive = TRUE, longlat = F)\n\n\nWe can now compare the original image with the smoothed image.\n\n\nCode\nr2 &lt;- raster::setValues(r1, gwstats$SDF$layer_LM)\nr &lt;- st_as_stars(raster::addLayer(r1, r2))\n\nggplot() + \n  geom_stars(data = r) +\n  facet_wrap(~ band) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n\n\n\n\n\nHowever, it isn’t only the geographically weighted mean that is calculated. You could use the geographically weighted standard deviation, for example, as a high pass filter – a form of edge detection.\n\n\nCode\nr2 &lt;- raster::setValues(r1, gwstats$SDF$layer_LSD)\nr &lt;- st_as_stars(r2)\n\nggplot() + \n  geom_stars(data = r) +\n  coord_equal() +\n  theme_void() +\n  scale_fill_continuous_c4a_seq(name = \"\", palette = \"scico.oslo\")\n\n\n\n\n\n\nYou might spot that I have not loaded (required) the raster package in the code above but have accessed its functions via the :: notation; for example raster::setValues(). That is because if I do load the package, its select function will then mask a different function but with the same name in tidyverse’s dplyr. As I am only using the raster package very briefly, I didn’t think it was worth the potential confusion."
  },
  {
    "objectID": "gwstats.html#geographically-weighted-correlation",
    "href": "gwstats.html#geographically-weighted-correlation",
    "title": "Geographically Weighted Statistics",
    "section": "Geographically weighted correlation",
    "text": "Geographically weighted correlation\nAccording to the regression model earlier, there is a negative correlation between the age of the population and the percentage without schooling. The correlation across the Western Cape is,\n\n\nCode\ncor(wcape_wards$No_schooling, wcape_wards$age)\n\n\n[1] -0.4756981\n\n\nThat is, however, the global correlation for what appears (below) to be a heteroscedastic relationship.\n\n\nCode\nggplot(data = wcape_wards, aes(x = age, y = No_schooling)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship so let’s consider that by calculating and mapping the geographically weighted correlations between the two variables.\n\n\nCode\nwcape_wards_sp &lt;- as_Spatial(wcape_wards)\n\nbw &lt;- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats &lt;- gwss(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwcape_wards$Corr_No_schooling.age &lt;- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_continuous_c4a_seq(name = \"Correlation\", palette = \"hcl.blues3\",\n                                reverse = TRUE) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\nThe local correlations range -0.868 to -0.105, with an interquartile range from -0.714 to -0.451:\n\n\nCode\nsummary(wcape_wards$Corr_No_schooling.age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.8679 -0.7136 -0.6294 -0.5801 -0.4506 -0.1054 \n\n\nIf we add a little ‘cartographic know-how’ from an earlier session, we can identify that the correlation is stronger in and around Parow than it is in and around Blue Downs, for example.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\",\n                                                 dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") |&gt;\n  filter(place == \"city\" | place == \"town\") |&gt;\n  st_join(wcape_wards) |&gt;\n    # A spatial spatial join,\n    # giving the point places the attributes of the wards they fall in\n  mutate(population = as.integer(population)) |&gt;\n  filter(!is.na(ProvinceNa) & population &gt; 60000) -&gt;\n  places\n\nlast_plot() +\n  geom_sf_label_repel(data = places, aes(label = name), alpha = 0.7,\n                      force = 5, size = 2, max.overlaps = 20) +\n  xlab(element_blank()) +\n  ylab(element_blank())\n\n\n\n\n\nIn general, the correlation appears to be strongest in cities:\n\n\nCode\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %&gt;%\n  st_join(wcape_wards) %&gt;%\n  st_drop_geometry %&gt;%\n  group_by(place) %&gt;%\n  summarise(meancorr = mean(Corr_No_schooling.age, na.rm = TRUE)) %&gt;%\n  arrange(meancorr)\n\n\n# A tibble: 5 × 2\n  place             meancorr\n  &lt;chr&gt;                &lt;dbl&gt;\n1 city                -0.855\n2 hamlet              -0.620\n3 isolated_dwelling   -0.572\n4 village             -0.555\n5 town                -0.514"
  },
  {
    "objectID": "gwstats.html#statistical-inference-and-significance",
    "href": "gwstats.html#statistical-inference-and-significance",
    "title": "Geographically Weighted Statistics",
    "section": "Statistical inference and significance",
    "text": "Statistical inference and significance\nWe can use a randomisation procedure to determine whether any of the local summary statistics may be said to be significantly different from those obtained by chance. The randomisation procedure is found in the function, gwss.montecarlo() with a default number of nsim = 99 simulations. This isn’t very many but they are time-consuming to calculate and will be sufficient to demonstrate the process.\nThe following code chunk returns to the local mean percentages of high earners. It goes through the complete process of determining a bandwidth using the bw.gwr() function, then calculating the local and geographically weighted statistics using gwss(), determining the p-values under randomisation, using gwsss.montecarlo, then mapping the results. Very few of the results would be adjudged significant but a few are.\n\nCalculating the p-values under randomisation takes some time so please be patient.\n\n\nCode\nbw &lt;- bw.gwr(High_income ~ 1, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\ngwstats &lt;- gwss(wcape_wards_sp , vars = \"High_income\", bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values &lt;- gwss.montecarlo(wcape_wards_sp, vars = \"High_income\", bw = bw,\n                            kernel = \"bisquare\",\n                            adaptive = TRUE, longlat = T)\np.values &lt;- as.data.frame(p.values)\n\nwcape_wards$High_income_LM &lt;- gwstats$SDF$High_income_LM\nwcape_wards$High_income_LM[p.values$High_income_LM &gt; 0.025 &\n                     p.values$High_income_LM &lt; 0.975] &lt;- NA\n\nggplot(wcape_wards, aes(fill = High_income_LM)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", na.value = \"light grey\",\n                       direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Local mean percentage of higher earrners\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\nA second example considers the local correlations between the No_schooling and age variables. Again, most of the correlations are insignificant but with a few exceptions.\n\n\nCode\nbw &lt;- bw.gwr(No_schooling ~ age, data = wcape_wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\n\n\nCode\ngwstats &lt;- gwss(wcape_wards_sp , vars = c(\"No_schooling\", \"age\"), bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\np.values &lt;- gwss.montecarlo(wcape_wards_sp, vars = c(\"No_schooling\", \"age\"),\n                bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\np.values &lt;- as.data.frame(p.values)\n\nwcape_wards$Corr_No_schooling.age &lt;- gwstats$SDF$Corr_No_schooling.age\n\nwcape_wards$Corr_No_schooling.age[p.values$Corr_No_schooling.age &gt; 0.025 &\n                            p.values$Corr_No_schooling.age &lt; 0.975] &lt;- NA\n\nggplot(wcape_wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\",\n                       na.value = \"light grey\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\n\nWe have the problem of repeating testing that we also had when looking for spatial hotspots in the session about spatial autocorrelation. The function p.adjust() could be used but we may wonder, as previously, where the methods are too conservative."
  },
  {
    "objectID": "gwstats.html#improving-the-leigibility-of-the-map",
    "href": "gwstats.html#improving-the-leigibility-of-the-map",
    "title": "Geographically Weighted Statistics",
    "section": "Improving the leigibility of the map",
    "text": "Improving the leigibility of the map\n\nUsing a map insert\nBefore for completing this session there is value in addressing the problem that some parts of the map are so small that their contents are almost illegible (too small to be seen). The traditional and probably the most effective way to address this is to add one or more inserts to the map to magnify the small areas. One way to do this is by using the cowplot package with ggplot2. The following example is based on this tutorial and maps the percentages of the populations who are relatively high earners in each ward.\nFirst, we need to install and require the cowplot package.\n\n\nCode\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\", dependencies = TRUE)\nrequire(cowplot)\n\n\nSecond, the part of the wards map to be included in the map insert is extracted, using st_crop.\n\n\nCode\nwards_extract &lt;- st_crop(wcape_wards, xmin = 18.2, xmax = 19, ymin = -34.3,\n                         ymax = -33.5)\nplot(st_geometry(wards_extract))\n\n\n\n\n\nThe bounding box for the extracted area is also obtained as it will be used as a feature in the final map (it will be drawn as a rectangle showing the geographical extent of the insert in the original map).\n\n\nCode\nbbox &lt;- st_as_sfc(st_bbox(wards_extract))\n\n\nNext, the main part of the map is created…\n\n\nCode\nmain_map &lt;- ggplot(wcape_wards, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\nplot(main_map)\n\n\n\n\n\n…as is the map insert:\n\n\nCode\ninsert &lt;- ggplot(wards_extract, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_binned_c4a_seq(palette = \"hcl.blues3\") +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_void() +\n  theme(legend.position = \"none\")\nplot(insert)\n\n\n\n\n\nFinally, the maps are brought together using cowplot’s ggdraw() and draw_plot() functions. We know that some of the values it shows are not necessarily significant under randomisation but we will include them here to just to get\n\n\nCode\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(insert, x = 0, y = 0.25, scale = 0.22) +\n  draw_label(\"Cape Town\\nand\\nsurrounding areas\", x = 0.62, y = 0.78, size = 8)\n\n\n\n\n\n\nThe positioning of the map insert (x = ... & y = ...), the scaling of it, and the size and position of the label were all found by trial and error, using various values until I was happy with the results.\n\n\nUsing a ‘balanced carogtram’\nAnother approach is to use what has been described as a visually balanced cartogram. A cartogram – of which there are lots of interesting examples on this website – usually works by distorting a map in a way that rescales the areas in proportion not to their physical size but by some other measured attribute such as their population count. Much of the motivation for this lies in political studies and mapping the results of an election wherein a traditional map can give a very distorted view of the outcome because of how much population density varies across a country. In the example below, rescaling the areas by population correctly shows that the 2020 US Presidential was not a Republican landslide, despite Trump’s baseless accusations!\n Source: www.viewsoftheworld.net\nThe problem with cartograms is the distortion. Essentially, they are trading one visual problem (invisibility of small areas) for another (the amount of geographical distortion). The idea of a balanced cartogram comes from an awareness that sometimes it is sufficient just to make the small areas bigger and/or the big areas smaller, without causing too much geographical distortion. One way to achieve this is to scale the places by the square root of the original areas, as in the following example.\n\n\nCode\nif(!(\"cartogram\" %in% installed)) install.packages(\"cartogram\",\n                                                   dependencies = TRUE)\nrequire(cartogram)\n\n# Convert from longitude/latitude to a grid projection\nwcape_wards %&gt;%\n  st_transform(22234) %&gt;%\n  mutate(area = as.numeric(st_area(.)),\n         w = as.numeric(sqrt(area))) -&gt;\n  wards_prj\n\n# Create the cartogram -- here a contiguous cartogram is used, see ?cartogram\nwards_carto &lt;- cartogram_cont(wards_prj, weight = \"w\", maxSizeError = 1.4,\n                              prepare = \"none\")\n\nggplot(wards_carto, aes(fill = High_income)) +\n  geom_sf(col = \"white\", size = 0.1) +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title =\n      \"Cartogram of the percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\n\n\n\n\n\nThe results are ok, drawing attention to the wards with higher percentages of higher earners but clearly there is geographical distortion in the map, too.\n\n\nUsing a hexogram\nThe final approach to be considered here is what has been described as hexograms – a combination of tile maps and cartograms that make small areas bigger on the map but try and limit the geographic distortion. The original method also preserved typology (i.e. contiguous neighbours remained so in the final map) but is dated and very slow to operationalise as it was really just a proof of concept. A faster method but one that cannot guarantee to preserve typology is presented below.\nFirst, we need some functions to create the hexogram.\n\n\nCode\nif(!(\"cartogram\" %in% installed.packages()[,1])) install.packages(\"cartogram\")\nif(!(\"sf\" %in% installed.packages()[,1])) install.packages(\"sf\")\nif(!(\"lpSolve\" %in% installed.packages()[,1])) install.packages(\"lpSolve\") \n\nrequire(sf)\n\ncreate_carto &lt;- \\(x, w = NULL, k = 2, itermax = 25, maxSizeError = 1.4) {\n  if(class(x)[1] != \"sf\") stop(\"Object x is not of class sf\")\n  if(is.null(w)) x$w &lt;- as.numeric(st_area(x))^(1/k)\n  cartogram::cartogram_cont(x, \"w\", itermax = itermax,\n                            maxSizeError = maxSizeError, prepare = \"none\")\n}\n\n\ncreate_grid &lt;- \\(x, m = 6) {\n  bbox &lt;- st_bbox(x)\n  ydiff &lt;- bbox[4] - bbox[2]\n  xdiff &lt;- bbox[3] - bbox[1]\n  \n  n &lt;- m * nrow(x)\n  ny &lt;- sqrt(n / (xdiff/ydiff))\n  nx &lt;- n / ny\n  nx &lt;- ceiling(nx)\n  ny &lt;- ceiling(ny)\n  \n  grd &lt;- st_sf(st_make_grid(x, n = c(nx, ny), square = FALSE))\n}\n\n\ngrid_data &lt;- \\(x, grd, k = 2) {\n  x_pts &lt;- st_centroid(st_geometry(x), of_largest_polygon = TRUE)\n  grd_pts &lt;- st_centroid(grd)\n  \n  cost.mat &lt;- st_distance(x_pts, grd_pts)^k\n  row.rhs &lt;- rep(1, length(x_pts))\n  col.rhs &lt;- rep(1, nrow(grd_pts))\n  row.signs &lt;- rep(\"=\", length(x_pts))\n  col.signs &lt;- rep(\"&lt;=\", nrow(grd_pts))\n\n  optimisation &lt;- lpSolve::lp.transport(cost.mat, \"min\", row.signs, row.rhs,\n                                        col.signs, col.rhs)$solution\n  \n  mch &lt;- sapply(1:nrow(optimisation), \\(i) which(optimisation[i, ] == 1))\n  grd &lt;- st_sf(grd[mch,])\n  cbind(grd, st_drop_geometry(x))\n}\n\n\ncreate_layers &lt;- \\(x, grd) {\n  \n  area &lt;- sapply(1: nrow(x), \\(i) {\n    y &lt;- st_intersects(x[i, ], grd)[[1]]\n    if(i %in% y) {\n      if(length(y) == 1) return(st_area(x[i, ]))\n      if(length(y) &gt; 1) {\n        area &lt;- st_area(x[i, ])\n        overlaps &lt;- y[-(y == i)]\n        area &lt;- area - sum(st_area(st_intersection(st_geometry(x[i, ]),\n                                                st_geometry(grd[overlaps, ]))))\n        return(area) \n      }\n    } else {\n      return(0)\n    }\n  })\n\n  i &lt;- which(area &gt; 2*as.numeric(st_area(grd))[1])\n  list(x[i, ], grd[-i, ])\n}\n\n\nNext, we run through the stages of its creation, which are to start with a balanced cartogram (create_carto()), then overlay that with a grid of hexagons (create_grid()), next assign places in the original map to a hexagon (grid_data()), then plot the results as two layers (create_layers()) to give a mash-up of the cartogram and the hexagons.\n\n\nCode\nwards_carto &lt;- create_carto(wards_prj)\ngrd &lt;- create_grid(wards_carto)\nwards_grid &lt;- grid_data(wards_carto, grd)\nwards_layers &lt;- create_layers(wards_carto, wards_grid)\n\nggplot() +\n  geom_sf(data = wards_layers[[1]], aes(fill = High_income),\n          col = \"white\", size = 0.4) +\n  geom_sf(data = wards_layers[[2]], aes(fill = High_income),\n          col = \"dark grey\") +\n  scale_fill_binned_c4a_seq(name = \"%\", palette = \"hcl.blues3\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"Hexogram % of the population who are higher earners\",\n    subtitle = \"South African Western Cape (2011)\"\n  )\n\n\n\n\n\nI find the result quite visually appealing and it isn’t just me: a separate study has provided empirical evidence for the value of balanced cartograms and hexograms as a visualisation tool mapping spatial distributions."
  },
  {
    "objectID": "gwstats.html#summary",
    "href": "gwstats.html#summary",
    "title": "Geographically Weighted Statistics",
    "section": "Summary",
    "text": "Summary\nThis session has introduced the concept of geographically weighted statistics to examine spatial heterogeneity in a measured variable and to allow for the possibility that its strength (and direction) of correlation with another variable varies from place-to-place. As such, these statistics are a form of local statistic, which is to say they can vary across the map. Sometimes, the parts of the map that we are interested in are small in relation to other parts, creating a problem of invisibility/legibility. Maps inserts, balanced cartograms and hexograms have been introduced as a means to address this visualisation problem."
  },
  {
    "objectID": "gwstats.html#futher-reading",
    "href": "gwstats.html#futher-reading",
    "title": "Geographically Weighted Statistics",
    "section": "Futher reading",
    "text": "Futher reading\nGollini I, Lu B, Charlton M, Brunsdon C & Harris P (2015). GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. Journal of Statistical Software, 63(17), 1–50. https://doi.org/10.18637/jss.v063.i17\n\nBrunsdon C, Comber A, Harris P, Rigby J & Large A (2021). In memoriam: Martin Charlton. Geographical Analysis, 54(4), 713–4. https://doi.org/10.1111/gean.12309"
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "",
    "text": "(Draft version)"
  },
  {
    "objectID": "multilevel.html#introduction",
    "href": "multilevel.html#introduction",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Introduction",
    "text": "Introduction\nGeographically Weighted Regression, which we looked at in the previous session, treats geographic space as a continuous spatial variable in the sense that regression relationships can vary from one location to another across a geographic study region. That continuous view of space is clear if we fit a simple ‘null model’ of the COVID-19 rates in London in the week before Christmas 2021 and map the results. It is a null model because it includes no predictor variables other than the local estimate of the intercept (i.e. the local mean rate of COVID-19). What is clear is how that local estimate is allowed to vary across geographic space.\n(Download and extract the data for London and fit the GWR model)\n\n\nCode\nrequire(sf)\nrequire(ggplot2)\nrequire(tidyverse)\nrequire(GWmodel)\n\nif(!file.exists(\"covid_xmas_2021.geojson\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_xmas_2021.geojson\", \"covid_xmas_2021.geojson\", mode = \"wb\", quiet = TRUE)\nxmas_covid &lt;- read_sf(\"covid_xmas_2021.geojson\")\n\nxmas_covid %&gt;% \n  filter(regionName == \"London\") -&gt;\n  ldn_covid\n\nldn_covid_sp &lt;- as_Spatial(ldn_covid)\nbw &lt;- bw.gwr(rate ~ 1, data = ldn_covid_sp, adaptive = TRUE)\ngwrmod &lt;- gwr.basic(rate ~ 1, data = ldn_covid_sp, bw = bw, adaptive = TRUE)\n\n\n(Map the predicted COVID-19 rates across London based on the spatially varying local mean estimate)\n\n\nCode\nldn_covid$yhat &lt;- gwrmod$SDF$yhat\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nMultilevel models, by contrast, have more of a discrete view of space because observations at some base level are treated as members of a group at a second, more aggregate level. In a geographic context, the base level could be neighbourhoods and the groups could be the local authorities to which the neighbourhoods belong. This more discrete view of space becomes clear if we fit a very simple (too simple, really) null model of the COVID-19 rates in London, using the local authority estimates from this multilevel model as the nearest equivalent to the local means of the GWR model.\n\n\nCode\nrequire(lme4)\nmlm &lt;- lmer(rate ~ (1|LtlaName), data = ldn_covid)\nldn_covid$yhat &lt;- predict(mlm, re.form = ~ (1|LtlaName))\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nIf we compare the map above with that produced from the GWR estimates then we find that there are commonalities in the two maps but the multilevel model clearly has the more bounded view of geographic space."
  },
  {
    "objectID": "multilevel.html#multilevel-models",
    "href": "multilevel.html#multilevel-models",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Multilevel models",
    "text": "Multilevel models\nThe package that we are using to fit the models here is lme4. The formula rate ~ (1|LtlaName) means that we are modelling the COVID-19 rate against a random intercept that varies by local authority (by local authority name, LtlaName). It is not the only way of fitting multilevel models in R (an alternative, for example, is brms) but it is sufficient for the purposes of this exercise.\nThe more discrete and bounded view of geographic space that the multilevel model (in its simplest form, at least) employs seems quite restricting – in this case study it is suggesting that the causes of COVID-19 are somehow related to ‘things’ that happen or have consequences at a local authority scale. The implication is that the boundaries of local authorities are relevant to the geographical causes and rates of COVID-19. This is not an assumption that GWR makes so why use multilevel models instead?\nOne answer is that a multilevel model is often faster to fit. Consider the following example, which extends the study region to London and the South East.\n(Fit the GWR model and the multilevel model)\n\n\nCode\nxmas_covid %&gt;%\n  filter(regionName == \"London\" | regionName == \"South East\") -&gt;\n  ldn_se_covid\nldn_se_covid_sp &lt;- as_Spatial(ldn_se_covid)\nt1 &lt;- Sys.time()\nbw &lt;- bw.gwr(rate ~ 1, data = ldn_se_covid_sp, adaptive = TRUE)\ngwrmod &lt;- gwr.basic(rate ~ 1, data = ldn_se_covid_sp, bw = bw, adaptive = TRUE)\ngwrt &lt;- Sys.time() - t1\nt2 &lt;- Sys.time()\nmlm &lt;- lmer(rate ~ (1|LtlaName), data = ldn_se_covid)\nmlmt &lt;- Sys.time() - t2\n\n\nThe computation times are,\n\n\nCode\ngwrt\n\n\nTime difference of 7.035109 secs\n\n\nCode\nmlmt\n\n\nTime difference of 0.06526804 secs\n\n\nAdmittedly, neither took very long to fit, with the GWR taking 0.12 minutes and the multilevel model (MLM) taking 0.07 seconds on my laptop. However, these differences become more noticeable with more complex models or larger study regions. The multilevel model is estimating far fewer model parameters than the GWR. This is because the GWR is really lots of models that are fitted separately (one for each location) and then compared, whereas the multilevel model is one model but one which allows for variance at the geographic level(s) of the model, so better local authorities, for example. Put another way, the GWR is a series of local models that are compared, whereas the multilevel model is a global model that allows for local variations around it: at the moment, all it is doing is estimating the average COVID rate for the whole study region but also recognising that different local authorities have rates that vary around the overall average.\nSecond, multilevel models can be used – as their name suggests! – to fit multi-level models to examine the scale of the geographic pattern of a variable. Consider the following example, which fits a multilevel model to the COVID-19 rates of all English neighbourhoods in the week before Christmas 2021. There are now three levels in the model, which are the base neighbourhoods level, then the local authorities in which the neighourhoods are located, and then the English regions to which the local authorities belong. Hence, three levels and three simultaneous scales of analysis: neighbourhoods, local authorities and regions. Although the resulting map suffers a little from the ‘invisibility problem’ of some small areas within it, the higher COVID-19 rate in parts of London and the South East is evident (but not solely confined to these regions, as there is also a high rate evident in the North West).\n\n\nCode\nmlm &lt;- lmer(rate ~ (1|LtlaName) + (1|regionName), data = xmas_covid)\nxmas_covid$yhat &lt;- predict(mlm, re.form = ~ (1|LtlaName) + (1|regionName))\nggplot(data = xmas_covid, aes(fill = yhat)) +\n  geom_sf(col = NA) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1) +\n  theme_minimal()\n\n\n\n\n\nHaving fitted a model with three geographic levels, the geographical question that we can now ask is which of the levels of the model contributes most to the geographical patterning of the disease. Do we get most variation between neighbourhoods (suggesting the geographic pattern is mostly at the neighbourhood level), between local authorities (so a local authority patterning) or between regions (evidence of strongest regional differences)? We can answer the question by looking at how much of the variance belongs to each level, which we can see in the summary under Random effects.\n\n\nCode\nsummary(mlm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rate ~ (1 | LtlaName) + (1 | regionName)\n   Data: xmas_covid\n\nREML criterion at convergence: 2858.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3127 -0.6222 -0.0334  0.5832  6.8771 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n LtlaName   (Intercept) 0.08043  0.2836  \n regionName (Intercept) 0.14923  0.3863  \n Residual               0.07714  0.2777  \nNumber of obs: 6789, groups:  LtlaName, 315; regionName, 9\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)     1.26       0.13   9.687\n\n\nFrom these summary values we can work out the percentage of the variance which is at each level, calculating what is known as the intraclass correlation (ICC). For this particular week of the pandemic it is,\n\n\nCode\nsummary(mlm)$varcor %&gt;%\n  as_tibble %&gt;%\n  select(grp, vcov) %&gt;%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThe ICC appears to suggest the presence of regional effects, in particular (because the ICC for regionName is greater than the ICC for any other level). These regional effects, which are treated as random effects in the model, can be examined using the code below, from which they are found to be strongest for London. This means that the COVID-19 rate for this week is higher for London that it is for any other region.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %&gt;%\n  as_tibble %&gt;%\n  arrange(desc(condval))\n\n\n# A tibble: 9 × 5\n  grpvar     term        grp                      condval condsd\n  &lt;chr&gt;      &lt;fct&gt;       &lt;fct&gt;                      &lt;dbl&gt;  &lt;dbl&gt;\n1 regionName (Intercept) London                    0.814  0.0505\n2 regionName (Intercept) East of England           0.236  0.0433\n3 regionName (Intercept) North West                0.181  0.0463\n4 regionName (Intercept) South East                0.0644 0.0356\n5 regionName (Intercept) East Midlands            -0.0688 0.0463\n6 regionName (Intercept) West Midlands            -0.189  0.0527\n7 regionName (Intercept) Yorkshire and The Humber -0.241  0.0626\n8 regionName (Intercept) North East               -0.389  0.0816\n9 regionName (Intercept) South West               -0.407  0.0536\n\n\nOf course, somewhere has to be highest so the fact the it is highest for London does not mean the difference is necessarily statistically significant. However, as it turns out, it is significant (significantly different from zero), which we can see if we plot what is called a caterpillar plot in the multilevel literature, drawn here with a 95% confidence interval around each of the regional effects.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %&gt;%\n  as_tibble %&gt;%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %&gt;%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nNot only is London’s regional ‘uplift’ in the COVID rates significantly greater than zero, London’s regional effect is greater than for any other regions. The following chart suggests that London is different from the rest because its confidence interval does not overlap with any others. Note that if you are testing to see if two places differ from each other as opposed to from zero then you need to shorten the confidence interval before looking to see if they overlap. As a rule of thumb, the 95% confidence interval for a test of difference in two values is 1.39 \\(\\times\\) the standard error of the parameter estimate instead of the more usual 1.96.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %&gt;%\n  as_tibble %&gt;%\n  mutate(lwr = condval - 1.39 * condsd,\n         upr = condval + 1.39 * condsd) %&gt;%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nDespite the clear evidence of regional differences that are driven by London’s much higher COVID-19 rate in the week before Christmas 2021, keep in mind that not all the pattern in the COVID-19 rates is at the regional level. To recall,\n\n\nCode\nsummary(mlm)$varcor %&gt;%\n  as_tibble %&gt;%\n  select(grp, vcov) %&gt;%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThere are, for example, differences between the local authorities, too, at the next level down in the model. These are evident from a caterpillar plot for the authorities although because there are more of these authorities than there are regions (315 authorities Vs rlength(unique(xmas_covid$regionName))` regions, individual ones are harder to discern.\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %&gt;%\n  as_tibble %&gt;%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %&gt;%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_blank()) +\n           xlab(\"Local authorities\")\n\n\n\n\n\nLooking at the ‘top ten’ with COVID-19 rates higher than the national and their regional averages would predict are,\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %&gt;%\n  as_tibble %&gt;%\n  arrange(desc(condval)) %&gt;%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %&gt;%\n  slice_max(condval, n = 10)\n\n\n# A tibble: 10 × 7\n   grpvar   term        grp                  condval condsd   lwr   upr\n   &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;                  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 LtlaName (Intercept) Thurrock               0.849 0.0746 0.702 0.995\n 2 LtlaName (Intercept) Dartford               0.688 0.0814 0.528 0.847\n 3 LtlaName (Intercept) Bristol, City of       0.679 0.0645 0.553 0.805\n 4 LtlaName (Intercept) Lambeth                0.674 0.0675 0.541 0.806\n 5 LtlaName (Intercept) Manchester             0.666 0.0584 0.551 0.780\n 6 LtlaName (Intercept) Elmbridge              0.643 0.0722 0.502 0.785\n 7 LtlaName (Intercept) Reigate and Banstead   0.587 0.0722 0.446 0.729\n 8 LtlaName (Intercept) Epsom and Ewell        0.582 0.0937 0.398 0.765\n 9 LtlaName (Intercept) Salford                0.567 0.0671 0.435 0.698\n10 LtlaName (Intercept) Gedling                0.554 0.0820 0.393 0.714\n\n\nThe key point here is that the multilevel model is useful to help unpack at what scale(s) geographic outcomes are arising and to indentify some of the key places driving those outcomes."
  },
  {
    "objectID": "multilevel.html#spatial-varying-coefficient-effects",
    "href": "multilevel.html#spatial-varying-coefficient-effects",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Spatial varying coefficient effects",
    "text": "Spatial varying coefficient effects\nThe mutilevel models fitted above are random intercepts models – only the intercept term is permitted to vary from place to place. Under these models, some places are expected to have a higher or lower average COVID-19 rate than others but that rate is not affected by any predictor variables that can also very in their effects from place-to-place.\nImagine that the percentage of the population of secondary school age is a factor in the COVID-19 rates in London ahead of Christmas 2021. It is a very simple model but it appears to have some (limited) explanatory power. As it happens, more children of school age appears to be less associated with COVID, perhaps because they and the families had already caught it?\n\n\nCode\nols &lt;- lm(rate ~ age12.17, data = ldn_covid_sp)\nsummary(ols)\n\n\n\nCall:\nlm(formula = rate ~ age12.17, data = ldn_covid_sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11561 -0.29813 -0.00225  0.27856  1.20720 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.629006   0.058593  44.869   &lt;2e-16 ***\nage12.17    -0.076615   0.008245  -9.292   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4111 on 980 degrees of freedom\nMultiple R-squared:  0.08097,   Adjusted R-squared:  0.08003 \nF-statistic: 86.34 on 1 and 980 DF,  p-value: &lt; 2.2e-16\n\n\nA geographically weighted regression suggests that there might be spatial variation in the relationship between age12.17 and the COVID rate. In truth, not all of these local estimates are necessarily significant but let us ignore that for the moment.\n(Fit the GWR model allowing for spatial variation in the effects of age12.17 on rate)\n\n\nCode\nrequire(GWmodel)\nbw &lt;- bw.gwr(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE)\n\n\nAdaptive bandwidth: 614 CV score: 124.8849 \nAdaptive bandwidth: 387 CV score: 113.7512 \nAdaptive bandwidth: 246 CV score: 103.5928 \nAdaptive bandwidth: 159 CV score: 94.75184 \nAdaptive bandwidth: 105 CV score: 88.35261 \nAdaptive bandwidth: 72 CV score: 84.00381 \nAdaptive bandwidth: 51 CV score: 81.04989 \nAdaptive bandwidth: 38 CV score: 79.4773 \nAdaptive bandwidth: 30 CV score: 79.26434 \nAdaptive bandwidth: 25 CV score: 79.3698 \nAdaptive bandwidth: 33 CV score: 79.28656 \nAdaptive bandwidth: 28 CV score: 79.23342 \nAdaptive bandwidth: 27 CV score: 79.22796 \nAdaptive bandwidth: 26 CV score: 79.35989 \nAdaptive bandwidth: 27 CV score: 79.22796 \n\n\nCode\ngwrmod &lt;- gwr.basic(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE, bw = bw)\n\n\n(Map the results)\n\n\nCode\nldn_covid$age12.17GWR &lt;- gwrmod$SDF$age12.17\nggplot(data = ldn_covid, aes(fill = age12.17GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n  theme_minimal() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nThe closest multilevel model to the above GWR model (given the current data) is one that allows the effects of age12.17 to vary by local authority (LtlaName).\n(fit the model)\n\n\nCode\nmlm &lt;- lmer(rate ~ (age12.17|LtlaName), data = ldn_covid)\n\n\n(plot the results)\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %&gt;%\n  as_tibble %&gt;%\n  filter(term == \"age12.17\") %&gt;%\n  rename(LtlaName = grp,\n         age12.17LM = condval) %&gt;%\n  select(LtlaName, age12.17LM) %&gt;%\n  inner_join(ldn_covid, ., by = \"LtlaName\") %&gt;%\n  ggplot(aes(fill = age12.17LM)) +\n    geom_sf(size = 0.25) +\n    scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n    theme_minimal() +\n    guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nAgain, there are commonalities in the GWR and MLM estimates but the differing conceptions of the structure of geographic space and the spatial relationships it contains makes a difference to the results."
  },
  {
    "objectID": "multilevel.html#more-to-be-added",
    "href": "multilevel.html#more-to-be-added",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "More to be added!",
    "text": "More to be added!\nThis session is incomplete. For now, the key point is that GWR and multilevel models conceptualise space in different ways. I am not meaning to imply that multilevel models are worse, as they can be very useful and flexible. It useful to note that there are now methods that intgrate spatial regression models with multilevel models (see, for example, here and GWR with hierarchical group structures (for example, this and this but these are pretty advanced methods of analysis!"
  },
  {
    "objectID": "multilevel.html#further-reading",
    "href": "multilevel.html#further-reading",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Further Reading",
    "text": "Further Reading\nMultilevel models – an introduction to multilevel models from the Handbook of Spatial Analysis in the Social Sciences."
  }
]