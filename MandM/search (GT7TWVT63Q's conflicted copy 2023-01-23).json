[
  {
    "objectID": "gwstats.html",
    "href": "gwstats.html",
    "title": "Geographically Weighted Statistics",
    "section": "",
    "text": "In the previous session we looked at identifying and measuring patterns of spatial autocorrelation (clustering) in data. If those patterns exist then there is potential to use them to our advantage by ‘pooling’ the data for geographical sub-spaces of the map, creating local summary statistics for those various parts of the map, and then comparing those statistics to look for spatial variation (heterogeneity) across the map and in the data. The method we shall use here is found in GWmodel – an R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. These are geographically weighted statistics."
  },
  {
    "objectID": "gwstats.html#geographical-weighted-statistics",
    "href": "gwstats.html#geographical-weighted-statistics",
    "title": "Geographically Weighted Statistics",
    "section": "Geographical Weighted Statistics",
    "text": "Geographical Weighted Statistics\nThe idea behind geographically weighted statistics is simple. Instead of calculating the (global) mean average, for example, for the all the data and the whole map, a series of (local) averages are calculated for various sub-spaces within the map.\nImagine a point location, \\(i\\), at position \\((u_i, v_i)\\) on the map. To calculate the local statistic, first find either the \\(k\\) nearest neighbours to \\(i\\) or all of those within a fixed distance, \\(d\\), from it. Second, to add the geographical weighting in the name of the statistics, apply a weighting scheme whereby the neighbours nearest to \\(i\\) have most weight in the subsequent calculations and the weights decrease with increasing distance from \\(i\\), becoming zero at the \\(k\\)th nearest neighbour or at the distance threshold, \\(d\\). Third, calculate, for the point and its neighbours, the weighted mean value (or some other summary statistic) of a variable, using the inverse distance weighting in the calculation. Fourth, repeat the process for other points on the map. This means that if there are \\(n\\) points of calculation then there will be \\(n\\) geographically weighted mean values calculated across the map. These can be then be compared to look for spatial variation.\nLet’s see this in action, beginning by ensuring the necessary packages are installed and required.\n\n\nCode\ninstalled <- installed.packages()[,1]\nrequired <- c(\"GWmodel\", \"sf\", \"tidyverse\", \"tmap\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE, repos = \"https://cloud.r-project.org\")\n\nrequire(GWmodel)\nrequire(sf)\nrequire(tidyverse)\nrequire(tmap)\n\n\nWe will use the same data as in the previous session but confine the analysis to the Western Cape of South Africa. This is largely to reduce run times but there is another reason that I shall return to presently.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nwards %>%\n  filter(ProvinceNa == \"Western Cape\") ->\n  wards\n\n\nThe calculated points for the geographically weighted statistics will be the ward centroids. A slight complication here is that GWmodel is built around the elder sp not sf formats for handling spatial data in R so the wards need to be converted from the one format to the other.\n\n\nCode\nwards_sp <- as_Spatial(wards)\n\n\nWe can see that wards is of class sf,\n\n\nCode\nclass(wards)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nwhereas wards_sp is now of class SpatialPolygonsDataFrame, which is what we want.\n\n\nCode\nclass(wards_sp)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nHaving made the conversion we are ready to calculate some geographically weighted statistics, doing so for the High_income variable – the percentage of the population with income R614401 or greater in 2011 – in the example below.\n\n\nCode\ngwstats <- gwss(wards_sp , vars = \"High_income\", bw = 10, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\n\nThe results are contained in the spatial data frame ($SDF),\n\n\nCode\nhead(gwstats$SDF)\n\n\n  High_income_LM High_income_LSD High_income_LVar High_income_LSKe\n1       5.887743      2.92991675        8.5844122       -0.2838245\n2       3.280802      2.53680091        6.4353588        0.1765590\n3       1.109567      1.61167650        2.5975011        1.7142999\n4       2.377564      2.80047422        7.8426558        1.3837936\n5       5.781072      2.59128151        6.7147399        0.3118916\n6       0.101829      0.05610169        0.0031474        0.1862646\n  High_income_LCV\n1       0.4976298\n2       0.7732258\n3       1.4525268\n4       1.1778756\n5       0.4482354\n6       0.5509400\n\n\nAs well as the local means, the local standard deviations, variances, skews and coefficients of variation. The local medians, interquartile ranges and quantile imbalances could also be added by including the argument quantile = TRUE in gwss() – see ?gwss.\nThe results are dependent on the data (of course) but also the kernel (i.e. the shape of the weighting – the distance decay – around each point) and the bandwidth (i.e. the maximum number of neighbours to include, \\(k\\) or the distance threshold, \\(d\\)). The kernel matters…\n\nSource: GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models\n… but it matters much less than the bandwidth, which controls the amount of spatial smoothing: the larger it is, the more neighbours are being averaged over. The trade-off is between (geographical) precision and bias. A smaller bandwidth is less likely to average-out geographical detail in the data but it is also dependent on a small number of observations, some or more of which could be in error.\nThe following maps compare a bandwidth of \\(k = 10\\) nearest neighbours to \\(k = 100\\). If you zoom into the areas around the north of Cape Town you will see some of the differences. Note that the argument adaptive = TRUE sets the bandwidth to be in terms of nearest neighbours. The advantage of using nearest neighbours is it allows for varying population densities. Otherwise, and using a fixed distance, \\(d\\), more rural areas will tend to have fewer neighbours than urban ones because those rural areas are larger and more spaced apart.\n\n\nCode\ngwstats <- gwss(wards_sp , vars = \"High_income\", bw = 10, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwards$bw10 <- gwstats$SDF$High_income_LM\n\ngwstats <- gwss(wards_sp , vars = \"High_income\", bw = 100, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\nwards$bw100 <- gwstats$SDF$High_income_LM\n\nwards2 <- pivot_longer(wards, cols = starts_with(\"bw\"), names_to = \"bw\",\n             values_to = \"GWmean\")\n\ntmap_mode(\"view\")\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(wards2, names = \"wards\") +\n  tm_fill(\"GWmean\", palette = \"Reds\", title = \"%\",\n          alpha = 0.7,\n          id = \"District_1\",\n          popup.vars = c(\"GW mean:\" = \"GWmean\",\n                         \"Ward ID:\" = \"WardID\"),\n          popup.format = list(digits = 1)) +\n  tm_borders() +\n  tm_facets(by = \"bw\") +\n  tm_legend(title = \"Geographically weighted % population with income R614401 or greater\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA slight tangent\nYou may note the use of the pivot_longer function from the tidyverse packages in the code above. To see what this does, take a look at,\n\n\nCode\nwards %>%\n  st_drop_geometry %>%\n  select(WardID, bw10, bw100) %>%\n  arrange(WardID) %>%\n  head(n = 3)\n\n\n    WardID      bw10     bw100\n1 10101001 0.3868500 0.4349990\n2 10101002 0.4108465 0.4435761\n3 10101003 0.3742776 0.4254601\n\n\nNow compare it with,\n\n\nCode\nwards %>%\n  st_drop_geometry %>%\n  select(WardID, bw10, bw100) %>%\n  pivot_longer(cols = starts_with(\"bw\"), names_to = \"bw\",\n          values_to = \"GWmean\") %>%\n  arrange(WardID) %>%\n  head(n = 6)\n\n\n# A tibble: 6 × 3\n  WardID   bw    GWmean\n  <chr>    <chr>  <dbl>\n1 10101001 bw10   0.387\n2 10101001 bw100  0.435\n3 10101002 bw10   0.411\n4 10101002 bw100  0.444\n5 10101003 bw10   0.374\n6 10101003 bw100  0.425\n\n\nWhat you can see is that the two columns, bw10 and bw100 from the first table have been stacked into rows in the second. Doing this allows us to create the two linked plots by faceting on the bandwidth variable, bw, using tm_facets(by = \"bw\"). The reverse operation to pivot_longer() is pivot_wider() – see ?pivot_wider.\n\n\n‘Pre-calculating’ the distance matrix\nIn the calculations above, the distances between the ward centroids that are used in the geographical weighting are calculated twice. First in gwss(wards_sp , vars = \"High_income\", bw = 10, ...) and then again in gwss(wards_sp , vars = \"High_income\", bw = 100, ...). Since those distances don’t actually change (the centroids are fixed so therefore are the distances between them) so we might have saved a little computational time by calculating the distance matrix in advance and then using it in the geographically weighted statistics. Curiously, though, it actually takes longer!\n\n\nCode\n# Time to do the calculations without pre-calculating the distance matrix\nsystem.time({\n gwstats <- gwss(wards_sp , vars = \"High_income\", bw = 10, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wards_sp , vars = \"High_income\", bw = 100, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n  wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n  0.039   0.001   0.040 \n\n\nCode\n# Time to do the calculations with the pre-calculated distance matrix\nsystem.time({\n  coords <- st_coordinates(st_centroid(wards))\n  dmatrix <- gw.dist(coords, longlat = T)\n  gwstats <- gwss(wards_sp , vars = \"High_income\", bw = 10, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wards$bw10 <- gwstats$SDF$High_income_LM\n\n  gwstats <- gwss(wards_sp , vars = \"High_income\", bw = 100, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n  wards$bw100 <- gwstats$SDF$High_income_LM\n})\n\n\n   user  system elapsed \n  0.725   0.002   0.726"
  },
  {
    "objectID": "gwstats.html#selecting-the-bandwidth",
    "href": "gwstats.html#selecting-the-bandwidth",
    "title": "Geographically Weighted Statistics",
    "section": "Selecting the bandwidth",
    "text": "Selecting the bandwidth\nAs observed in the maps above, the geographically weighted statistics are a function of the geographical weighting that largely is controlled by the bandwidth. This raises the question of which is the correct bandwidth to use? Unfortunately, the most honest answer is that there is no correct answer, although an automatic bandwidth selection might be tried by calibrating the statistics around the local means.\nThe following uses a cross-validation approach\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\n# The selected number of nearest neighbours:\nbw\n\n\n[1] 17\n\n\nThe following uses an AIC corrected approach\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\")\n\n\nAdaptive bandwidth (number of nearest neighbours): 256 AICc value: 1480.449 \nAdaptive bandwidth (number of nearest neighbours): 166 AICc value: 1473.901 \nAdaptive bandwidth (number of nearest neighbours): 110 AICc value: 1442.05 \nAdaptive bandwidth (number of nearest neighbours): 75 AICc value: 1398.784 \nAdaptive bandwidth (number of nearest neighbours): 54 AICc value: 1370.417 \nAdaptive bandwidth (number of nearest neighbours): 40 AICc value: 1345.457 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 1335.979 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1320.554 \nAdaptive bandwidth (number of nearest neighbours): 23 AICc value: 1314.985 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 1306.969 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1305.399 \nAdaptive bandwidth (number of nearest neighbours): 20 AICc value: 1303.224 \n\n\nCode\nbw\n\n\n[1] 20\n\n\nPerhaps a bandwidth of around 20 is about right?\nThat automatic bandwidth selection applies only for the Western Cape wards, however. Recall earlier that the data were filtered (wards %>% filter(ProvinceNa == \"Western Cape\") -> wards) with the partial explanation for doing so being to reduce run times. Another explanation is that there is no reason to assume that the spatial autocorrelation that is quantified by the bandwidth selection will be the same everywhere across the map. In fact, it varies from province to province:\n\n\nCode\nload(\"wards.RData\") \nbandwidths <- sapply(unique(wards$ProvinceNa), \\(x) {\n  wards %>%\n    filter(ProvinceNa == x) %>%\n    as_Spatial %>%\n    bw.gwr(High_income ~ 1, data = .,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach =\"AIC\") %>%\n    paste0(\"Bandwidth = \", .)\n})\n\n\n\n\nCode\nbandwidths\n\n\n       Free State           Limpopo        Mpumalanga      Western Cape \n\"Bandwidth = 126\"  \"Bandwidth = 25\"  \"Bandwidth = 95\"  \"Bandwidth = 20\" \n    KwaZulu-Natal      Eastern Cape        North West           Gauteng \n \"Bandwidth = 21\"  \"Bandwidth = 24\"  \"Bandwidth = 19\"  \"Bandwidth = 19\" \n    Northern Cape \n\"Bandwidth = 201\" \n\n\nThis suggests that fitting geographically weighted statistics to too large a study region is not desirable because there is little reason to presume that the same bandwidth should apply throughout it."
  },
  {
    "objectID": "gwstats.html#changing-the-interpolation-calculation-points",
    "href": "gwstats.html#changing-the-interpolation-calculation-points",
    "title": "Geographically Weighted Statistics",
    "section": "Changing the interpolation (calculation) points",
    "text": "Changing the interpolation (calculation) points\nSo far we have been using the ward centroids as the points for which the geographically weighted statistics are calculated. There is no requirement to do so as they could be interpolated at any point within this study region. To demonstrate this, let’s reselect the wards in the Western Cape and convert them into a raster grid using the stars package.\n\n\nCode\nwards %>%\n  filter(ProvinceNa == \"Western Cape\") ->\n  wards\n\nif(!(\"stars\" %in% installed)) install.packages(\"stars\", dependencies = TRUE)\nrequire(stars)\n\nwards %>%\n  st_rasterize(nx = 100, ny = 100) %>%\n  st_as_sf ->\n  gridded\n\npar(mai=c(0,0,0,0))\ngridded %>%\n  st_geometry %>%\n  plot\n\n\n\n\n\nWe shall now calculate the geographically weighted mean for each raster cell, with an arbitrary bandwidth of 30 nearest neighbours.\n\n\nCode\ngridded_sp <- as_Spatial(gridded)\n\ngwstats <- gwss(wards_sp, gridded_sp, vars = \"High_income\", bw = 30, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\ngridded$GWmean <- gwstats$SDF$High_income_LM\n\nggplot(gridded, aes(fill = GWmean)) +\n  geom_sf(col = \"light grey\", size = 0) + # size is the width of the raster cell border\n  scale_fill_gradient(low = \"white\", high = \"dark red\")\n\n\n\n\n\nThis ability to interpolate at any point within the study region provides a means to deal with missing values in the data. Contained in the wards data is a variable giving the average age of the population in each ward in 2011 but it contains 15 missing values:\n\n\nCode\nsummary(wards$age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.90   27.95   29.80   30.91   33.00   51.30      15 \n\n\nThe same observations also record NAs for the No_schooling variable.\n\n\nCode\nwhich(is.na(wards$age))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nCode\nwhich(is.na(wards$No_schooling))\n\n\n [1] 111 112 113 114 115 147 201 202 270 288 294 306 346 347 378\n\n\nMissing value are a problem when fitting a regression model, for example. Usually they are simply omitted, as in the following case,\n\n\nCode\nols1 <- lm(No_schooling ~ age, data = wards)\nsummary(ols1)\n\n\n\nCall:\nlm(formula = No_schooling ~ age, data = wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1998 -0.5384  1.7687 13.2843 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 19.69367    1.14121   17.26   <2e-16 ***\nage         -0.38835    0.03657  -10.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.064 on 385 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.2265,    Adjusted R-squared:  0.2245 \nF-statistic: 112.8 on 1 and 385 DF,  p-value: < 2.2e-16\n\n\n An alternative approach is to replace the missing values with a ‘safe’ alternative such as the mean age for the values that are not missing (the global mean for the non-NA values) and the same for the percentages of the populations without schooling. However, we could also replace each missing value with a locally interpolated mean which fits with the geographical context.\nHere are the wards with missing age and no schooling values. These are the points that need to be interpolated.\n\n\nCode\nwards %>%\n  filter(is.na(age)) %>%\n  as_Spatial ->\n  missing\n\n\nThese are the wards with the values that serve as the data points.\n\n\nCode\nwards %>%\n  filter(!is.na(age)) %>%\n  as_Spatial ->\n  present\n\n\nFortunately, the missing values seem to be fairly randomly distributed across the study region. It would be a problem if they were all geographically clustered together because interpolating their values from their neighbours would not be successful if their neighbours’ values were also missing!\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nplot(present, border = \"light grey\")\nplot(missing, col = \"red\", add = T)\n\n\n\n\n\nWe can, then, interpolate the missing values from the present ones and match them into the data using base R’s match() function, matching on their WardID. Note that I have done this twice, once for the age variable and once for No_schooling. This is to allow for the possibility of them having differently sized bandwidths from each other (which they are when using approach = \"AIC\", although not, as it happens, with the default approach = \"CV\").\n\n\nCode\nbw <- bw.gwr(age ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\n\nAdaptive bandwidth (number of nearest neighbours): 246 AICc value: 2219.406 \nAdaptive bandwidth (number of nearest neighbours): 160 AICc value: 2210.723 \nAdaptive bandwidth (number of nearest neighbours): 105 AICc value: 2187.165 \nAdaptive bandwidth (number of nearest neighbours): 73 AICc value: 2164.013 \nAdaptive bandwidth (number of nearest neighbours): 51 AICc value: 2156.473 \nAdaptive bandwidth (number of nearest neighbours): 39 AICc value: 2154.864 \nAdaptive bandwidth (number of nearest neighbours): 30 AICc value: 2155.447 \nAdaptive bandwidth (number of nearest neighbours): 43 AICc value: 2155.08 \nAdaptive bandwidth (number of nearest neighbours): 35 AICc value: 2153.689 \nAdaptive bandwidth (number of nearest neighbours): 34 AICc value: 2153.551 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 2154.848 \nAdaptive bandwidth (number of nearest neighbours): 34 AICc value: 2153.551 \n\n\nCode\ngwstats <- gwss(present, missing, vars = \"age\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch <- match(missing$WardID, wards$WardID)\nwards$age[mch] <- gwstats$SDF$age_LM\n\nbw <- bw.gwr(No_schooling ~ 1, data = present,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T, approach = \"AIC\")\n\n\nAdaptive bandwidth (number of nearest neighbours): 246 AICc value: 1977.254 \nAdaptive bandwidth (number of nearest neighbours): 160 AICc value: 1955.892 \nAdaptive bandwidth (number of nearest neighbours): 105 AICc value: 1931.756 \nAdaptive bandwidth (number of nearest neighbours): 73 AICc value: 1897.789 \nAdaptive bandwidth (number of nearest neighbours): 51 AICc value: 1882.877 \nAdaptive bandwidth (number of nearest neighbours): 39 AICc value: 1876.113 \nAdaptive bandwidth (number of nearest neighbours): 30 AICc value: 1872.465 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1870.442 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1864.747 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 1861.27 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1852.67 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 1852.67 \n\n\nCode\ngwstats <- gwss(present, missing, vars = \"No_schooling\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nmch <- match(missing$WardID, wards$WardID)\nwards$No_schooling[mch] <- gwstats$SDF$No_schooling_LM\n\n\nIt is useful to keep a note of which values are interpolated, so…\n\n\nCode\nwards$interpolated <- FALSE\nwards$interpolated[mch] <- TRUE\n\n\nThere should be rlength(wards\\(interpolated[wards\\)interpolated])` of them.\n\n\nCode\ntable(wards$interpolated)\n\n\n\nFALSE  TRUE \n  387    15 \n\n\nNow returning to our regression model, there are, of course, no longer any missing values and, reassuringly, no evidence that the interpolated values are significantly different from the rest in either their mean No_schooling value or in their effect of age upon No_schooling.\n\n\nCode\nols2 <- update(ols1, . ~ . + interpolated*age)\nsummary(ols2)\n\n\n\nCall:\nlm(formula = No_schooling ~ age + interpolated + age:interpolated, \n    data = wards)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7037 -2.1709 -0.5446  1.7484 13.2843 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          19.69367    1.12855  17.450   <2e-16 ***\nage                  -0.38835    0.03617 -10.738   <2e-16 ***\ninterpolatedTRUE     -1.72125   10.95102  -0.157    0.875    \nage:interpolatedTRUE  0.02817    0.34961   0.081    0.936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.03 on 398 degrees of freedom\nMultiple R-squared:  0.2285,    Adjusted R-squared:  0.2226 \nF-statistic: 39.28 on 3 and 398 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "gwstats.html#geographically-weighted-correlation",
    "href": "gwstats.html#geographically-weighted-correlation",
    "title": "Geographically Weighted Statistics",
    "section": "Geographically weighted correlation",
    "text": "Geographically weighted correlation\nAccording to the model above, there is a negative correlation between the age of the population and the percentage without schooling. The correlation across the Western Cape is,\n\n\nCode\ncor(wards$No_schooling, wards$age)\n\n\n[1] -0.4756981\n\n\nThat is, however, the global correlation for what appears (below) to be a heteroscedastic relationship.\n\n\nCode\nwards %>%\n  ggplot(aes(x = age, y = No_schooling)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship so let’s consider that by calculating and mapping the geographically weighted correlations between the two variables.\n\n\nCode\nwards_sp <- as_Spatial(wards)\n\nbw <- bw.gwr(No_schooling ~ age, data = wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\ngwstats <- gwss(wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\n\nwards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nggplot(wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\nThe local correlations range -0.868 to -0.105, with an interquartile range from -0.714 to -0.451:\n\n\nCode\nsummary(wards$Corr_No_schooling.age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.8679 -0.7136 -0.6294 -0.5801 -0.4506 -0.1054 \n\n\nIf we add a little ‘cartographic know-how’ from an earlier session, we can identify that the correlation is stronger in and around Parow than it is in and around Blue Downs, for example.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  filter(place == \"city\" | place == \"town\") %>%\n  st_join(wards) %>%\n  # The above a spatial join, giving the point places the attributes of the wards they fall in\n  mutate(population = as.integer(population)) %>%\n  filter(!is.na(ProvinceNa) & population > 60000) ->\n  places\n\nlast_plot() +\n  geom_sf_label_repel(data = places, aes(label = name), alpha = 0.7,\n                      force = 5, size = 2, max.overlaps = 20) +\n  xlab(element_blank()) +\n  ylab(element_blank())\n\n\n\n\n\nIn general, the correlation appears to be strongest in cities:\n\n\nCode\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  st_join(wards) %>%\n  st_drop_geometry %>%\n  group_by(place) %>%\n  summarise(meancorr = mean(Corr_No_schooling.age, na.rm = TRUE)) %>%\n  arrange(meancorr)\n\n\n# A tibble: 5 × 2\n  place             meancorr\n  <chr>                <dbl>\n1 city                -0.855\n2 hamlet              -0.620\n3 isolated_dwelling   -0.572\n4 village             -0.555\n5 town                -0.514"
  },
  {
    "objectID": "gwstats.html#statistical-inference-and-significance",
    "href": "gwstats.html#statistical-inference-and-significance",
    "title": "Geographically Weighted Statistics",
    "section": "Statistical inference and significance",
    "text": "Statistical inference and significance\nWe can use a randomisation procedure to determine whether any of the local summary statistics can be said to be significantly different from those obtained by chance. The randomisation procedure is found in the function, gwss.montecarlo() with a default number of nsim = 99 simulations. This isn’t very many but they are time-consuming to calculate and will be sufficient to demonstrate the process.\nThe following code chunk returns to the local mean percentages of high earners. It goes through the complete process of determining a bandwidth using the bw.gwr() function, then calculating the local and geographically weighted statistics using gwss(), determining the p-values under randomisation, using gwsss.montecarlo, then mapping the results. Very few of the results would be adjudged significant but a few are.\n\nCalculating the p-values under randomisation takes some time so please be patient.\n\n\nCode\nbw <- bw.gwr(High_income ~ 1, data = wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\nAdaptive bandwidth: 256 CV score: 925.0618 \nAdaptive bandwidth: 166 CV score: 907.8583 \nAdaptive bandwidth: 110 CV score: 839.9751 \nAdaptive bandwidth: 75 CV score: 754.9144 \nAdaptive bandwidth: 54 CV score: 701.1287 \nAdaptive bandwidth: 40 CV score: 657.4657 \nAdaptive bandwidth: 32 CV score: 637.2662 \nAdaptive bandwidth: 26 CV score: 607.9657 \nAdaptive bandwidth: 23 CV score: 594.7227 \nAdaptive bandwidth: 20 CV score: 575.617 \nAdaptive bandwidth: 19 CV score: 575.2766 \nAdaptive bandwidth: 18 CV score: 566.5161 \nAdaptive bandwidth: 17 CV score: 561.563 \nAdaptive bandwidth: 17 CV score: 561.563 \n\n\nCode\ngwstats <- gwss(wards_sp , vars = \"High_income\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n\np.values <- gwss.montecarlo(wards_sp, vars = \"High_income\", bw = bw, kernel = \"bisquare\",\n                            adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwards$High_income_LM <- gwstats$SDF$High_income_LM\nwards$High_income_LM[p.values$High_income_LM > 0.025 &\n                     p.values$High_income_LM < 0.975] <- NA\n\nggplot(wards, aes(fill = High_income_LM)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", na.value = \"light grey\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Local mean percentage of higher earrners\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\nA second example considers the local correlations between the No_schooling and age variables. Again, most of the correlations are insignificant but with a few exceptions.\n\n\nCode\nbw <- bw.gwr(No_schooling ~ age, data = wards_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = T)\n\n\n\n\nCode\ngwstats <- gwss(wards_sp , vars = c(\"No_schooling\", \"age\"), bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T, dMat = dmatrix)\n\np.values <- gwss.montecarlo(wards_sp, vars = c(\"No_schooling\", \"age\"), bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = T)\np.values <- as.data.frame(p.values)\n\nwards$Corr_No_schooling.age <- gwstats$SDF$Corr_No_schooling.age\n\nwards$Corr_No_schooling.age[p.values$Corr_No_schooling.age > 0.025 &\n                            p.values$Corr_No_schooling.age < 0.975] <- NA\n\nggplot(wards, aes(fill = Corr_No_schooling.age)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"Correlation\", palette = \"Blues\", na.value = \"light grey\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Correlation between % No schooling and average age\",\n    subtitle = \"Geographically weighted (2011)\"\n  )\n\n\n\n\n\n\nWe have the problem of repeating testing that we also had when looking for spatial hotspots in the session about spatial autocorrelation. The function p.adjust() could be used but we may wonder, as previously, where the methods are too conservative."
  },
  {
    "objectID": "gwstats.html#improving-the-leigibility-of-the-map",
    "href": "gwstats.html#improving-the-leigibility-of-the-map",
    "title": "Geographically Weighted Statistics",
    "section": "Improving the leigibility of the map",
    "text": "Improving the leigibility of the map\n\nUsing a map insert\nBefore for completing this session there is value in addressing the problem that some parts of the map are so small that their contents are almost illegible (too small to be seen). The traditional and probably the most effective way to address this is to add one or more inserts to the map to magnify the small areas. One way to do this is by using the cowplot package with ggplot2. The following example is based on this tutorial and maps the percentages of the populations who are relatively high earners in each ward.\nFirst, we need to install and require the cowplot package.\n\n\nCode\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\", dependencies = TRUE)\nrequire(cowplot)\n\n\nSecond, the part of the wards map to be included in the map insert is extracted, using st_crop.\n\n\nCode\nwards_extract <- st_crop(wards, xmin = 18.2, xmax = 19, ymin = -34.3, ymax = -33.5)\nplot(st_geometry(wards_extract))\n\n\n\n\n\nThe bounding box for the extracted area is also obtained as it will be used as a feature in the final map (it will be drawn as a rectangle showing the geographical extent of the insert in the original map).\n\n\nCode\nwards_extract <- st_crop(wards, xmin = 18.2, xmax = 19, ymin = -34.3, ymax = -33.5)\nbbox <- st_as_sfc(st_bbox(wards_extract))\n\n\nNext, the main part of the map is created…\n\n\nCode\nmain_map <- ggplot(wards, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", direction = 1) +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\nplot(main_map)\n\n\n\n\n\n…as is the map insert:\n\n\nCode\ninsert <- ggplot(wards_extract, aes(fill = High_income)) +\n  geom_sf(col = \"transparent\") +\n  scale_fill_distiller(palette = \"Blues\", direction =  1) +\n  geom_sf(data = bbox, fill = NA, col = \"black\", size = 1.2) +\n  theme_void() +\n  theme(legend.position = \"none\")\nplot(insert)\n\n\n\n\n\nFinally, the maps are brought together using cowplot’s ggdraw() and draw_plot() functions. We know that some of the values it shows are not necessarily significant under randomisation but we will include them here to just to get\n\n\nCode\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(insert, x = 0, y = 0.25, scale = 0.22) +\n  draw_label(\"Cape Town\\nand\\nsurrounding areas\", x = 0.62, y = 0.78, size = 8)\n\n\n\n\n\n\nThe positioning of the map insert (x = ... & y = ...), the scaling of it, and the size and position of the label were all found by trial and error, using various values until I was happy with the results.\n\n\nUsing a ‘balanced carogtram’\nAnother approach is to use what has been described as a visually balanced cartogram. A cartogram – of which there are lots of interesting examples on this website – usually works by distorting a map in a way that rescales the areas in proportion not to their physical size but by some other measured attribute such as their population count. Much of the motivation for this lies in political studies and mapping the results of an election wherein a traditional map can give a very distorted view of the outcome because of how much population density varies across a country. In the example below, rescaling the areas by population correctly shows that the 2020 US Presidential was not a Republican landslide, despite Trump’s baseless accusations!\n Source: www.viewsoftheworld.net\nThe problem with cartograms is the distortion. Essentially, they are trading one visual problem (invisibility of small areas) for another (the amount of geographical distortion). The idea of a balanced cartogram comes from an awareness that sometimes it is sufficient just to make the small areas bigger and/or the big areas smaller, without causing too much geographical distortion. One way to achieve this is to scale the places by the square root of the original areas, as in the following example.\n\n\nCode\nif(!(\"cartogram\" %in% installed)) install.packages(\"cartogram\", dependencies = TRUE)\nrequire(cartogram)\n\n# Convert from longitude/latitude to a grid projection\nwards %>%\n  st_transform(22234) %>%\n  mutate(area = as.numeric(st_area(.)),\n         w = as.numeric(sqrt(area))) ->\n  wards_prj\n\n# Create the cartogram -- here a contiguous cartogram is used, see ?cartogram\nwards_carto <- cartogram_cont(wards_prj, weight = \"w\", maxSizeError = 1.4, prepare = \"none\")\n\nggplot(wards_carto, aes(fill = High_income)) +\n  geom_sf(col = \"white\", size = 0.1) +\n  scale_fill_distiller(\"%\", palette = \"Blues\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(\n    title = \"Cartogram of the percentage of the population who are higher earners\",\n    subtitle = \"(2011)\"\n  )\n\n\n\n\n\nThe results are ok, drawing attention to the wards with higher percentages of higher earners but clearly there is geographical distortion in the map, too.\n\n\nUsing a hexogram\nThe final approach to be considered here is what has been described as hexograms – a combination of tile maps and cartograms that make small areas bigger on the map but try and limit the geographic distortion. The original method also preserved typology (i.e. contiguous neighbours remained so in the final map) but is dated and very slow to operationalise as it was really just a proof of concept. A faster method but one that cannot guarantee to preserve typology is presented below.\nFirst, we need some functions to create the hexogram.\n\n\nCode\nif(!(\"cartogram\" %in% installed.packages()[,1])) install.packages(\"cartogram\")\nif(!(\"sf\" %in% installed.packages()[,1])) install.packages(\"sf\")\nif(!(\"lpSolve\" %in% installed.packages()[,1])) install.packages(\"lpSolve\") \n\nrequire(sf)\n\ncreate_carto <- \\(x, w = NULL, k = 2, itermax = 25, maxSizeError = 1.4) {\n  if(class(x)[1] != \"sf\") stop(\"Object x is not of class sf\")\n  if(is.null(w)) x$w <- as.numeric(st_area(x))^(1/k)\n  cartogram::cartogram_cont(x, \"w\", itermax = itermax, maxSizeError = maxSizeError, prepare = \"none\")\n}\n\n\ncreate_grid <- \\(x, m = 6) {\n  bbox <- st_bbox(x)\n  ydiff <- bbox[4] - bbox[2]\n  xdiff <- bbox[3] - bbox[1]\n  \n  n <- m * nrow(x)\n  ny <- sqrt(n / (xdiff/ydiff))\n  nx <- n / ny\n  nx <- ceiling(nx)\n  ny <- ceiling(ny)\n  \n  grd <- st_sf(st_make_grid(x, n = c(nx, ny), square = FALSE))\n}\n\n\ngrid_data <- \\(x, grd, k = 2) {\n  x_pts <- st_centroid(st_geometry(x), of_largest_polygon = TRUE)\n  grd_pts <- st_centroid(grd)\n  \n  cost.mat <- st_distance(x_pts, grd_pts)^k\n  row.rhs <- rep(1, length(x_pts))\n  col.rhs <- rep(1, nrow(grd_pts))\n  row.signs <- rep(\"=\", length(x_pts))\n  col.signs <- rep(\"<=\", nrow(grd_pts))\n\n  optimisation <- lpSolve::lp.transport(cost.mat, \"min\", row.signs, row.rhs, col.signs, col.rhs)$solution\n  \n  mch <- sapply(1:nrow(optimisation), \\(i) which(optimisation[i, ] == 1))\n  grd <- st_sf(grd[mch,])\n  cbind(grd, st_drop_geometry(x))\n}\n\n\ncreate_layers <- \\(x, grd) {\n  \n  area <- sapply(1: nrow(x), \\(i) {\n    y <- st_intersects(x[i, ], grd)[[1]]\n    if(i %in% y) {\n      if(length(y) == 1) return(st_area(x[i, ]))\n      if(length(y) > 1) {\n        area <- st_area(x[i, ])\n        overlaps <- y[-(y == i)]\n        area <- area - sum(st_area(st_intersection(st_geometry(x[i, ]), st_geometry(grd[overlaps, ]))))\n        return(area) \n      }\n    } else {\n      return(0)\n    }\n  })\n\n  i <- which(area > 2*as.numeric(st_area(grd))[1])\n  list(x[i, ], grd[-i, ])\n}\n\n\nNext, we run through the stages of its creation, which are to start with a balanced cartogram (create_carto()), then overlay that with a grid of hexagons (create_grid()), next assign places in the original map to a hexagon (grid_data()), then plot the results as two layers (create_layers()) to give a mash-up of the cartogram and the hexagons.\n\n\nCode\nwards_carto <- create_carto(wards_prj)\ngrd <- create_grid(wards_carto)\nwards_grid <- grid_data(wards_carto, grd)\nwards_layers <- create_layers(wards_carto, wards_grid)\n\nggplot() +\n  geom_sf(data = wards_layers[[1]], aes(fill = High_income), col = \"white\", size = 0.4) +\n  geom_sf(data = wards_layers[[2]], aes(fill = High_income), col = \"dark grey\") +\n  scale_fill_distiller(\"%\", palette = \"Blues\", direction =  1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"Hexogram % of the population who are higher earners\",\n    subtitle = \"South African Western Cape (2011)\"\n  )\n\n\n\n\n\nI find the result quite visually appealing and it isn’t just me: a separate study has provided empirical evidence for the value of balanced cartograms and hexograms as a visualisation tool mapping spatial distributions."
  },
  {
    "objectID": "gwstats.html#summary",
    "href": "gwstats.html#summary",
    "title": "Geographically Weighted Statistics",
    "section": "Summary",
    "text": "Summary\nThis session has introduced the concept of geographically weighted statistics to examine spatial heterogeneity in a measured variable and to allow for the possibility that its strength (and direction) of correlation with another variable varies from place-to-place. As such, these statistics are a form of local statistic, which is to say they can vary across the map. Sometimes, the parts of the map that we are interested in are small in relation to other parts, creating a problem of invisibility/legibility. Maps inserts, balanced cartograms and hexograms have been introduced as a means to address this visualisation problem."
  },
  {
    "objectID": "gwstats.html#futher-reading",
    "href": "gwstats.html#futher-reading",
    "title": "Geographically Weighted Statistics",
    "section": "Futher reading",
    "text": "Futher reading\nGollini, I., Lu, B., Charlton, M., Brunsdon, C., & Harris, P. (2015). GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models. Journal of Statistical Software, 63(17), 1–50. https://doi.org/10.18637/jss.v063.i17"
  },
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "A Cartographic Answer",
    "section": "",
    "text": "Let’s answer the ‘why?’ question with a quick example of R in use. We will not worry about the exact detail of what the code means at this stage or attempt to explain it in full. Instead, we will largely take it as we find it, copying and pasting from this webpage into the R Console. The focus is on some of what R can do from a geographic perspective and not, at this stage, on how it does it.\n\nIf you find that the + sign stays on your screen, in the R Console, for a while and isn’t followed by > then you have either forgotten to hit Enter/Return or have not included all of the code that is needed to complete an operation (to complete a function, for example). You can always press esc on your keyboard and try again.\n\n\nFirst, we will check that the necessary packages are installed and then require them, which means to load them so they are available to use. The usual way to install a package is with the function, install.packages() so, for example, the graphics package ggplot2 is installed using install.packages(\"ggplot2\"). The code below is a bit more elaborate as it checks which packages have not yet been installed and installs them. However, the two-step process is the same: install and then require – use install.packages(...) to install packages (only needs to be done once on your computer, unless you re-install R / replace it with a more recent version); then require(...) to load the desired packages (needs to be done each time R is restarted).\n\n\nCode\n# Checks to see which packages are already installed:\ninstalled <- installed.packages()[,1]\n# Creates a character vector of packages that are required:\nrequired <- c(\"XML\", \"tidyverse\", \"readxl\", \"sf\", \"ggplot2\", \"classInt\", \"ggspatial\")\n# Checks which of the required packages have not yet been installed:\ninstall <- required[!(required %in% installed)]\n# Installs any that have not yet been installed:\nif(length(install)) install.packages(install, dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(readxl)\nrequire(sf)\nrequire(ggplot2)\nrequire(classInt)\nrequire(ggspatial)\n\n\n\nThe use of # indicates a comment in the code. It is there just for explanation. It is not executable code (it is ignored not run).\nThe required packages can also be loaded together rather than separately as the were in the code chunk before.\n\n\nCode\nlapply(required, require, character.only = TRUE)\n\n\n\nYou will find that some people prefer to use library(...) instead of require(...). There difference between them is subtle but you can find an argument in favour of using library here.\n\n\n\nNext, we will download a data table published by Statistics South Africa that provides estimates of the number of people speaking various languages in the South African Provinces in 2011. These data were downloaded from https://superweb.statssa.gov.za/webapi/. The data are found in an Excel spreadsheet, which is read in and manipulated, converting the counts into percentages.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/table_2022-06-22_17-36-26.xlsx?raw=true\", \"language.xlsx\", quiet = TRUE, mode = \"wb\")\n\nread_xlsx(\"language.xlsx\", sheet = \"Data Sheet 0\", skip = 8) |>\n  rename(Name = 2) |>\n  drop_na(Afrikaans) |>\n  select(-1) |>\n  mutate(across(where(is.numeric), ~ round(. / Total * 100, 2))) -> languages\n\n\nHere is the top of the data, viewed in the R environment:\n\n\nCode\nhead(languages)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# … with 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\nThere is often more than one way of achieving something in R. Here we could also use,\n\n\nCode\nslice_head(languages, n = 6)\n\n\n# A tibble: 6 × 14\n  Name     Afrikaans English IsiNdebele IsiXhosa IsiZulu Sepedi Sesotho Setswana\n  <chr>        <dbl>   <dbl>      <dbl>    <dbl>   <dbl>  <dbl>   <dbl>    <dbl>\n1 Eastern…      9.32    3.62       0.06    83.4     0.8    0.05    2.37     0.03\n2 Free St…     11.9     1.16       0.37     9.09    5.1    0.26   64.4      6.85\n3 Gauteng      14.4    12.5        1.94     7.59   21.5   10.7    13.1      8.39\n4 KwaZulu…      1.49   13.6        0.2      2.33   80.9    0.11    0.71     0.06\n5 Limpopo       2.32    0.55       1.49     0.27    0.65  52.2     1.32     1.58\n6 Mpumala…      6.15    1.66      12.1      1.49   26.4   10.8     3.66     2.72\n# … with 5 more variables: SiSwati <dbl>, Tshivenda <dbl>, Xitsonga <dbl>,\n#   Other <dbl>, Total <dbl>\n\n\n\n\n\nWhat R allows is the opportunity to map the data without needing to go outside R to use separate software such as GIS. To do so, we will need a ‘blank map’ of the Provinces that can be joined with the data to create a choropleth map (a type of thematic map).\nFirst, we will download a pre-existing map, also from https://superweb.statssa.gov.za/webapi/.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/mapview.kmz?raw=true\", \"map.kmz\", quiet = TRUE, mode = \"wb\")\nunzip(\"map.kmz\")\nst_read(\"doc.kml\") |>\n  select(-Description) -> map\n\n\nHere is the outline of that map:\n\n\nCode\nggplot(data = map) +\n  geom_sf()\n\n\n\n\n\n\n\n\nNow we can link the data table to the map\n\n\nCode\nmap |>\n  left_join(languages, by = \"Name\") -> map\n\n\nand then plot one of the variables.\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(aes(fill = IsiXhosa), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark blue\") +\n  ggtitle(\"% Population speaking Xhosa\")\n\n\n\n\n\n The really nice thing about this is that it is now very easy to change the appearance of the map with only minor updates to the code.\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"stamenwatercolor\", progress = \"none\") +\n  geom_sf(aes(fill = English), alpha = 0.8) +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  ggtitle(\"% Population speaking English\")\n\n\n\n\n\n\n\nCode\nggplot(data = map) +\n  annotation_map_tile(type = \"thunderforestlandscape\", progress = \"none\") +\n  geom_sf(aes(fill = Afrikaans), alpha = 0.8, col = \"transparent\") +\n  scale_fill_gradient(low = \"white\", high = \"dark red\") +\n  annotation_north_arrow(which_north = \"grid\", location = \"topright\") +\n  ggtitle(\"% Population speaking Afrikaans\")\n\n\n\n\n\n\n\n\nFinally, once we are happy with it, we can export the image in a format suitable for a journal publication or to insert into other documents such as Microsoft Word.\nAs jpeg, to print quality:\n\n\nCode\nggsave(\"mymap.jpg\", device = \"jpeg\", width = 7, height = 6, units = \"in\", dpi = \"print\")\n\n\nAs pdf:\n\n\nCode\nggsave(\"mymap.pdf\", device = \"pdf\", width = 7, height = 6, units = \"in\")\n\n\nAs bmp, to screen quality:\n\n\nCode\nggsave(\"mymap.bmp\", device = \"bmp\", width = 7, height = 6, units = \"in\", dpi = \"screen\")\n\n\nIf we now look in your working directory, they should be there:\n\n\nCode\nlist.files(pattern = \"mymap\")\n\n\n[1] \"mymap.bmp\" \"mymap.jpg\" \"mymap.pdf\""
  },
  {
    "objectID": "why.html#another-example",
    "href": "why.html#another-example",
    "title": "A Cartographic Answer",
    "section": "Another example",
    "text": "Another example\nThe following example is much more complex however it should still make a simple point – it does not take too many lines of code to produce a high quality visual output. It might take a little bit of searching around online to find the code and instruction to produce exactly what you want but I rarely struggle to find an answer fairly quickly.\nI originally developed the following maps in response to the release of the 2021 UK Census data showing the ethnic composition of small area neighbourhoods. The four cities – Birmingham, Leicester, London and Manchester – are the ones that are no longer majority White British (i.e. less than half their population self-identified as White British). A consequence of this demographic change is that the cities are becoming more ethnically diverse, which is what the maps show, using a standardised census geography that I also created in R.\n\n\nCode\n# Read-in the attribute data and the boundary file:\ndf <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/data/diversity.csv?raw=true\")\nmap <- st_read(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/cities.geojson\", quiet = TRUE)\n\n# Although more complex, at heart what the following code does is\n# join the map to the data and then produce a separate map for\n# each city and time period, using a consistent style\ndf |>\n  pivot_longer(where(is.numeric), values_to = \"index\", names_to = \"year\") %>%\n  mutate(year = paste0(\"20\",substring(year, 3, 4))) %>%\n  left_join(map, ., by = \"OAXXCD\") %>%\n  mutate(group = paste(CITY, year, sep = \" ~ \")) %>%\n  split(.$group) %>%\n\n  lapply(function(x) {\n    \n    ggplot(x, aes(fill = index)) +\n      geom_sf(col = \"transparent\") +\n      scale_fill_viridis_c(\"Diversity\", values = c(0,0.25,0.5,0.7,0.85,0.95,1)) +\n      annotation_north_arrow(location = \"tl\", style = north_arrow_minimal(text_size = 10),\n                             height = unit(0.6, \"cm\"), width = unit(0.6, \"cm\")) +\n      annotation_scale(location = \"br\", style = \"ticks\", line_width = 0.5,\n                       text_cex = 0.5, tick_height = 0.4,\n                       height = unit(0.15, \"cm\"), text_pad = unit(0.10, \"cm\")) +\n      theme_minimal() +\n      theme(axis.text = element_blank(),\n            axis.ticks = element_blank(),\n            plot.title = element_text(size = 8, hjust = 0.5),\n            legend.title = element_text(size = 7, vjust = 3),\n            legend.text =element_text(size = 6), \n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            plot.margin = margin(t = 0,  \n                                 r = 0,  \n                                 b = 0,\n                                 l = 0)) +\n      labs(title = paste0(x$CITY[1], \": \", x$year[1]))\n  }) -> g\n\n# The cowplot library offers some additional plotting functionality\nif(!(\"cowplot\" %in% installed)) install.packages(\"cowplot\")\nrequire(cowplot)\n\n# The following gets the common legend for the maps\n# and stops it being printed 12 times -- once will be enough!\nlegend <- get_legend(g[[1]])\nlapply(g, function(x) {\n  x + theme(legend.position='none')\n}) -> g\n\n# This brings all the maps together as one\nggdraw(plot_grid(plot_grid(plotlist = g, ncol=3, align='v'),\n                 plot_grid(NULL, legend, ncol=1, scale = 0.5),\n                 rel_widths=c(1, 0.1),\n                 rel_heights=c(1, 0,1))) -> g\n\nprint(g)"
  },
  {
    "objectID": "why.html#convinced",
    "href": "why.html#convinced",
    "title": "A Cartographic Answer",
    "section": "Convinced?",
    "text": "Convinced?\nOf course, maps can also be produced in open source software such as QGIS and GIS software certainly have their use. R is not automatically better or necessarily a replacement for these. However, what it does offer is an integrated environment for what we might call geographic data science: we can download data from external websites, load and tidy-up those data, fit statistical or other models to them and map the results – all from within R. Our stages of working can be saved as scripts, which are faster to change and modify than using ‘point-and-click’ operations, and we can share our code with other people (even those using different operating systems) facilitating collaborative working and reproducible social-/ science. Finally, there are lots of packages available for reading, visualising, and analysing spatial data in R. Some of them are summarised here. These are attractive reasons for mapping and modelling within R."
  },
  {
    "objectID": "why.html#alternatives",
    "href": "why.html#alternatives",
    "title": "A Cartographic Answer",
    "section": "Alternatives",
    "text": "Alternatives\n\nAside from software such as QGIS, an interesting area of development is Geographic Data Science with Python. You can learn more about it here."
  },
  {
    "objectID": "why.html#need-more-convincing",
    "href": "why.html#need-more-convincing",
    "title": "A Cartographic Answer",
    "section": "Need more convincing?",
    "text": "Need more convincing?\nIf you have time, have a look at this exercise that we sometimes use with prospective students at University open days. The idea of the exercise is not to teach the students R but to show them how we use R for geographic data science in the School of Geographical Sciences. What the exercise does is take COVID-19 data for English neighbourhoods, fit statistical models to it and map the results – all in R. Again, it is the ability to use R for all the stages shown below that makes it so useful.\n\nSource: R for Data Science"
  },
  {
    "objectID": "thematicmaps.html",
    "href": "thematicmaps.html",
    "title": "Thematic maps in R",
    "section": "",
    "text": "There are lots of ways to produce maps in R. But, however, they are drawn, two things are usually need to produce a choropleth map of the sort seen in the previous session: some data, and a map to join the data to. Once we have those, R offers plenty of options to produce quick or publication quality maps, which may have either static or dynamic content."
  },
  {
    "objectID": "thematicmaps.html#getting-started",
    "href": "thematicmaps.html#getting-started",
    "title": "Thematic maps in R",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoad the data\nLet’s begin with the easy bit and load the data, which are from http://superweb.statssa.gov.za. These includes the variable No_schooling which is the percentage of the population without schooling per South African municipality in 2011.\n\n\nCode\ninstalled <- installed.packages()[,1]\nif(!(\"tidyverse\" %in% installed)) install.packages(\"tidyverse\", dependencies = TRUE)\nrequire(tidyverse)\n\neducation <- read_csv(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/education.csv\")\nprint(education, n = 3)\n\n\n# A tibble: 234 × 8\n  LocalMunicipality… LocalMunicipali… No_schooling Some_primary Complete_primary\n  <chr>              <chr>                   <dbl>        <dbl>            <dbl>\n1 EC101              Camdeboo                 13.4         34.5             8.94\n2 EC102              Blue Crane Route         16.0         36.2             8.80\n3 EC103              Ikwezi                   18.4         35.4             8.59\n# … with 231 more rows, and 3 more variables: Some_secondary <dbl>,\n#   Grade_12_Std_10 <dbl>, Higher <dbl>\n\n\n\n\nLoading the map\nNext we need a ‘blank map’ of the same South African municipalities that are included in the data above. It is read-in below in geoJSON format but it would not have been unusual if it had been in .shp (shapefile) or .kml format, instead. The source of the data is https://dataportal-mdb-sa.opendata.arcgis.com/. There are several ways of reading this file into R but it is better to use the sf package because older options such as maptool::readShapePoly() (which was for reading shapefiles) or rgdal::readOGR are either deprecated already or in the process of being retired.\n\n\nCode\nif(!(\"sf\" %in% installed)) install.packages(\"sf\", dependencies = TRUE)\nrequire(sf)\n\nmunicipal <- read_sf(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/boundary%20files/MDB_Local_Municipal_Boundary_2011.geojson\")\n\n\n If we now look at the top of the municipal object then we find it is of class sf, which is short for simple features. It has a vector geometry (it is of type multipolygon) and has its coordinate reference system (CRS) set as WGS 84. It also contains some attribute data, although not the schooling data we are looking to map.\n\n\nCode\nprint(municipal, n = 1)\n\n\nSimple feature collection with 234 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 16.45189 ymin: -34.83417 xmax: 32.94498 ymax: -22.12503\nGeodetic CRS:  WGS 84\n# A tibble: 234 × 11\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n     <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n# … with 233 more rows, and 6 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>, geometry <MULTIPOLYGON [°]>\n\n\nHere are just the attribute data\n\n\nCode\nst_drop_geometry(municipal) %>% print(n = 5)\n\n\n# A tibble: 234 × 10\n  OBJECTID ProvinceCode ProvinceName LocalMunicipalityCode LocalMunicipalityName\n*    <int> <chr>        <chr>        <chr>                 <chr>                \n1        1 EC           Eastern Cape BUF                   Buffalo City         \n2        2 EC           Eastern Cape EC101                 Camdeboo             \n3        3 EC           Eastern Cape EC102                 Blue Crane Route     \n4        4 EC           Eastern Cape EC103                 Ikwezi               \n5        5 EC           Eastern Cape EC104                 Makana               \n# … with 229 more rows, and 5 more variables: DistrictMunicipalityCode <chr>,\n#   DistrictMunicipalityName <chr>, Year <int>, Shape__Area <dbl>,\n#   Shape__Length <dbl>\n\n\nAnd here is the ‘blank’ map.\n\n\nCode\npar(mai=c(0, 0, 0, 0))  # Removes the plot margins\nmunicipal %>% st_geometry %>% plot\n\n\n\n\n\nHad it been necessary to set the coordinate reference system then the function st_set_crs() would be used. Instead, and just for fun, we will change it: here is the map transformed on to a ‘south up’ coordinate reference system, achieved by changing its EPSG code to 2050 with the function st_transform().\n\n\nCode\npar(mai=c(0, 0, 0, 0))\nmunicipal %>%\n  st_transform(2050) %>%\n  st_geometry %>%\n  plot"
  },
  {
    "objectID": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "href": "thematicmaps.html#joining-the-attribute-data-to-the-map",
    "title": "Thematic maps in R",
    "section": "Joining the attribute data to the map",
    "text": "Joining the attribute data to the map\nIf we look again at the map and schooling data, we find that they have two variables in common:\n\n\nCode\nintersect(names(municipal), names(education))\n\n\n[1] \"LocalMunicipalityCode\" \"LocalMunicipalityName\"\n\n\nThis is encouraging but, in this example, we need to be careful using the municipal names because not all of those in the map are in the education data or vice versa.\n\n\nCode\ntable(municipal$LocalMunicipalityName %in% education$LocalMunicipalityName)\n\n\n\nFALSE  TRUE \n    7   227 \n\n\nCode\ntable(education$LocalMunicipalityName %in% municipal$LocalMunicipalityName)\n\n\n\nFALSE  TRUE \n    7   227 \n\n\nFortunately, the municipal codes are consistent even where the names are not.\n\n\nCode\ntable(municipal$LocalMunicipalityCode %in% education$LocalMunicipalityCode)\n\n\n\nTRUE \n 234 \n\n\nCode\ntable(education$LocalMunicipalityCode %in% municipal$LocalMunicipalityCode)\n\n\n\nTRUE \n 234 \n\n\nWe therefore join the data to the map using the variable LocalMunicipalityCode and check that the schooling data are now attached to the map. They are.\n\n\nCode\nmunicipal <- left_join(municipal, education, by = \"LocalMunicipalityCode\")\nnames(municipal)\n\n\n [1] \"OBJECTID\"                 \"ProvinceCode\"            \n [3] \"ProvinceName\"             \"LocalMunicipalityCode\"   \n [5] \"LocalMunicipalityName.x\"  \"DistrictMunicipalityCode\"\n [7] \"DistrictMunicipalityName\" \"Year\"                    \n [9] \"Shape__Area\"              \"Shape__Length\"           \n[11] \"geometry\"                 \"LocalMunicipalityName.y\" \n[13] \"No_schooling\"             \"Some_primary\"            \n[15] \"Complete_primary\"         \"Some_secondary\"          \n[17] \"Grade_12_Std_10\"          \"Higher\"                  \n\n\n\nNote that the variables LocalMunicipalityName.x and LocalMunicipalityName.y have been created in the process of the join. This is because there are non-joined variables with duplicated names in the data, i.e. LocalMunicipalityName from municipal and LocalMunicipalityName from education."
  },
  {
    "objectID": "thematicmaps.html#mapping-the-data",
    "href": "thematicmaps.html#mapping-the-data",
    "title": "Thematic maps in R",
    "section": "Mapping the data",
    "text": "Mapping the data\n\nUsing plot{sf}\nThe ‘one line’ way of plotting the data is to use the in-built plot() function for sf.\n\n\nCode\nplot(municipal[\"No_schooling\"])\n\n\n\n\n\nIt is important to include the variable(s) you wish to include in the plot or else it will plot them all up to the value specified by the argument max.plot, which has a default of nine:\n\n\nCode\nplot(municipal)\n\n\n\n\n\nThe map can be customised. For example,\n\n\nCode\nif(!(\"RColorBrewer\" %in% installed)) install.packages(\"RColorBrewer\", dependencies = TRUE)\nrequire(RColorBrewer)\n\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"jenks\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\nFor the above map RColorBrewer package has been used to create a diverging red-yellow-blue colour palette that has then been reversed using the function rev() so that red is assigned to the highest values, not lowest. RColorBrewer provides colour palettes based on https://colorbrewer2.org/. A ‘natural breaks’ (jenks) classification with 7 colours has been used (breaks = \"jenks\"). Compare it with the result from using breaks = \"equal\",\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"equal\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"Percentage of Population with No Schooling\")\n\n\n\n\n\n… or breaks = \"quantile\".\n\n\nCode\nplot(municipal[\"No_schooling\"], key.pos = 1, breaks = \"quantile\", nbreaks = 7,\n     pal = rev(brewer.pal(7, \"RdYlBu\")),\n     graticule = TRUE, axes = TRUE,\n     main = \"% of Population with No Schooling\")\n\n\n\n\n\nClearly the maps do not all appear alike. This reveals that the geographical patterns and therefore the geographical information that we view in the map are a function of how the map is constructed, including the number, colouring and widths (ranges) of the map classes. Ideally, these should be set to reflect the distribution of the data and what is being look for in it.\nThe following histograms show the break points in the distributions used in the various maps. The code works by creating a list of plots (specifically, a list of ggplots, see below) – one plot each for the jenks, equal and quantile styles – and then using a package called gridExtra to arrange those plots into a single grid. However, the code matters less than what it reveals, which is that Jenks or other ‘natural breaks’ classifications are reasonably good for identifying break points that reflect the distribution of the data in the absence of the user having cause to set those break points in some other way.\n\n\nCode\nif(!(\"gridExtra\" %in% installed)) install.packages(\"gridExtra\", dependencies = TRUE)\nif(!(\"classInt\" %in% installed)) install.packages(\"classInt\", dependencies = TRUE)\n\nrequire(gridExtra)\nrequire(classInt)\n\nstyles <- c(\"jenks\", \"equal\", \"quantile\")\ng <- lapply(styles, \\(x) {\n    ggplot(municipal, aes(x = No_schooling)) +\n    geom_histogram() +\n    xlab(\"% of Population with No Schooling\") +\n    geom_vline(xintercept = classIntervals(municipal$No_schooling, n = 7, style = x)$brks, col = \"white\") +\n    geom_rug() +\n    theme_minimal() +\n    ggtitle(paste(x,\"classification\"))\n})\ngrid.arrange(grobs = g)\n\n\n\n\n\n For further information on using plot{sf} see here and look at the help menu, ?sf::plot.\n\n\nUsing ggplot2\nWhilst the plot() function for sf objects is useful for producing quick maps, I tend to prefer ggplot2 for better quality ones. We already have seen examples of ggplot2 output in earlier sessions and also in the histograms above.\nggplot2 is based on The Grammar of Graphics. I find it easiest to think of it in four stages:\n\nSay which data are to be plotted;\nSay which aesthetics of the chart (e.g. colour, line type, point size) will vary with the data;\nSay which types of plots (which ‘geoms’) are to feature in the chart;\n(Optional) change other attributes of the chart to add titles, rename the axis labels, and so forth.\n\nHere are those four stages applied to a boxplot showing the distribution of the no schooling variable by South African Provinces. First, the data = municipal. Consulting with the ggplot2 cheatsheet, I find that the aesthetics, aes(...), for the boxplot require a discrete x and a continuous y, which are provided by ProvinceName and No_schooling, respectively. ProvinceName has also been used to assign a fill colour to each box. The optional changes arise from me preferring theme_minimal() to the default style, although I have then modified it to remove the legend, change the angle of the text on the x-axis, remove the x-axis label and change the y-axis one.\n\n\nCode\nggplot(data = municipal, aes(x = ProvinceName, y = No_schooling, fill = ProvinceName)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) +\n  xlab(element_blank()) +\n  ylab(\"% No schooling within municipalities\")\n\n\n\n\n\n Let’s now take that process and apply it to create a map, using the same RColorBrewer colour palette as previously and adding the map using geom_sf (for a full list of geoms available for ggplot2 see here). The arguments theme(... = element_blank(), ...) suppress the axis titles. What labs() does should be obvious.\n\n\nCode\nggplot(municipal, aes(fill = No_schooling)) +\n  geom_sf() +\n  scale_fill_distiller(\"%\", palette = \"RdYlBu\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )  \n\n\n\n\n\n Presently the map has a continuous shading scheme. This can be changed to discrete map classes and colours by converting the continuous municipal$No_schooling variable to a factor, using the cut() function, here with break points found using ClassIntervals(..., style = \"jenks\"). The argument scale_fill_brewer(..., direction = -1) reverses the RdYlBu palette so that the highest values are coloured red. Adding guides(fill = guide_legend(reverse = TRUE)) reverses the legend so that the highest values are on top in the legend, which is another preference of mine.\n\n\nCode\n# Find the break points in the distribution using a Jenks classification\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\n\n# Factor the No_schooling variable using those break points\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\n\nggplot(municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n\nAnnotating the map with ggspatial\nHaving created the basic map using ggplot2, we can add some additional map elements using ggspatial. The following code adds a backdrop to the map. Different backgrounds (alternative map tiles) can be chosen from the list at rosm::osm.types(); see here for what they look like.\n\nIn the previous code chunk (above), the data, municipal are handed-to ggplot in the top line ggplot(municipal, ...). In essence, this sets municipal as a global parameter: it is where ggplot will look for the variable called for in aes(fill = No_schooling_gp) and where it will look for other variables too. Whilst this is fine, for now, to preempt any problems occurring from when I introduce a second geom_sf in few code chunks time, in the code below I specifically name municipal as the fill data, in the line geom_sf(data = municipal, aes(fill = No_schooling_gp)), which will leave me free to associate other data with different aesthetics in due course.\n\n\nCode\nif(!(\"ggspatial\" %in% installed)) install.packages(\"ggspatial\", dependencies = TRUE)\nrequire(ggspatial)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf(data = municipal, aes(fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\n A north arrow and a scale bar can also be added, although including the scale bar generates a warning because the map-to-true life distance ratio is not actually constant across the map (it varies with longitude and latitude). The argument, location = \"tl\" is short for top left; location = \"br\" for bottom right. See ?annotation_north_arrow() and ?annotation_scale() for further details and options. Note also the use of the last_plot() function to more easily add content to the last ggplot.\n\n\nCode\nlast_plot() +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\n\n\n\n\n In the next example, the locations of South African cities are added to the map, with a symbol drawn in proportion to their population size. The source of the data is a shapefile from https://data.humdata.org/dataset/hotosm_zaf_populated_places. The symbol shape that is specified by pch = 3 has the same numeric coding as those in ?graphics::points() (e.g. 0 is a square, 1, is a circle, 2 is a triangle, and so forth). The function scales::label_comma() forces decimal display of numbers to avoid displaying scientific notation.\n\n\nCode\nif(!(\"scales\" %in% installed)) install.packages(\"scales\", dependencies = TRUE)\n\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\nunzip(\"cities.zip\")\n\nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  filter(place == \"city\") %>%\n  mutate(population = as.numeric(population)) ->\n  cities\n\nlast_plot() +\n  geom_sf(data = cities, aes(size = population), pch = 3) +\n  scale_size(\"Population\", labels = scales::label_comma())\n\n\n\n\n\n\nSlightly confusingly, A shapefile actually consists of at least three files, one with the extension .shp (the coordinate/shape data), one .shx (an index file) and one .dbf (the attribute data). If you use a shapefile you need to make sure you download all of them and keep them together in the same folder.\n\n\nLabelling using ggsflabel\nNice labelling of the cities is provided by ggsflabel, in this case showing cities with over a million population. The function geom_sf_label_repel() is designed to stop labels from being placed over each other.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nlast_plot() +\n  geom_sf_label_repel(data = cities %>% filter(population > 1e6),\n                      aes(label = name), alpha = 0.7, size = 3)\n\n\n\n\n\n\n\nSaving the map\nHaving created the map, it can now be saved. You may wish to change your working directory first, using setwd(dir) and substituting dir with the pathname to the preferred directory, or by using Session -> Set Working Directory -> Choose Directory from the dropdown menus. Once you have done so, the last_plot() is easily saved using the function ggsave. For example, in .pdf format, to a print quality,\n\n\nCode\nggsave(\"no_schooling.pdf\", device = \"pdf\", width = 6, units = \"in\", dpi = \"print\")\n\n\nAlternatively, we can write directly to a graphics device, using one of the functions bmp(), jpeg(), png(), tiff() or pdf(). For instance,\n\n\nCode\njpeg(\"no_schooling.jpg\", res = 72)\nlast_plot()\ndev.off()\n\n\n\n\nCreating an interactive map using ggiraph\nSo far all the maps we have created have been static. This is obviously better for anything that will be printed but, for a website or similar, we may wish to include more ‘interaction’. The package ggiraph package creates interactive ggplot2 and we can use it to create an interactive map where information about the areas appears as we brush over those areas on the map with the mouse pointer. This is achieved by replacing in the code geom_sf() with the geom_sf_interactive() function from ggiraph and rendering the resulting ggplot2 object with girafe().\n\n\nCode\nif(!(\"ggiraph\" %in% installed)) install.packages(\"ggiraph\", dependencies = TRUE)\nrequire(ggiraph)\n\ng <- ggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  annotation_map_tile(type = \"cartolight\", progress = \"none\") +\n  geom_sf_interactive(aes(tooltip = paste0(LocalMunicipalityName.x, \"\\n\", round(No_schooling,1), \"%\"),\n                             fill = No_schooling_gp)) +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_minimal(text_size = 14)) +\n  annotation_scale(location = \"br\", style = \"ticks\")\n\ngirafe(ggobj = g)\n\n\n\n\n\n\n\n\n\nUsing tmap\nClearly there is a lot of scope to produce high quality maps using ggplot2 and various associated packages. However, it has a rival, in tmap, which is arguably easier to use. Like ggplot2, tmap adopts the Grammar of Graphics but approaches it in a slightly different way that uses layers: it builds-up the layers of the graphic by first specifying a spatial object or background, then doing things to the map based on it, then specifying another spatial object and/or other map elements to do things with, and so forth. The types of layers available can be viewed here and here. A brief introduction to tmap is available here.\nThe following code builds-up the layers of the map to produce one quite like what was previously created in ggplot2.\n\n\nCode\nif(!(\"tmap\" %in% installed)) install.packages(\"tmap\", dependencies = TRUE)\nrequire(tmap)\n\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_fill(\"No_schooling\", palette = \"-RdYlBu\", title = \"%\", style = \"jenks\", n = 7) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %>% filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\n\n\n\n\n\nUnfortunately, there is a error in the geometry of the underlying municipal map file: all(st_is_valid(municipal)). The problem lies in the 128th area, where one edge crosses another: st_is_valid(municipal[128,], reason = TRUE). At the time of writing, this has not been fixed and, because tmap is less forgiving of it than ggplot2 is, it has been omitted from the map. \n\nThe map looks pretty good and can be saved using the function tmap_save. For example,\n\n\nCode\ntmap_save(tmap_last(), \"no_schooling2.jpg\", width = 7, units = \"in\")\n\n\nHowever, there is a cartographic irritation. If you look at the map classes, they are non-unique: e.g. 5.64 to 9.87, 9.87 to 13.38, 13.38 to 17.19, and so forth. Which category would a value of 9.87 (or 13.38, etc.) fall into?\nTo solve this problem, we can do what we did for the ggplots too, which is to create a factor from the municipal$No_schooling variable, which is what the first two lines of code below do. The third line reverses the order of the factors, so that the highest not lowest valued group is treated as the first level and so forth. The reason I have added this is because of my preference for the highest values to appear top in the legend.\n\n\nCode\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\nmunicipal$No_schooling_gp <- factor(municipal$No_schooling_gp,\n                                    levels = rev(levels(municipal$No_schooling_gp)))\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\", shape = 3) +\n  tm_shape(cities %>% filter(population > 1e6)) + \n  tm_text(\"name\", bg.color = \"white\", auto.placement = TRUE, bg.alpha = 0.6) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\n\n\n\n\n Where tmap really excels is in rendering interactive maps to leaflet by changing the tmap_mode to tmap_mode(\"view\") from tmap_mode(\"plot\"). The following allows panning, can be zoomed in and out of, can have different map layers displayed (move your mouse cursor over the map layers icon to do so) and, if you right click on any of the areas shown, will bring-up information about them.\n\n\nCode\ntmap_mode(\"view\")\n\ntm_basemap(c(Stamen = \"Stamen.Watercolor\",\n             Carto = \"CartoDB\",\n             OSM = \"OpenStreetMap\")) +\n  tm_shape(municipal[-128,], is.master = TRUE, name = \"municipalities\") +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\",\n          id = \"LocalMunicipalityName.x\",\n          popup.vars = c(\"% No schooling:\" = \"No_schooling\",\n                         \"Province: \" = \"ProvinceName\"),\n          popup.format = list(digits = 1)) +\n  tm_borders(col = \"black\") +\n  tm_shape(cities) +\n  tm_dots(size = \"population\",\n          id = \"name\",\n          popup.vars = c(\"Population: \" = \"population\")) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_view(view.legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n \nDifferent layers are available dependent upon the tmap_mode. For example, there is no straightforward way of adding a maptile (tm_basemap) as the backdrop to a \"plot\" map, nor a compass (tm_compass) or a scale bar (tm_scale_bar) to a \"view\".\n We can also have some fun! Here is an animated map where the animation is produced from a combination of the tm_facets(along = \"ProvinceName\", ...) layer and the use of tmap_animation() function. The animation may be saved as .gif file by including the argument filename (see ?tmap_animation).\n\n\nCode\nif(!(\"gifski\" %in% installed)) install.packages(\"gifski\", dependencies = TRUE)\n\ntmap_mode(\"plot\")\n\nt <- tm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,], is.master = TRUE) +\n  tm_polygons(col = \"grey\", border.col = \"black\") +\n  tm_shape(municipal[-128,]) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"%\") +\n  tm_borders(col = \"white\") +\n  tm_facets(along = \"ProvinceName\", free.coords = FALSE) +\n  tm_legend(title = \"Percentage of Population with No Schooling\", bg.color = \"white\", bg.alpha = 0.7) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), bg.color = \"white\") +\n  tm_credits(\"Source: 2011 Census / Statistics South Africa\", bg.color = \"white\")\n\ntmap_animation(t, delay = 100)\n\n\n\n Faceting can also be used on static maps, as in the following example, where tm_facets(by = \"ProvinceName\", free.coords = TRUE) creates a choropleth map for each Province, with a common legend, positioned outside each provincial map with tm_layout(legend.outside.position = \"bottom\").\n\n\nCode\ntmap_mode(\"plot\")\n\ntm_graticules(col = \"light grey\") +\n  tm_shape(municipal[-128,]) +\n  tm_fill(\"No_schooling_gp\", palette = \"RdYlBu\", title = \"% Population with No Schooling\",\n          legend.is.portrait = FALSE) +\n  tm_borders(col = \"white\") +\n  tm_facets(by = \"ProvinceName\", free.coords = TRUE) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.outside.position = \"bottom\")"
  },
  {
    "objectID": "thematicmaps.html#saving-the-map-and-attribute-data",
    "href": "thematicmaps.html#saving-the-map-and-attribute-data",
    "title": "Thematic maps in R",
    "section": "Saving the map and attribute data",
    "text": "Saving the map and attribute data\nBefore finishing, we will save the map and joined attribute data to the working directory as an R object.\n\n\nCode\nsave(municipal, file = \"municipal.RData\")"
  },
  {
    "objectID": "thematicmaps.html#summary",
    "href": "thematicmaps.html#summary",
    "title": "Thematic maps in R",
    "section": "Summary",
    "text": "Summary\nThis session has demonstrated that R is a powerful tool for drawing publication quality maps. The native plot functions for sf objects are useful as a quick way to draw a map and both ggplot2 and tmap offer a range of functionality to customise their cartographic outputs to produce really nice looking maps. I tend to use ggplot2 but that is really more out of habit than anything else as tmap might actually be the easier to use. It depends a bit on whether I am drawing static maps (usually in ggplot2) or interactive ones (probably better in tmap). There are other packages available, too, including mapview, which the following code chunk uses (see also, here), and Leaflet to R. Perhaps the key take-home point is that these maps can look at lot better than those produced by some conventional GIS and can be linked to other analytical processes in R.\n\n\nCode\nif(!(\"mapview\" %in% installed)) install.packages(\"mapview\", dependencies = TRUE)\nrequire(mapview)\n\nmapview(municipal %>% mutate(No_schooling = round(No_schooling, 1)), \n        zcol = \"No_schooling\",\n        layer.name = \"% No Schooling\",\n        map.types = \"Stamen.Watercolor\",\n        col.regions = colorRampPalette(rev(brewer.pal(9, \"RdYlBu\"))))"
  },
  {
    "objectID": "thematicmaps.html#further-reading",
    "href": "thematicmaps.html#further-reading",
    "title": "Thematic maps in R",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 2 on Spatial data and R packages for mapping from Geospatial Health Data by Paula Morga (2019)\n\nChapter 9 on Making maps with R from Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow."
  },
  {
    "objectID": "markdown_example.html",
    "href": "markdown_example.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "markdown_example.html#scripts",
    "href": "markdown_example.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is based on the opening parts of Geographic Data in ipumsr, where ipumsr provides “an easy way to import census, survey and geographic data provided by ‘IPUMS’ into R”, and IPUMS “provides census and survey data from around the world integrated across time and space.” I have modified their code for greater consistency with other parts of this course but what it does is largely the same:\n\nIt loads some example data and manipulates it to calculate the percentages of people using solid fuels for cooking in regions of Colombia at three time periods. These are the attribute data\nIt loads a geographic boundary file of those regions. This is the map.\nIt simplifies (reduces the detail of) the map to make it faster for plotting.\nIt joins the attribute data to the map by means of their common geography.\nIt produces a choropleth map (thematic map) of the geographic variation in the percentages using solid fuel.\nIt saves the spatially joined data as a shapefile.\n\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the following maps.\n\nBe patient whilst the code takes a few moments to run.\n\n\n\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety.\nAs suggested earlier, an advantage of using a script is that it is easy to make changes to and then re-run it either in part or in full. In the script, find the type = \"cartolight\" argument in the line that says annotation_map_tile(type = \"cartolight\", progress = \"none\") + and change the type to any other of the following, such as cartodark.\n\n\nCode\nrosm::osm.types()\n\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"hikebike\"              \n [7] \"hillshade\"              \"osmgrayscale\"           \"stamenbw\"              \n[10] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[13] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\nIf you wish to view what the different types look like, you can do so here.\nOnce you have made the change, select the parts of the script you wish to rerun (everything under Map the data should be sufficient) and press the Run button or hit command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) to execute the selected code.\n\nA note about the :: notation\nThe use of the :: notation in the code rosm::osm.types() allows a function to be run from a package that has not been loaded. In the example, osm.types() is a function in the rosm package. That package provides access to Open Steet Map and other maps tiles. We could also require(rosm) and then use the function directly as we have in other cases, i.e. using osm.types() instead of rosm::osm.types(). However, sometimes, if a function only is to be used once then there is no need to require the whole package. Also, sometimes we load multiple packages that have functions within them that share the same name. In such circumstances, the package::function format may be required to make sure the correct function (from the correct package) is being called."
  },
  {
    "objectID": "markdown_example.html#r-markdown",
    "href": "markdown_example.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs and to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -> New File -> R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet.\n\nThe source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "markdown_example.html#summary",
    "href": "markdown_example.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "markdown_example.html#further-reading",
    "href": "markdown_example.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/."
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "",
    "text": "(Draft version)"
  },
  {
    "objectID": "multilevel.html#introduction",
    "href": "multilevel.html#introduction",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Introduction",
    "text": "Introduction\nGeographically Weighted Regression, which we looked at in the previous session, treats geographic space as a continuous spatial variable in the sense that regression relationships can vary from one location to another across a geographic study region. That continuous view of space is clear if we fit a simple ‘null model’ of the COVID-19 rates in London in the week before Christmas 2021 and map the results. It is a null model because it includes no predictor variables other than the local estimate of the intercept (i.e. the local mean rate of COVID-19). What is clear is how that local estimate is allowed to vary across geographic space.\n(Download and extract the data for London and fit the GWR model)\n\n\nCode\nrequire(sf)\nrequire(ggplot2)\nrequire(tidyverse)\nrequire(GWmodel)\n\nif(!file.exists(\"covid_xmas_2021.geojson\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_xmas_2021.geojson\", \"covid_xmas_2021.geojson\", mode = \"wb\", quiet = TRUE)\nxmas_covid <- read_sf(\"covid_xmas_2021.geojson\")\n\nxmas_covid %>% \n  filter(regionName == \"London\") ->\n  ldn_covid\n\nldn_covid_sp <- as_Spatial(ldn_covid)\nbw <- bw.gwr(rate ~ 1, data = ldn_covid_sp, adaptive = TRUE)\ngwrmod <- gwr.basic(rate ~ 1, data = ldn_covid_sp, bw = bw, adaptive = TRUE)\n\n\n(Map the predicted COVID-19 rates across London based on the spatially varying local mean estimate)\n\n\nCode\nldn_covid$yhat <- gwrmod$SDF$yhat\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nMultilevel models, by contrast, have more of a discrete view of space because observations at some base level are treated as members of a group at a second, more aggregate level. In a geographic context, the base level could be neighbourhoods and the groups could be the local authorities to which the neighbourhoods belong. This more discrete view of space becomes clear if we fit a very simple (too simple, really) null model of the COVID-19 rates in London, using the local authority estimates from this multilevel model as the nearest equivalent to the local means of the GWR model.\n\n\nCode\nrequire(lme4)\nmlm <- lmer(rate ~ (1|LtlaName), data = ldn_covid)\nldn_covid$yhat <- predict(mlm, re.form = ~ (1|LtlaName))\nggplot(data = ldn_covid, aes(fill = yhat)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1, limits = c(1.2,3)) +\n  theme_minimal()\n\n\n\n\n\nIf we compare the map above with that produced from the GWR estimates then we find that there are commonalities in the two maps but the multilevel model clearly has the more bounded view of geographic space."
  },
  {
    "objectID": "multilevel.html#multilevel-models",
    "href": "multilevel.html#multilevel-models",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Multilevel models",
    "text": "Multilevel models\nThe package that we are using to fit the models here is lme4. The formula rate ~ (1|LtlaName) means that we are modelling the COVID-19 rate against a random intercept that varies by local authority (by local authority name, LtlaName). It is not the only way of fitting multilevel models in R (an alternative, for example, is brms) but it is sufficient for the purposes of this exercise.\nThe more discrete and bounded view of geographic space that the multilevel model (in its simplest form, at least) employs seems quite restricting – in this case study it is suggesting that the causes of COVID-19 are somehow related to ‘things’ that happen or have consequences at a local authority scale. The implication is that the boundaries of local authorities are relevant to the geographical causes and rates of COVID-19. This is not an assumption that GWR makes so why use multilevel models instead?\nOne answer is that a multilevel model is often faster to fit. Consider the following example, which extends the study region to London and the South East.\n(Fit the GWR model and the multilevel model)\n\n\nCode\nxmas_covid %>%\n  filter(regionName == \"London\" | regionName == \"South East\") ->\n  ldn_se_covid\nldn_se_covid_sp <- as_Spatial(ldn_se_covid)\nt1 <- Sys.time()\nbw <- bw.gwr(rate ~ 1, data = ldn_se_covid_sp, adaptive = TRUE)\ngwrmod <- gwr.basic(rate ~ 1, data = ldn_se_covid_sp, bw = bw, adaptive = TRUE)\ngwrt <- Sys.time() - t1\nt2 <- Sys.time()\nmlm <- lmer(rate ~ (1|LtlaName), data = ldn_se_covid)\nmlmt <- Sys.time() - t2\n\n\nThe computation times are,\n\n\nCode\ngwrt\n\n\nTime difference of 2.784072 secs\n\n\nCode\nmlmt\n\n\nTime difference of 0.01118588 secs\n\n\nAdmittedly, neither took very long to fit, with the GWR taking 0.05 minutes and the multilevel model (MLM) taking 0.01 seconds on my laptop. However, these differences become more noticeable with more complex models or larger study regions. The multilevel model is estimating far fewer model parameters than the GWR. This is because the GWR is really lots of models that are fitted separately (one for each location) and then compared, whereas the multilevel model is one model but one which allows for variance at the geographic level(s) of the model, so better local authorities, for example. Put another way, the GWR is a series of local models that are compared, whereas the multilevel model is a global model that allows for local variations around it: at the moment, all it is doing is estimating the average COVID rate for the whole study region but also recognising that different local authorities have rates that vary around the overall average.\nSecond, multilevel models can be used – as their name suggests! – to fit multi-level models to examine the scale of the geographic pattern of a variable. Consider the following example, which fits a multilevel model to the COVID-19 rates of all English neighbourhoods in the week before Christmas 2021. There are now three levels in the model, which are the base neighbourhoods level, then the local authorities in which the neighourhoods are located, and then the English regions to which the local authorities belong. Hence, three levels and three simultaneous scales of analysis: neighbourhoods, local authorities and regions. Although the resulting map suffers a little from the ‘invisibility problem’ of some small areas within it, the higher COVID-19 rate in parts of London and the South East is evident (but not solely confined to these regions, as there is also a high rate evident in the North West).\n\n\nCode\nmlm <- lmer(rate ~ (1|LtlaName) + (1|regionName), data = xmas_covid)\nxmas_covid$yhat <- predict(mlm, re.form = ~ (1|LtlaName) + (1|regionName))\nggplot(data = xmas_covid, aes(fill = yhat)) +\n  geom_sf(col = NA) +\n  scale_fill_distiller(\"Modelled rate\", palette = \"YlOrRd\", direction = 1) +\n  theme_minimal()\n\n\n\n\n\nHaving fitted a model with three geographic levels, the geographical question that we can now ask is which of the levels of the model contributes most to the geographical patterning of the disease. Do we get most variation between neighbourhoods (suggesting the geographic pattern is mostly at the neighbourhood level), between local authorities (so a local authority patterning) or between regions (evidence of strongest regional differences)? We can answer the question by looking at how much of the variance belongs to each level, which we can see in the summary under Random effects.\n\n\nCode\nsummary(mlm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rate ~ (1 | LtlaName) + (1 | regionName)\n   Data: xmas_covid\n\nREML criterion at convergence: 2858.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3127 -0.6222 -0.0334  0.5832  6.8771 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n LtlaName   (Intercept) 0.08043  0.2836  \n regionName (Intercept) 0.14923  0.3863  \n Residual               0.07714  0.2777  \nNumber of obs: 6789, groups:  LtlaName, 315; regionName, 9\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)     1.26       0.13   9.687\n\n\nFrom these summary values we can work out the percentage of the variance which is at each level, calculating what is known as the intraclass correlation (ICC). For this particular week of the pandemic it is,\n\n\nCode\nsummary(mlm)$varcor %>%\n  as_tibble %>%\n  select(grp, vcov) %>%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  <chr>       <dbl> <dbl>\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThe ICC appears to suggest the presence of regional effects, in particular (because the ICC for regionName is greater than the ICC for any other level). These regional effects, which are treated as random effects in the model, can be examined using the code below, from which they are found to be strongest for London. This means that the COVID-19 rate for this week is higher for London that it is for any other region.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  arrange(desc(condval))\n\n\n# A tibble: 9 × 5\n  grpvar     term        grp                      condval condsd\n  <chr>      <fct>       <fct>                      <dbl>  <dbl>\n1 regionName (Intercept) London                    0.814  0.0505\n2 regionName (Intercept) East of England           0.236  0.0433\n3 regionName (Intercept) North West                0.181  0.0463\n4 regionName (Intercept) South East                0.0644 0.0356\n5 regionName (Intercept) East Midlands            -0.0688 0.0463\n6 regionName (Intercept) West Midlands            -0.189  0.0527\n7 regionName (Intercept) Yorkshire and The Humber -0.241  0.0626\n8 regionName (Intercept) North East               -0.389  0.0816\n9 regionName (Intercept) South West               -0.407  0.0536\n\n\nOf course, somewhere has to be highest so the fact the it is highest for London does not mean the difference is necessarily statistically significant. However, as it turns out, it is significant (significantly different from zero), which we can see if we plot what is called a caterpillar plot in the multilevel literature, drawn here with a 95% confidence interval around each of the regional effects.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nNot only is London’s regional ‘uplift’ in the COVID rates significantly greater than zero, London’s regional effect is greater than for any other regions. The following chart suggests that London is different from the rest because its confidence interval does not overlap with any others. Note that if you are testing to see if two places differ from each other as opposed to from zero then you need to shorten the confidence interval before looking to see if they overlap. As a rule of thumb, the 95% confidence interval for a test of difference in two values is 1.39 \\(\\times\\) the standard error of the parameter estimate instead of the more usual 1.96.\n\n\nCode\nranef(mlm, whichel = \"regionName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.39 * condsd,\n         upr = condval + 1.39 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           theme_minimal() +\n           theme(axis.text.x = element_text(angle = 90, vjust = 0.5), axis.title.x = element_blank())\n\n\n\n\n\nDespite the clear evidence of regional differences that are driven by London’s much higher COVID-19 rate in the week before Christmas 2021, keep in mind that not all the pattern in the COVID-19 rates is at the regional level. To recall,\n\n\nCode\nsummary(mlm)$varcor %>%\n  as_tibble %>%\n  select(grp, vcov) %>%\n  mutate(ICC = round(vcov / sum(vcov) * 100, 1))\n\n\n# A tibble: 3 × 3\n  grp          vcov   ICC\n  <chr>       <dbl> <dbl>\n1 LtlaName   0.0804  26.2\n2 regionName 0.149   48.6\n3 Residual   0.0771  25.1\n\n\nThere are, for example, differences between the local authorities, too, at the next level down in the model. These are evident from a caterpillar plot for the authorities although because there are more of these authorities than there are regions (315 authorities Vs rlength(unique(xmas_covid$regionName))` regions, individual ones are harder to discern.\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  ggplot(aes(x = grp, y = condval, ymin = lwr, ymax = upr)) +\n           geom_errorbar() +\n           geom_hline(yintercept = 0, linetype = \"dotted\") +\n           theme_minimal() +\n           theme(axis.text.x = element_blank()) +\n           xlab(\"Local authorities\")\n\n\n\n\n\nLooking at the ‘top ten’ with COVID-19 rates higher than the national and their regional averages would predict are,\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  arrange(desc(condval)) %>%\n  mutate(lwr = condval - 1.96 * condsd,\n         upr = condval + 1.96 * condsd) %>%\n  slice_max(condval, n = 10)\n\n\n# A tibble: 10 × 7\n   grpvar   term        grp                  condval condsd   lwr   upr\n   <chr>    <fct>       <fct>                  <dbl>  <dbl> <dbl> <dbl>\n 1 LtlaName (Intercept) Thurrock               0.849 0.0746 0.702 0.995\n 2 LtlaName (Intercept) Dartford               0.688 0.0814 0.528 0.847\n 3 LtlaName (Intercept) Bristol, City of       0.679 0.0645 0.553 0.805\n 4 LtlaName (Intercept) Lambeth                0.674 0.0675 0.541 0.806\n 5 LtlaName (Intercept) Manchester             0.666 0.0584 0.551 0.780\n 6 LtlaName (Intercept) Elmbridge              0.643 0.0722 0.502 0.785\n 7 LtlaName (Intercept) Reigate and Banstead   0.587 0.0722 0.446 0.729\n 8 LtlaName (Intercept) Epsom and Ewell        0.582 0.0937 0.398 0.765\n 9 LtlaName (Intercept) Salford                0.567 0.0671 0.435 0.698\n10 LtlaName (Intercept) Gedling                0.554 0.0820 0.393 0.714\n\n\nThe key point here is that the multilevel model is useful to help unpack at what scale(s) geographic outcomes are arising and to indentify some of the key places driving those outcomes."
  },
  {
    "objectID": "multilevel.html#spatial-varying-coefficient-effects",
    "href": "multilevel.html#spatial-varying-coefficient-effects",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Spatial varying coefficient effects",
    "text": "Spatial varying coefficient effects\nThe mutilevel models fitted above are random intercepts models – only the intercept term is permitted to vary from place to place. Under these models, some places are expected to have a higher or lower average COVID-19 rate than others but that rate is not affected by any predictor variables that can also very in their effects from place-to-place.\nImagine that the percentage of the population of secondary school age is a factor in the COVID-19 rates in London ahead of Christmas 2021. It is a very simple model but it appears to have some (limited) explanatory power. As it happens, more children of school age appears to be less associated with COVID, perhaps because they and the families had already caught it?\n\n\nCode\nols <- lm(rate ~ age12.17, data = ldn_covid_sp)\nsummary(ols)\n\n\n\nCall:\nlm(formula = rate ~ age12.17, data = ldn_covid_sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11561 -0.29813 -0.00225  0.27856  1.20720 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.629006   0.058593  44.869   <2e-16 ***\nage12.17    -0.076615   0.008245  -9.292   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4111 on 980 degrees of freedom\nMultiple R-squared:  0.08097,   Adjusted R-squared:  0.08003 \nF-statistic: 86.34 on 1 and 980 DF,  p-value: < 2.2e-16\n\n\nA geographically weighted regression suggests that there might be spatial variation in the relationship between age12.17 and the COVID rate. In truth, not all of these local estimates are necessarily significant but let us ignore that for the moment.\n(Fit the GWR model allowing for spatial variation in the effects of age12.17 on rate)\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE)\n\n\nAdaptive bandwidth: 614 CV score: 124.8849 \nAdaptive bandwidth: 387 CV score: 113.7512 \nAdaptive bandwidth: 246 CV score: 103.5928 \nAdaptive bandwidth: 159 CV score: 94.75184 \nAdaptive bandwidth: 105 CV score: 88.35261 \nAdaptive bandwidth: 72 CV score: 84.00381 \nAdaptive bandwidth: 51 CV score: 81.04989 \nAdaptive bandwidth: 38 CV score: 79.4773 \nAdaptive bandwidth: 30 CV score: 79.26434 \nAdaptive bandwidth: 25 CV score: 79.3698 \nAdaptive bandwidth: 33 CV score: 79.28656 \nAdaptive bandwidth: 28 CV score: 79.23342 \nAdaptive bandwidth: 27 CV score: 79.22796 \nAdaptive bandwidth: 26 CV score: 79.35989 \nAdaptive bandwidth: 27 CV score: 79.22796 \n\n\nCode\ngwrmod <- gwr.basic(rate ~ age12.17, data = ldn_covid_sp, adaptive = TRUE, bw = bw)\n\n\n(Map the results)\n\n\nCode\nldn_covid$age12.17GWR <- gwrmod$SDF$age12.17\nggplot(data = ldn_covid, aes(fill = age12.17GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n  theme_minimal() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nThe closest multilevel model to the above GWR model (given the current data) is one that allows the effects of age12.17 to vary by local authority (LtlaName).\n(fit the model)\n\n\nCode\nmlm <- lmer(rate ~ (age12.17|LtlaName), data = ldn_covid)\n\n\n(plot the results)\n\n\nCode\nranef(mlm, whichel = \"LtlaName\") %>%\n  as_tibble %>%\n  filter(term == \"age12.17\") %>%\n  rename(LtlaName = grp,\n         age12.17LM = condval) %>%\n  select(LtlaName, age12.17LM) %>%\n  inner_join(ldn_covid, ., by = \"LtlaName\") %>%\n  ggplot(aes(fill = age12.17LM)) +\n    geom_sf(size = 0.25) +\n    scale_fill_gradient2(trans = \"reverse\", limits = c(0.237, -0.282)) +\n    theme_minimal() +\n    guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nAgain, there are commonalities in the GWR and MLM estimates but the differing conceptions of the structure of geographic space and the spatial relationships it contains makes a difference to the results."
  },
  {
    "objectID": "multilevel.html#more-to-be-added",
    "href": "multilevel.html#more-to-be-added",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "More to be added!",
    "text": "More to be added!\nThis session is incomplete. For now, the key point is that GWR and multilevel models conceptualise space in different ways. I am not meaning to imply that multilevel models are worse, as they can be very useful and flexible. It useful to note that there are now methods that intgrate spatial regression models with multilevel models (see, for example, here and GWR with hierarchical group structures (for example, this and this but these are pretty advanced methods of analysis!"
  },
  {
    "objectID": "multilevel.html#further-reading",
    "href": "multilevel.html#further-reading",
    "title": "Continuous and discrete views of the spatial variable",
    "section": "Further Reading",
    "text": "Further Reading\nMultilevel models – an introduction to multilevel models from the Handbook of Spatial Analysis in the Social Sciences."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "Welcome",
    "section": "About the course",
    "text": "About the course\nThe contents of this course were first developed for a short course at the University of Cape Town (UCT) in August 2022. It also forms part of the MSc Geographic Data Science and Spatial Analytics in the School of Geographical Sciences, University of Bristol.\nThe aims of this course are to teach an introduction to mapping and spatial modelling in R. It is a course in geographic data science with a particular focus on mapping, measuring and quantifying spatial patterns in data. The present parts of the course are:\n\nWhy use R for mapping and spatial modelling?\nThe basics of mapping in R\nThe Spatial Variable: from maps towards models\nSpatial clustering and spatial heterogeneity: measuring patterns in data\nHarnessing spatial autocorrelation with geographically weighted statistics\nSpatial regression models\n\n\nThis is a work in progress\nChanges will be made and additional content added over time so check back here for the latest updates."
  },
  {
    "objectID": "index.html#pre-reading",
    "href": "index.html#pre-reading",
    "title": "Welcome",
    "section": "Pre-reading",
    "text": "Pre-reading\nThe following short pre-reading is recommended for the course:\nHarris RJ (2019). Not just nuisance: spatialising social statistics. In A Whitworth (ed.) Towards a Spatial Social Policy: Bridging the Gap Between Geography and Social Policy. Chapter 8. Bristol: Policy Press. Available here (or, if that doesn’t work try here)."
  },
  {
    "objectID": "index.html#other-useful-resources",
    "href": "index.html#other-useful-resources",
    "title": "Welcome",
    "section": "Other useful resources",
    "text": "Other useful resources\nSpatial Regression Models for the Social Sciences covers similar statistical ground to this course, For University of Bristol students, it is available to view as an eBook here.\n\nIn addition, Geocomputation with R by Robin Lovelace, Jakub Nawosad & Jannes Muenchow offers an extremely useful reference to have to hand if you are stuck when undertaking geocomputation with R. There is a free online version available."
  },
  {
    "objectID": "index.html#provisional-masters-programme",
    "href": "index.html#provisional-masters-programme",
    "title": "Welcome",
    "section": "Provisional Masters programme",
    "text": "Provisional Masters programme\nFor the 2022-3 iteration of the Masters unit, the draft teaching schedule is:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\nMon Jan 23, 3 - 6pm\nWhy R & Flavours of R\n\n\n2\nMon Jan 30, 3 - 6pm\nMapping the spatial variable\n(Thematic maps in R)\n\n\n3\nMon Feb 13, 3 - 6pm\nMeasuring spatial autocorrelation\n\n\n4\nMon Feb 20, 3 - 6pm\nFrom Maps to models (1):\nGeographically Weighted Statistics\n\n\n5\nMon Mar 6, 3 - 6pm\nFrom Maps to models (2):\nSpatial Regression\n\n\n6\nMon Mar 13, 3 - 6pm\nTBC"
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Welcome",
    "section": "About the author",
    "text": "About the author\nThis course is authored by Richard Harris, Professor of Quantitative Social Geography at the University of Bristol. You can find out more about me, my research and other interests at https://profrichharris.github.io/.\n @profrichharris"
  },
  {
    "objectID": "index.html#copyright-notice",
    "href": "index.html#copyright-notice",
    "title": "Welcome",
    "section": "Copyright notice",
    "text": "Copyright notice\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n \n@GeogBristol #justsaying!"
  },
  {
    "objectID": "spregress.html",
    "href": "spregress.html",
    "title": "Spatial Regression",
    "section": "",
    "text": "In the previous session we looked at geographically weighted statistics, including geographically weighted correlation, which examines whether the correlation between two variables varies across a map. In this session we extend from correlation to looking at regression modelling with a spatial component. The data that we shall use are a map of the rate of COVID-19 infections per thousand population in neighbourhoods of the North West of England over the period from the week commencing 2020-03-07 to the week commencing 2022-04-16. That rate is calculated as the total number of reported infections per neighbourhood, divided by the mid-2020 ONS estimated population. There are three problems with these data, which originate (prior to adjustment) from https://coronavirus.data.gov.uk/details/download:\n\nThe rate, as calculated, does not allow for re-infection (i.e. the same person can catch COVID-19 more than once).\nNot everyone who had COVID was tested for a positive diagnosis. Limitations in the testing regime are described in this paper and are most severe earlier in the pandemic and again towards the end of the data period as testing was scaled-back and then largely withdrawn.\nWhere a neighbourhood had less than three cases in a week, that number was reported as zero even though it could really be one or two. That second type of undercount adds up, although, unsurprisingly it affects the largest cities most (because they have more neighbourhoods to be undercounted) in weeks when COVID is not especially prevalent. An adjustment has been made to the data to allow for this but not for the undercount caused by not testing positive.\n\nNevertheless, the data are sufficient to illustrate the methods."
  },
  {
    "objectID": "spregress.html#getting-started",
    "href": "spregress.html#getting-started",
    "title": "Spatial Regression",
    "section": "Getting Started",
    "text": "Getting Started\nThe data are downloaded as follows. The required packages should be installed already, from previous sessions.\n\n\nCode\nif(!(\"remotes\" %in% installed.packages()[,1])) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed.packages()[,1])) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(sf)\nrequire(tidyverse)\nrequire(ggplot2)\nrequire(ggsflabel)\n\nif(!file.exists(\"covid.geojson\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid.geojson\",\n\"covid.geojson\", mode = \"wb\", quiet = TRUE)\n\nread_sf(\"covid.geojson\") %>%\n  filter(regionName == \"North West\") ->\n  covid\n\n\nLooking at a map of the COVID-19 rates, it appears that there are patches of higher rates that tend to cluster in cities such as Preston, Manchester and Liverpool, although not exclusively so\n\n\nCode\ncovid %>%\n  filter(Rate > 400) %>%\n  select(LtlaName) %>%   # LtlaName is the name of the local authority\n  filter(!duplicated(LtlaName)) ->\n  high_rate\n\nggplot() +\n  geom_sf(data = covid, aes(fill = Rate), size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  geom_sf_label_repel(data = high_rate, aes(label = LtlaName), size = 3, alpha = 0.5) +\n  theme_void()\n\n\n\n\n\n… and not without variation within the cities such as Manchester:\n\n\nCode\nggplot(data = covid %>% filter(LtlaName == \"Manchester\")) +\n  geom_sf(aes(fill = Rate), size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  theme_void()\n\n\n\n\n\nAcross the map there appears to be a pattern of positive spatial autocorrelation in the COVID-19 rates of contiguous neighbours, whether this is measured using a Moran test,\n\n\nCode\nrequire(spdep)\nspweight <- nb2listw(poly2nb(covid, snap = 1))\nmoran.test(covid$Rate, spweight)\n\n\n\n    Moran I test under randomisation\n\ndata:  covid$Rate  \nweights: spweight    \n\nMoran I statistic standard deviate = 24.422, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4919206758     -0.0010834236      0.0004075223 \n\n\nor using the Pearson correlation,\n\n\nCode\ncor(lag.listw(spweight, covid$Rate), covid$Rate)\n\n\n[1] 0.6726405\n\n\nSome of the ‘hot spots’ of infection are suggested using the geographically-weighted means,\n\n\nCode\nrequire(GWmodel)\ncovid_sp <- as_Spatial(covid)\nbw <- bw.gwr(Rate ~ 1, data = covid_sp,\n             adaptive = TRUE, kernel = \"bisquare\", longlat = F)\ngwstats <- gwss(covid_sp, vars = \"Rate\", bw = bw, kernel = \"bisquare\",\n                adaptive = TRUE, longlat = F)\n\n\nPlotting these,\n\n\nCode\ncovid$Rate_LM <- gwstats$SDF$Rate_LM\nggplot(covid, aes(fill = Rate_LM)) +\n  geom_sf(size = 0.25) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  theme_void()\n\n\n\n\n\nThey also are suggested using the G-statistic with a somewhat arbitrary bandwidth of 5km.\n\n\nCode\ncoords <- st_centroid(covid, of_largest_polygon = TRUE)\nneighbours <- dnearneigh(coords, 0, 5000)\nspweightB <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\ncovid$localG <- localG(covid$Rate, spweightB)\nbrks <- c(min(covid$localG, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$localG, na.rm = TRUE))\ncovid$localG_gp <- cut(covid$localG, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = localG_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "spregress.html#an-initial-model-to-explain-the-spatial-patterns-in-the-covid-19-rates",
    "href": "spregress.html#an-initial-model-to-explain-the-spatial-patterns-in-the-covid-19-rates",
    "title": "Spatial Regression",
    "section": "An initial model to explain the spatial patterns in the COVID-19 rates",
    "text": "An initial model to explain the spatial patterns in the COVID-19 rates\nIt is likely that the spatial variation in the COVID-19 rates is due to differences in the physical attributes of the different neighbourhoods and/or of the populations who live in them. For example, the rates may be related to the relative level of deprivation in the neighbourhoods (the Index of Multiple Deprivation, IMD), the age composition of their populations (e.g. percentage aged 0 to 11, age0.11), the population density (density), the number of carehome beds (carebeds, because particularly early on in the pandemic, carehome residents were at very high risk), and whether they contain an Accident and Emergency hospital (AandE, coded 1 if they do and 0 if they don’t). Incorporating this into a standard regression model gives,\n\n\nCode\nols1 <- lm(Rate ~ IMD + age0.11 + age12.17 + age18.24 + age25.34 + age35.39 + \n             age50.59 + age60.69 + age70plus + density +\n             carebeds + AandE, data = covid)\nsummary(ols1)\n\n\n\nCall:\nlm(formula = Rate ~ IMD + age0.11 + age12.17 + age18.24 + age25.34 + \n    age35.39 + age50.59 + age60.69 + age70plus + density + carebeds + \n    AandE, data = covid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-134.816  -17.761    1.592   20.124  139.444 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 182.26850  113.79175   1.602 0.109553    \nIMD          -0.29381    0.10610  -2.769 0.005736 ** \nage0.11       2.21553    1.36681   1.621 0.105376    \nage12.17     -2.36944    1.94032  -1.221 0.222341    \nage18.24     -0.16683    1.16041  -0.144 0.885719    \nage25.34      2.31424    1.19221   1.941 0.052550 .  \nage35.39      6.65665    2.65716   2.505 0.012413 *  \nage50.59      6.31033    1.74161   3.623 0.000307 ***\nage60.69      0.34605    1.46550   0.236 0.813386    \nage70plus    -0.71097    1.23595  -0.575 0.565269    \ndensity     164.39302  636.21170   0.258 0.796162    \ncarebeds      0.03583    0.01489   2.407 0.016292 *  \nAandE         1.13369    4.03313   0.281 0.778703    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.75 on 911 degrees of freedom\nMultiple R-squared:  0.2039,    Adjusted R-squared:  0.1934 \nF-statistic: 19.44 on 12 and 911 DF,  p-value: < 2.2e-16\n\n\n which, as its R-squared value of 0.204 suggests, goes some way to explaining the variation in the rates. Looking at R’s standard diagnostic plots there is not any strong evidence that the residual Normality assumption of the regression is being violated although, not very surprisingly, the variance inflation values (VIF) do suggest high levels of colinearity between the age variables.\n\n\nCode\npar(mfrow = c(2,2))\nplot(ols1)\n\n\n\n\n\nCode\nif(!(\"car\" %in% installed.packages()[,1])) install.packages(\"car\")\nrequire(car)\nvif(ols1)\n\n\n      IMD   age0.11  age12.17  age18.24  age25.34  age35.39  age50.59  age60.69 \n 2.834069 18.836543  6.050232 38.347245 27.513979 10.228388 16.430177 14.397087 \nage70plus   density  carebeds     AandE \n38.881759  2.123517  1.135127  1.030432 \n\n\n\nThere are no hard and fast rules but, in broad terms, a VIF value of 4 or 5 or above is considering and a value above 10 suggests a high level of multicollinearity.\n Although I am generally cautious about automated selection procedures, in this case a stepwise model selection appears useful to address the colinearity:\n\n\nCode\nols2 <- step(ols1)\n\n\nThe results are,\n\n\nCode\nsummary(ols2)\n\n\n\nCall:\nlm(formula = Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + \n    carebeds, data = covid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-133.943  -18.266    1.855   20.205  147.079 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 147.22402   14.35674  10.255  < 2e-16 ***\nIMD          -0.25971    0.09411  -2.760 0.005903 ** \nage0.11       1.81138    0.53224   3.403 0.000695 ***\nage25.34      2.90621    0.47569   6.109 1.48e-09 ***\nage35.39      7.27665    1.53525   4.740 2.48e-06 ***\nage50.59      6.65925    0.63811  10.436  < 2e-16 ***\ncarebeds      0.03328    0.01417   2.348 0.019069 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.7 on 917 degrees of freedom\nMultiple R-squared:  0.2011,    Adjusted R-squared:  0.1959 \nF-statistic: 38.48 on 6 and 917 DF,  p-value: < 2.2e-16\n\n\nCode\nvif(ols2)\n\n\n     IMD  age0.11 age25.34 age35.39 age50.59 carebeds \n2.236588 2.865175 4.394042 3.425196 2.212515 1.031632 \n\n\nIt may seem surprising to discover that the deprivation index is negatively correlated with the COVID rate – implying more deprivation, fewer infections – but this is due to the later variants of the disease which have spread quickly through more affluent populations when restrictions on mobility and social interaction have been relaxed."
  },
  {
    "objectID": "spregress.html#spatial-dependencies-in-the-model-residuals",
    "href": "spregress.html#spatial-dependencies-in-the-model-residuals",
    "title": "Spatial Regression",
    "section": "Spatial dependencies in the model residuals",
    "text": "Spatial dependencies in the model residuals\nAlthough the model appears to fit the data reasonably well, there is a problem. The residuals are supposed to be random noise, meaning their values should be independent of their location and of each other, with no spatial structure. They are not. In fact, if we apply a Moran’s test to the residuals, using the test for regression residuals, lm.morantest(), we find that they are significantly spatially correlated:\n\n\nCode\nlm.morantest(ols2, spweight)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age35.39 +\nage50.59 + carebeds, data = covid)\nweights: spweight\n\nMoran I statistic standard deviate = 22.357, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.4461249282    -0.0033794098     0.0004042296 \n\n\n The pattern is evident if we map the standardised residuals from the function rstandard(). At the risk of geographical over-simplification, there is something of a north-south divide, with a patch of negative residuals to the north of the study region. These are rural neighbouhoods where the rate of COVID-19 cases is lower than the model predicts.\n\n\nCode\ncovid$resids <- rstandard(ols2)\nbrks <- c(min(covid$resids, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$resids, na.rm = TRUE))\ncovid$resids_gp <- cut(covid$resids, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = resids_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"Standardised residual\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\n\nLooking again at the model\nIt is possible that the spatial structure in the residuals exists because the model has been mis-specified. In particular, we might wonder if it was a mistake to drop the population density variable which perhaps had a polynomial relationship with the COVID rates. Let’s find out.\n\n\nCode\nols3 <- update(ols2, . ~ . + poly(density, 2))\nsummary(ols3)\n\n\n\nCall:\nlm(formula = Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + \n    carebeds + poly(density, 2), data = covid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-125.385  -17.887    1.057   20.094  132.985 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        185.32682   15.18181  12.207  < 2e-16 ***\nIMD                 -0.33794    0.09407  -3.592 0.000345 ***\nage0.11              1.51411    0.51892   2.918 0.003612 ** \nage25.34             3.15458    0.46379   6.802 1.87e-11 ***\nage35.39             3.95864    1.55519   2.545 0.011077 *  \nage50.59             5.67418    0.69588   8.154 1.15e-15 ***\ncarebeds             0.02900    0.01378   2.105 0.035593 *  \npoly(density, 2)1   23.82432   43.11516   0.553 0.580690    \npoly(density, 2)2 -267.38867   35.45138  -7.542 1.11e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.79 on 915 degrees of freedom\nMultiple R-squared:  0.248, Adjusted R-squared:  0.2414 \nF-statistic: 37.72 on 8 and 915 DF,  p-value: < 2.2e-16\n\n\n Certainly, of the three models, this is the best fit to the data, as we can see from the various model fit diagnostics that are easily gathered together using the glance() function from the tidyverse package, broom.\n\n\nCode\nrequire(broom)\nbind_rows(glance(ols1), glance(ols2), glance(ols3))\n\n\n# A tibble: 3 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.204         0.193  31.7      19.4 4.88e-38    12 -4500. 9027. 9095.\n2     0.201         0.196  31.7      38.5 8.36e-42     6 -4501. 9018. 9057.\n3     0.248         0.241  30.8      37.7 5.93e-52     8 -4473. 8966. 9015.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nAn analysis of variance also suggests that the model fit has improved significantly.\n\n\nCode\nanova(ols2, ols3)\n\n\nAnalysis of Variance Table\n\nModel 1: Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + carebeds\nModel 2: Rate ~ IMD + age0.11 + age25.34 + age35.39 + age50.59 + carebeds + \n    poly(density, 2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    917 921337                                  \n2    915 867305  2     54032 28.502 9.819e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHowever, there is still spatial autocorrelation left in the model residuals, albeit slightly reduced from before.\n\n\nCode\nlm.morantest(ols2, spweight)$estimate\n\n\nObserved Moran I      Expectation         Variance \n    0.4461249282    -0.0033794098     0.0004042296 \n\n\nCode\nlm.morantest(ols3, spweight)$estimate\n\n\nObserved Moran I      Expectation         Variance \n    0.4133475904    -0.0037610429     0.0004036032 \n\n\nThe map of the residuals now looks like:\n\n\nCode\ncovid$resids <- rstandard(ols3)\nbrks <- c(min(covid$resids, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(covid$resids, na.rm = TRUE))\ncovid$resids_gp <- cut(covid$resids, brks, include.lowest = TRUE)\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\nggplot(covid, aes(fill = resids_gp)) +\n  geom_sf(size = 0.25) +\n  scale_fill_manual(\"Standardised residual\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_void() +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "spregress.html#spatial-regression-models",
    "href": "spregress.html#spatial-regression-models",
    "title": "Spatial Regression",
    "section": "Spatial regression models",
    "text": "Spatial regression models\n\nSpatial error model\nOne way to handle the error structure is to fit a spatial simultaneous autoregressive error model which decomposes the error (the residuals) into two parts: a spatially lagged component and a remaining error: \\(y = X\\beta + \\lambda W \\xi + \\epsilon\\). The model can be fitted using R’s spatialreg package.\n\n\nCode\nif(!(\"spatialreg\" %in% installed.packages()[,1])) install.packages(\"spatialreg\", dependencies = TRUE)\nrequire(spatialreg)\nerrmod <- errorsarlm(formula(ols3), data = covid, listw = spweight)\nsummary(errmod)\n\n\n\nCall:errorsarlm(formula = formula(ols3), data = covid, listw = spweight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-88.9057 -13.5631   1.0775  15.1597 108.7168 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                     Estimate  Std. Error z value  Pr(>|z|)\n(Intercept)        228.423016   13.194612 17.3118 < 2.2e-16\nIMD                 -0.595478    0.089535 -6.6508 2.915e-11\nage0.11              1.885127    0.449453  4.1943 2.738e-05\nage25.34             2.612570    0.416075  6.2791 3.406e-10\nage35.39             3.729977    1.369957  2.7227  0.006475\nage50.59             3.339295    0.608468  5.4880 4.064e-08\ncarebeds             0.051339    0.010181  5.0426 4.592e-07\npoly(density, 2)1 -111.989441   38.123935 -2.9375  0.003309\npoly(density, 2)2 -120.005371   28.931984 -4.1478 3.356e-05\n\nLambda: 0.69751, LR test value: 348.94, p-value: < 2.22e-16\nAsymptotic standard error: 0.028893\n    z-value: 24.142, p-value: < 2.22e-16\nWald statistic: 582.82, p-value: < 2.22e-16\n\nLog likelihood: -4298.755 for error model\nML residual variance (sigma squared): 572.93, (sigma: 23.936)\nNumber of observations: 924 \nNumber of parameters estimated: 11 \nAIC: 8619.5, (AIC for lm: 8966.5)\n\n\nThe model is a better fit to the data, as the following diagnostics tell us (the lower the AIC the better)\n\n\nCode\nglance(ols3)$r.squared\n\n\n[1] 0.2479886\n\n\nCode\nglance(errmod)$r.squared\n\n\n[1] 0.5543411\n\n\nCode\nAIC(ols3)\n\n\n[1] 8966.455\n\n\nCode\nAIC(errmod)\n\n\n[1] 8619.511\n\n\nThe differences are such that there is little doubt that the error model offers a much improved fit but if we do wish to test that difference statistically then\n\n\nCode\nlogLik(ols3)\n\n\n'log Lik.' -4473.228 (df=10)\n\n\nCode\nlogLik(errmod)\n\n\n'log Lik.' -4298.755 (df=11)\n\n\nCode\ndegf <- attr(logLik(errmod), \"df\") - attr(logLik(ols3), \"df\")\nLR <- -2 * (logLik(ols3) - logLik(errmod))\nLR > qchisq(0.99, degf)\n\n\n[1] TRUE\n\n\nUsing the error model changes the estimates of the regression coefficients.\n\n\nCode\ntidy(ols3)\n\n\n# A tibble: 9 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        185.       15.2       12.2   7.36e-32\n2 IMD                 -0.338     0.0941    -3.59  3.45e- 4\n3 age0.11              1.51      0.519      2.92  3.61e- 3\n4 age25.34             3.15      0.464      6.80  1.87e-11\n5 age35.39             3.96      1.56       2.55  1.11e- 2\n6 age50.59             5.67      0.696      8.15  1.15e-15\n7 carebeds             0.0290    0.0138     2.10  3.56e- 2\n8 poly(density, 2)1   23.8      43.1        0.553 5.81e- 1\n9 poly(density, 2)2 -267.       35.5       -7.54  1.11e-13\n\n\nCode\ntidy(errmod)\n\n\n# A tibble: 10 × 5\n   term               estimate std.error statistic  p.value\n   <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)        228.       13.2        17.3  0       \n 2 IMD                 -0.595     0.0895     -6.65 2.91e-11\n 3 age0.11              1.89      0.449       4.19 2.74e- 5\n 4 age25.34             2.61      0.416       6.28 3.41e-10\n 5 age35.39             3.73      1.37        2.72 6.48e- 3\n 6 age50.59             3.34      0.608       5.49 4.06e- 8\n 7 carebeds             0.0513    0.0102      5.04 4.59e- 7\n 8 poly(density, 2)1 -112.       38.1        -2.94 3.31e- 3\n 9 poly(density, 2)2 -120.       28.9        -4.15 3.36e- 5\n10 lambda               0.698     0.0289     24.1  0       \n\n\nFor example, where the 95% confidence interval for age50.59 used to be 95CI [4.31, 7.04], under the error model it is 95CI [2.15, 4.53].\n\n\nCode\nconfint(ols3)\n\n\n                          2.5 %        97.5 %\n(Intercept)        1.555316e+02  215.12202256\nIMD               -5.225593e-01   -0.15331307\nage0.11            4.956890e-01    2.53253071\nage25.34           2.244363e+00    4.06480619\nage35.39           9.064758e-01    7.01079426\nage50.59           4.308470e+00    7.03988250\ncarebeds           1.958051e-03    0.05604307\npoly(density, 2)1 -6.079177e+01  108.44040596\npoly(density, 2)2 -3.369641e+02 -197.81320842\n\n\nCode\nconfint(errmod)\n\n\n                         2.5 %       97.5 %\nlambda               0.6408859   0.75414261\n(Intercept)        202.5620509 254.28398018\nIMD                 -0.7709634  -0.41999350\nage0.11              1.0042143   2.76603881\nage25.34             1.7970781   3.42806142\nage35.39             1.0449095   6.41504382\nage50.59             2.1467207   4.53186972\ncarebeds             0.0313844   0.07129323\npoly(density, 2)1 -186.7109793 -37.26790212\npoly(density, 2)2 -176.7110180 -63.29972471\n\n\n\n\nSpatially lagged y model\nAlthough the spatial error model fits the data better than the standard OLS model, it tells us only that there is an unexplained spatial structure to the residuals, not what caused it. It may offer better estimates of the model parameters and their statistical significance but it does not presuppose any particular spatial process generating the patterns in the values. A different model that explicitly tests for whether the value at a point is functionally dependent on the values of neighbouring points is the spatially lagged y model: \\(y = \\rho Wy + X\\beta + \\epsilon\\). It models an ‘overspill’ or chain effect where the outcome (the Y value) at any location is affected by the outcomes at surrounding locations.\n\n\nCode\nlagmod <- lagsarlm(formula(ols3), data = covid, listw = spweight)\nsummary(lagmod)\n\n\n\nCall:lagsarlm(formula = formula(ols3), data = covid, listw = spweight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-98.8140 -14.8770   1.0467  14.7420 110.1081 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n                     Estimate  Std. Error z value  Pr(>|z|)\n(Intercept)         17.353057   14.403449  1.2048 0.2282864\nIMD                 -0.371501    0.074903 -4.9598 7.057e-07\nage0.11              1.377790    0.411239  3.3503 0.0008071\nage25.34             2.678105    0.367344  7.2905 3.089e-13\nage35.39             1.885234    1.235376  1.5260 0.1269996\nage50.59             3.636195    0.553621  6.5680 5.099e-11\ncarebeds             0.042747    0.010887  3.9264 8.623e-05\npoly(density, 2)1  -69.714321   34.062240 -2.0467 0.0406900\npoly(density, 2)2 -156.910150   28.073145 -5.5893 2.279e-08\n\nRho: 0.63167, LR test value: 342.73, p-value: < 2.22e-16\nAsymptotic standard error: 0.02966\n    z-value: 21.297, p-value: < 2.22e-16\nWald statistic: 453.57, p-value: < 2.22e-16\n\nLog likelihood: -4301.863 for lag model\nML residual variance (sigma squared): 591.59, (sigma: 24.323)\nNumber of observations: 924 \nNumber of parameters estimated: 11 \nAIC: 8625.7, (AIC for lm: 8966.5)\nLM test for residual autocorrelation\ntest value: 3.5747, p-value: 0.058666\n\n\nHere, the test for residual autocorrelation does not generate a statistically significant result at a 95% level, although it is close.\nNote that the beta estimates of the lagged y-model cannot be interpreted in the same way as for a standard OLS model. For example, the beta estimate of -0.372 for the IMD variable does not mean that if (hypothetically) we increased that variable by one unit at each location we should then expect the COVID-19 to everywhere decrease by 0.372 holding the other X variables constant. That is the correct interpretation for a standard (OLS) regression model and also for the spatial error model but not for where the lag of the Y variable is included as a predictor variable. The reason is because if we did raise the IMD value it would start something akin to a ‘chain reaction’ through the feedback of Y via the lagged Y values. The total impact is a sum of the direct effect – what is predicted to happen through the 1 unit change in IMD – and the indirect effect, which is that caused by ‘the chain reaction’ / feedback / ‘overspill’ in the system:\n\n\nCode\nimpacts(lagmod, listw = spweight)\n\n\nImpact measures (lag, exact):\n                        Direct      Indirect       Total\nIMD                 -0.4123765   -0.59624269   -1.008619\nage0.11              1.5293852    2.21129145    3.740677\nage25.34             2.9727718    4.29824022    7.271012\nage35.39             2.0926627    3.02571725    5.118380\nage50.59             4.0362781    5.83593162    9.872210\ncarebeds             0.0474502    0.06860679    0.116057\npoly(density, 2)1  -77.3848500 -111.88839883 -189.273249\npoly(density, 2)2 -174.1746633 -251.83384334 -426.008507\n\n\nAlthough it wasn’t especially noticeable, there was a short pause as those impacts were calculated. The calculations could take much longer if the size of the spatial weights matrix was larger. As this author notes, after Lesage and Pace, 2009, a faster approximation method can be used, here with R = 1000 simulated distributions for the impact measures.\n\n\nCode\nW <- as(spweight, \"CsparseMatrix\")\ntrMC <- trW(W, type=\"MC\")\nim <-summary(impacts(lagmod, tr = trMC, R = 1000), zstats = TRUE)\ndata.frame(im$res, row.names = names(lagmod$coefficients)[-1])   # The [-1] is to omit the intercept\n\n\n                         direct      indirect        total\nIMD                 -0.41314668   -0.59547151   -1.0086182\nage0.11              1.53224138    2.20843137    3.7406727\nage25.34             2.97832360    4.29268089    7.2710045\nage35.39             2.09657083    3.02180379    5.1183746\nage50.59             4.04381607    5.82838344    9.8721995\ncarebeds             0.04753881    0.06851806    0.1160569\npoly(density, 2)1  -77.52937047 -111.74368257 -189.2730530\npoly(density, 2)2 -174.49994405 -251.50812188 -426.0080659\n\n\nAn advantage of this approach is that we can also obtain z and p-values for the impact measures:\n\n\nCode\ndata.frame(im$zmat, row.names = names(lagmod$coefficients)[-1])\n\n\n                     Direct  Indirect     Total\nIMD               -4.849355 -4.344170 -4.691811\nage0.11            3.325582  3.087686  3.234782\nage25.34           7.123677  5.378026  6.285517\nage35.39           1.577504  1.549798  1.569504\nage50.59           6.416108  5.099640  5.826948\ncarebeds           3.938185  3.476458  3.730092\npoly(density, 2)1 -2.100254 -2.016281 -2.065373\npoly(density, 2)2 -5.721376 -4.875699 -5.409268\n\n\nCode\ndata.frame(im$pzmat, row.names = names(lagmod$coefficients)[-1])\n\n\n                        Direct     Indirect        Total\nIMD               1.238635e-06 1.398031e-05 2.707979e-06\nage0.11           8.823426e-04 2.017215e-03 1.217357e-03\nage25.34          1.050937e-12 7.530710e-08 3.267648e-10\nage35.39          1.146797e-01 1.211899e-01 1.165306e-01\nage50.59          1.398024e-10 3.403003e-07 5.645009e-09\ncarebeds          8.210023e-05 5.080847e-04 1.914101e-04\npoly(density, 2)1 3.570647e-02 4.377064e-02 3.888772e-02\npoly(density, 2)2 1.056648e-08 1.084238e-06 6.328280e-08\n\n\nMost but not all of the impacts are significant. We can drop age35.39 from the model with little loss of fit.\n\n\nCode\nlagmod2 <- lagsarlm(Rate ~  IMD + age0.11 + age25.34 + age50.59 + carebeds + \n    poly(density, 2), data = covid, listw = spweight)\nanova(lagmod, lagmod2)\n\n\n        Model df    AIC  logLik Test L.Ratio p-value\nlagmod      1 11 8625.7 -4301.9    1                \nlagmod2     2 10 8626.1 -4303.0    2  2.3408 0.12602\n\n\nCode\nbind_rows(glance(lagmod), glance(lagmod2))\n\n\n# A tibble: 2 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      <dbl> <dbl> <dbl>    <dbl>  <dbl> <int>\n1     0.532 8626. 8679.  546629. -4302.   924\n2     0.532 8626. 8674.  547460. -4303.   924\n\n\n In fact, an increase of 1 unit in, for example, the IMD variable, will play out slightly differently in different places (because they have different neighbours with different Y values and at different distances from each other). The code below, which is from the first edition of Ward & Gleditsch (2008, pp.47), will calculate the impact (of a one unit change in IMD) at each location but below I have limited it to the first ten so that it doesn’t take too long to run.\n\n\nCode\nn <- nrow(covid)\nI <- matrix(0, nrow = n, ncol = n)\ndiag(I) <- 1\nrho <- lagmod$rho\nbeta <- lagmod$coefficients[\"age25.34\"]\nweights.matrix <- listw2mat(spweight)\nresults <- rep(NA, times=10)\nresults <- sapply(1:10, \\(i) {\n  xvector <- rep(0, times=n)\n  xvector[i] <- 1\n  impact <- solve(I - rho * weights.matrix) %*% xvector * beta\n  results[i] <- impact[i]\n})\nresults\n\n\n [1] 2.953840 2.973231 2.949515 2.908907 3.025483 2.901597 2.949316 2.960332\n [9] 2.949171 2.946591\n\n\n\n\nChoosing between the spatial error and lagged y model\nBefore fitting the spatial error and lagged y models, we could have looked for evidence in support of them using the function lm.LMtests(). This tests the basic OLS specification against the more general spatial error and lagged y models. Anselin and Rey (2014, p.110) offer the following decision tree that can, in the absence of a more theoretical basis for the model choice (e.g. the type of process being modelled), be used in conjunction with the test results to help select the model.\n\nSource: Modern Spatial Econometrics in Practice\nIn the first step, we find that in this instance that both the LM-Error and LM-Lag tests are significant.\n\n\nCode\nols4 <- lm(Rate ~  IMD + age0.11 + age25.34 + age50.59 + carebeds + poly(density, 2), data = covid)\nlm.LMtests(ols4, spweight, test=c(\"LMerr\", \"LMlag\"))\n\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nLMerr = 411.48, df = 1, p-value < 2.2e-16\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nLMlag = 463.85, df = 1, p-value < 2.2e-16\n\n\nMoving on to the robust tests, only the LM-Lag test is significant. Given the nature of COVID-19 as an infectious disease, it does seem reasonable to suppose that high rates of infection in any an area will ‘overspill’ into neighbouring areas too.\n\n\nCode\nlm.LMtests(ols4, spweight, test=c(\"RLMerr\", \"RLMlag\"))\n\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nRLMerr = 2.3319, df = 1, p-value = 0.1267\n\n\n    Lagrange multiplier diagnostics for spatial dependence\n\ndata:  \nmodel: lm(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 +\ncarebeds + poly(density, 2), data = covid)\nweights: spweight\n\nRLMlag = 54.693, df = 1, p-value = 1.409e-13"
  },
  {
    "objectID": "spregress.html#a-geographically-weighted-spatial-weights-matrix",
    "href": "spregress.html#a-geographically-weighted-spatial-weights-matrix",
    "title": "Spatial Regression",
    "section": "A geographically weighted spatial weights matrix",
    "text": "A geographically weighted spatial weights matrix\nAs with the tests of spatial autocorrelation in an earlier session and as with the geographically weighted statistics, the results of the spatial regression models are dependent on the specification of the spatial weights matrix which can suffer from being somewhat arbitrary. We could, if we wish, try calibrating it around the geographically weighted mean.\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(Rate ~ 1, data = covid_sp, adaptive = TRUE, kernel = \"bisquare\")\n\n\nTo now create the corresponding inverse distance spatial weights matrix for the models, we can use the nn2 function in the RANN package to find the bw = 18 nearest neighbours to each centroid point:\n\n\nCode\nif(!(\"RANN\" %in% installed.packages()[,1]))  install.packages(\"RANN\")\nrequire(RANN)\ncoords <- st_coordinates(st_centroid(st_geometry(covid)))\nknn <- nn2(coords, coords, k = bw)\n\n\nHaving done so, those neighbours can be geographically weighted using a bisquare kernel, as in the GWR calibration above. (A Gaussian kernel is obtained if (1 - (x / max(x))^2)^2 is replaced by exp(-0.5 * (x / max(x))^2) in the code below).\n\n\nCode\nd <- knn$nn.dists\nglist <- apply(d, 1, \\(x) {\n  (1 - (x / max(x))^2)^2\n}, simplify = FALSE)\nknearnb <- knn2nb(knearneigh(st_centroid(st_geometry(covid)), k = bw))\nspweightK <- nb2listw(knearnb, glist, style = \"C\")\n\n\nThe results are not very successful though. The lag model with contiguous spatial weights fits the data better,\n\n\nCode\nlagmod3 <- lagsarlm(formula(ols4), data = covid, listw = spweightK)\nbind_rows(glance(lagmod2), glance(lagmod3))\n\n\n# A tibble: 2 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      <dbl> <dbl> <dbl>    <dbl>  <dbl> <int>\n1     0.532 8626. 8674.  547460. -4303.   924\n2     0.254 8959. 9008.  860562. -4470.   924\n\n\nIn addition, the inverse distance weights are not row-standardised, which may cause some knock-on problems. They could become row-standardised (spweightK <- nb2listw(knearnb, glist, style = \"W\")) but if they are, by re-scaling each row of the weights to equal one, then the inverse distance weighting is altered."
  },
  {
    "objectID": "spregress.html#geographically-weighted-regression",
    "href": "spregress.html#geographically-weighted-regression",
    "title": "Spatial Regression",
    "section": "Geographically Weighted Regression",
    "text": "Geographically Weighted Regression\nIn much the same way as we can calculate geographically weighted statistics that allow the measured value to vary locally from location to location across the study region, we can also calculate geographically weighted and locally estimated regression coefficients using geographically weighted regression. This is used to look for spatial variation in the regression relationships.\nIf the bandwidth is not known or there is no theoretical justification for it, it can be found using an automated selection procedure.\n\n\nCode\nrequire(GWmodel)\nbw <- bw.gwr(formula(ols4), data = covid_sp, adaptive = TRUE)\n\n\nThe model can then be fitted.\n\n\nCode\ngwrmod <- gwr.basic(formula(ols4), data = covid_sp, adaptive = TRUE, bw = bw)\ngwrmod\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-01-17 12:55:50 \n   Call:\n   gwr.basic(formula = formula(ols4), data = covid_sp, bw = bw, \n    adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density\n   Number of data points: 924\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-123.194  -18.380    1.473   20.623  127.554 \n\n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)        188.50375   15.17560  12.422  < 2e-16 ***\n   IMD                 -0.39649    0.09149  -4.334 1.63e-05 ***\n   age0.11              2.30191    0.41777   5.510 4.66e-08 ***\n   age25.34             3.93517    0.34898  11.276  < 2e-16 ***\n   age50.59             5.80810    0.69596   8.345 2.59e-16 ***\n   carebeds             0.02904    0.01382   2.101   0.0359 *  \n   poly(density, 2)1   29.14870   43.19300   0.675   0.4999    \n   poly(density, 2)2 -292.74904   34.12420  -8.579  < 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 30.88 on 916 degrees of freedom\n   Multiple R-squared: 0.2427\n   Adjusted R-squared: 0.2369 \n   F-statistic: 41.93 on 7 and 916 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 873446.4\n   Sigma(hat): 30.77887\n   AIC:  8970.975\n   AICc:  8971.172\n   BIC:  8151.892\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 80 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                            Min.     1st Qu.      Median     3rd Qu.     Max.\n   Intercept         -3.9934e+01  1.3693e+02  2.1422e+02  2.6180e+02 419.5064\n   IMD               -2.7499e+00 -7.8059e-01 -4.4223e-01 -1.6702e-01   0.6501\n   age0.11           -4.0541e+00  2.3759e-01  3.4220e+00  6.1701e+00  12.5921\n   age25.34          -3.9387e+00  1.6469e+00  2.6274e+00  4.2843e+00  13.8157\n   age50.59          -9.5346e+00  2.1727e+00  5.3255e+00  8.6233e+00  14.3515\n   carebeds          -5.6126e-02  2.5983e-02  5.8662e-02  8.7463e-02   0.2220\n   poly(density, 2)1 -2.7204e+03 -4.1814e+02 -1.5602e+02  4.1180e-01 667.9061\n   poly(density, 2)2 -2.1980e+03 -5.4619e+02 -2.5300e+02 -1.2247e+02 643.0451\n   ************************Diagnostic information*************************\n   Number of data points: 924 \n   Effective number of parameters (2trace(S) - trace(S'S)): 270.2441 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 653.7559 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 8552.959 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 8216.682 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 8513.32 \n   Residual sum of squares: 313860.8 \n   R-square value:  0.7278617 \n   Adjusted R-square value:  0.6151951 \n\n   ***********************************************************************\n   Program stops at: 2023-01-17 12:55:50 \n\n\nThe output requires some interpretation! The results of the global regression are those obtained from a standard OLS regression (i.e. summary(lm(formula(ols4), data = covid_sp))). It is ‘global’ because it is a model for the entire study region, without spatial variation in the regression estimates. The results of the geographically weighted regression are the results from, in this case, 924 separate but spatially overlapping regression models fitted, with geographic weighting, to spatial subsets of the data. All the local estimates are contained in the spatial data frame, gwrmod$SDF, including those for age0.11 which are in gwrmod$SDF$age0.11 and have the following distribution,\n\n\nCode\nsummary(gwrmod$SDF$age0.11)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-4.0541  0.2376  3.4220  3.3645  6.1701 12.5921 \n\n\nThis is the same summary of the distribution that is included in the GWR coefficient estimates. What it means is that in at least one location the relationship of age0.11 to Rate is, conditional on the other variables, negative. It is more usual for it to be positive – more children, more infections (perhaps a school effect?) – but the estimated effect size varies quite substantially. Its interquartile range is from 0.238 to 6.17. Mapping these local estimates reveals an interesting geography – it is largely in and around Manchester where a greater number of children of primary school age or younger appears to lower the infection rate.\n\n\nCode\ncovid$age0.11GWR <- gwrmod$SDF$age0.11\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nWith a bit of effort we can get all the variables on the same chart…\n\n\nCode\nif(!(\"gridExtra\" %in% installed.packages()[,1])) install.packages(\"gridExtra\", dependencies = TRUE)\nrequire(gridExtra)\ng <- lapply(2:8, \\(j) {\n  st_as_sf(gwrmod$SDF[, j]) %>%\n    rename(beta = 1) %>%\n    ggplot(aes(fill = beta)) +\n      geom_sf(col = NA) +\n      scale_fill_gradient2(\"beta\", mid = \"grey90\", trans = \"reverse\") +\n      theme_void() +\n      guides(fill = guide_colourbar(reverse = TRUE)) +\n      ggtitle(names(gwrmod$SDF[j]))\n})\ngrid.arrange(grobs = g)\n\n\n\n\n\n… however, not all of these regression estimates are necessarily statistically significant. If we use the estimated t-values to isolate those that are not, then, for the age0.11 variable we obtain,\n\n\nCode\ncovid$age0.11GWR[abs(gwrmod$SDF$age0.11_TV) > 1.96] <- NA\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\nand, with a bit more data wrangling, for the full set,\n\n\nCode\nsdf <- slot(gwrmod$SDF, \"data\")\ng <- lapply(2:8, \\(j) {\n  x <- st_as_sf(gwrmod$SDF[, j]) \n  t <- which(names(sdf) == paste0(names(x)[1],\"_TV\"))\n  x[abs(sdf[, t]) > 1.96, 1] <- NA\n  x %>%\n    rename(beta = 1) %>%\n    ggplot(aes(fill = beta)) +\n      geom_sf(col = NA) +\n      scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n      theme_void() +\n      guides(fill = guide_colourbar(reverse = TRUE)) +\n      ggtitle(names(gwrmod$SDF[j]))\n})\ngrid.arrange(grobs = g)\n\n\n\n\n\n\nI am sure there is a more elegant way of doing this. Don’t worry about the code especially - the point is that not every estimate, everywhere is significant.\n As with the basic GW statistics, we can undertake a randomisation test to suggest whether the spatial variation in the coefficient estimates is statistically significant. However this takes some time to run – with the default nsims = 99 simulations it took about 12 minutes on my laptop. I have ‘commented it out’ in the code block below with the suggestion that you don’t run it now.\n\n\nCode\n# I suggest you do not run this\n# gwr.mc <- gwr.montecarlo(formula(ols4), covid_sp, adaptive = TRUE, bw = bw)\n\n\nInstead, let’s jump straight to the results, which I have saved in a pre-prepared workspace.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/gwrmc.RData\")\nload(url)\nclose(url)\ngwr.mc\n\n\n                  p-value\n(Intercept)          0.40\nIMD                  0.38\nage0.11              0.00\nage25.34             0.15\nage50.59             0.22\ncarebeds             0.97\npoly(density, 2)1    0.00\npoly(density, 2)2    0.00\n\n\n\nMixed GWR\nIt appears that only the age0.11 and density variables exhibit significant spatial variation. In principle it is possible to drop the other variables out from the local estimates and treat them as fixed, global estimates instead, in a Mixed GWR model (mixed global and local regression estimates). In practice, it is generating an error.\n\n\nCode\n# This appears to generate an error\ngwrmixd <- gwr.mixed(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds + poly(density, 2),\n          data = covid_sp,\n          fixed.vars = c(\"IMD\", \"age25.34\", \"age50.59\", \"carebeds\"),\n          bw = bw, adaptive = TRUE)\n\n\nWe can work around this, though, using partial regressions, regressing Rate, age0.11 and poly(density, 2) on the fixed variables and using their residuals.\n\n\nCode\ndensity.1 <- poly(covid$density, 2)[, 1]\ndensity.2 <- poly(covid$density, 2)[, 2]\ncovid_sp$RateR <- residuals(lm(Rate ~ IMD + age25.34 + age50.59 + carebeds, data = covid_sp))\ncovid_sp$age0.11R <- residuals(lm(age0.11 ~ IMD + age25.34 + age50.59 + carebeds, data = covid_sp))\ncovid_sp$density.1R <- residuals(lm(density.1 ~ IMD + age25.34 + age50.59 + carebeds, data = covid_sp))\ncovid_sp$density.2R <- residuals(lm(density.2 ~ IMD + age25.34 + age50.59 + carebeds, data = covid_sp))\nfml <- formula(RateR ~ age0.11R + density.1R + density.2R)\nbw <- bw.gwr(fml, data = covid_sp, adaptive = TRUE)\ngwrmixd <- gwr.basic(fml, data = covid_sp, bw = bw, adaptive = TRUE)\n\n\nHere are the statistically significant results for the age0.11 variable.\n\n\nCode\ncovid$age0.11GWR <- gwrmixd$SDF$age0.11R\ncovid$age0.11GWR[abs(gwrmixd$SDF$age0.11R_TV) > 1.96] <- NA\nggplot(data = covid, aes(fill = age0.11GWR)) +\n  geom_sf(size = 0.25) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\n\n\nMultiscale GWR\nIt is also possible to fit a Multiscale GWR, which allows for the possibility of a different bandwith for each variable – see ?gwr.multiscale. Since there is no particular reason to presume that the bandwidth should be the same for all the variables, arguably Multiscale GWR should be preferred over standard GWR, at least in the first instance. Its main drawback, however, is that it takes a while to run – over 3 hours for the code chunk below! (In principle it can be parallelised but it generated an error for me when I tried this with parallel.method = cluster.)\n\n\nCode\ndensity.1 <- poly(covid$density, 2)[, 1]\ndensity.2 <- poly(covid$density, 2)[, 2]\n# Do not run!\n# Took over 3 hours!\n#gwrmulti <- gwr.multiscale(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds +\n# density.1 + density.2, data = covid_sp, adaptive = TRUE)\n\n\nTo save time, the results are available below. The bandwidths do appear to vary.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/gwrmulti.RData\")\nload(url)\nclose(url)\ngwrmulti\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-08-21 14:44:09 \n   Call:\n   gwr.multiscale(formula = Rate ~ IMD + age0.11 + age25.34 + age50.59 + \n    carebeds + density.1 + density.2, data = covid_sp, adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density.1 density.2\n   Number of data points: 924\n   ***********************************************************************\n   *                       Multiscale (PSDM) GWR                          *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidths for each coefficient(number of nearest neighbours): \n              (Intercept) IMD age0.11 age25.34 age50.59 carebeds density.1\n   Bandwidth           28 922      34      922      132      841       380\n              density.2\n   Bandwidth        186\n\n   ****************Summary of GWR coefficient estimates:******************\n                    Min.     1st Qu.      Median     3rd Qu.     Max.\n   Intercept   40.390229  158.475456  197.569654  250.106703 341.7481\n   IMD         -0.457266   -0.429212   -0.427347   -0.425449  -0.4210\n   age0.11     -4.383024   -0.168031    3.233327    5.482906  15.0608\n   age25.34     2.970842    2.980326    2.990406    2.999560   3.0909\n   age50.59    -1.540099    3.976661    4.868116    6.199428  10.9802\n   carebeds     0.041884    0.045506    0.050615    0.056445   0.0873\n   density.1 -228.739192 -171.871215 -117.550561   -6.118907  67.3146\n   density.2 -639.370049 -216.959473 -130.236970  -77.397916  26.1626\n   ************************Diagnostic information*************************\n   Number of data points: 924 \n   Effective number of parameters (2trace(S) - trace(S'S)): 227.0379 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 696.9621 \n   AICc value:  8395.1 \n   AIC value:  8142.156 \n   BIC value:  8216.88 \n   Residual sum of squares:  301719.5 \n   R-square value:  0.738389 \n   Adjusted R-square value:  0.6530458 \n\n   ***********************************************************************\n   Program stops at: 2022-08-21 17:46:34 \n\n\n\n\nGeographically and temporally weighted regression\nIt also possible to add a time dimension and undertake geographically and temporally weighted regression in GWmodel. The following workspace loads monthly COVID-19 data for Manchester neighbourhoods.\n\n\nCode\nurl <- url(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/workspaces/manchester.RData\")\nload(url)\nclose(url)\n\n\nHere are the monthly rates.\n\n\nCode\nggplot(covid, aes(fill = Rate )) +\n  geom_sf() +\n  facet_wrap(~ month) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme_void()\n\n\n\n\n\nIf now want to explore how the regression relationships vary in space-time then we can use the bw.gtwr() and gtwr() functions, for which we also need to specify a vector of time tags (obs.tv = ...). In these data, they are simply a numeric value, covid$month_code but see ?bw.gtwr() and ?gtwr() for further possibilities and for further information about the model.\n\nThe model will take a little time to run – enough time for you to get a cup of tea or coffee!\n\n\nCode\ncovid_sp <- as_Spatial(covid)\nfml <- formula(Rate ~ IMD + age0.11 + age25.34 + age50.59 + carebeds + poly(density, 2))\nbw <- bw.gtwr(fml, data = covid_sp, obs.tv = covid$month_code, adaptive = TRUE)\ngtwrmod <- gtwr(fml, data = covid_sp, st.bw = bw, obs.tv = covid$month_code, adaptive = TRUE)\n\n\nHere are the summary results of the model,\n\n\nCode\ngtwrmod\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-08-21 09:11:51 \n   Call:\n   gtwr(formula = fml, data = covid_sp, obs.tv = covid$month_code, \n    st.bw = bw, adaptive = TRUE)\n\n   Dependent (y) variable:  Rate\n   Independent variables:  IMD age0.11 age25.34 age50.59 carebeds density\n   Number of data points: 1768\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-3.8388 -2.4526 -0.6694  0.8773 23.0668 \n\n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)   \n   (Intercept)        1.2277082  0.8772151   1.400  0.16182   \n   IMD                0.0022343  0.0088457   0.253  0.80062   \n   age0.11           -0.0311138  0.0307405  -1.012  0.31161   \n   age25.34           0.0316351  0.0174764   1.810  0.07044 . \n   age50.59           0.1492102  0.0483997   3.083  0.00208 **\n   carebeds           0.0009431  0.0014616   0.645  0.51886   \n   poly(density, 2)1  8.4668105  5.2762201   1.605  0.10874   \n   poly(density, 2)2 -5.6870143  3.6957704  -1.539  0.12404   \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 3.542 on 1760 degrees of freedom\n   Multiple R-squared: 0.007583\n   Adjusted R-squared: 0.003636 \n   F-statistic: 1.921 on 7 and 1760 DF,  p-value: 0.06261 \n   ***Extra Diagnostic information\n   Residual sum of squares: 22079.68\n   Sigma(hat): 3.535908\n   AIC:  9499.228\n   AICc:  9499.331\n   ***********************************************************************\n   *    Results of Geographically and Temporally Weighted Regression     *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function for geographically and temporally weighting: bisquare \n   Adaptive bandwidth for geographically and temporally  weighting: 606 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric for geographically and temporally  weighting: A distance matrix is specified for this model calibration.\n\n   ****************Summary of GTWR coefficient estimates:*****************\n                              Min.       1st Qu.        Median       3rd Qu.\n   Intercept          -7.467273452  -1.429007024  -0.431890521   0.188930147\n   IMD                -0.060866478   0.000867536   0.006925644   0.016466529\n   age0.11            -0.263923302  -0.041615611   0.007456504   0.032878161\n   age25.34           -0.081155344   0.017377385   0.030977770   0.063555110\n   age50.59           -0.284989530   0.110990723   0.159690984   0.238809884\n   carebeds           -0.008797590   0.000070766   0.000863682   0.001820529\n   poly(density, 2)1 -22.971393286   5.633766857  12.761674565  22.069777132\n   poly(density, 2)2 -32.065614018  -8.663319026  -6.150064261  -3.539132987\n                        Max.\n   Intercept          6.5631\n   IMD                0.0631\n   age0.11            0.1432\n   age25.34           0.2475\n   age50.59           1.0677\n   carebeds           0.0066\n   poly(density, 2)1 94.7435\n   poly(density, 2)2 38.3984\n   ************************Diagnostic information*************************\n   Number of data points: 1768 \n   Effective number of parameters (2trace(S) - trace(S'S)): 57.81766 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1710.182 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 9304.009 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 9247.793 \n   Residual sum of squares: 18798.49 \n   R-square value:  0.1550625 \n   Adjusted R-square value:  0.1264803 \n\n   ***********************************************************************\n   Program stops at: 2022-08-21 09:13:00 \n\n\nThey should be treated cautiously because the Rate value is strongly positively skewed such that it may have been better to have transformed it (e.g. taken its square root) prior to analysis. Even so, there is evidence that the predictor age0.11 variable, for example, varies spatially and temporally.\n\n\nCode\ncovid$age0.11GWR <- gtwrmod$SDF$age0.11\ncovid$age0.11GWR[abs(gtwrmod$SDF$age0.11_TV) > 1.96] <- NA\nggplot(covid, aes(fill = age0.11GWR)) +\n  geom_sf() +\n  facet_wrap(~ month) +\n  scale_fill_gradient2(\"beta\", mid = \"grey90\", na.value = \"white\", trans = \"reverse\") +\n  theme_void() +\n  guides(fill = guide_colourbar(reverse = TRUE))\n\n\n\n\n\n\n\nGeneralised GWR models with Poisson and Binomial options\nSee ?ggwr.basic."
  },
  {
    "objectID": "spregress.html#summary",
    "href": "spregress.html#summary",
    "title": "Spatial Regression",
    "section": "Summary",
    "text": "Summary\nIn this session we have looked at spatial regression models as a way to address the problem of spatial autocorrelation in regression residuals and to allow for the possibility that regression relationships vary across a map. In presenting these methods, at least some could be taken as offering a ‘technical fix’ to the statistical issue of spatial dependencies in the data, violating assumptions of independence. Whilst that may be true, and the more spatial approaches can be used as diagnostic tools to check for a mis-specified model, notably one that lacks a critical explanatory variable or where the relationship between the X and Y variables is non-linear, the limitation of this thinking is that it treats geography as an error to be resolved and/or it rests on the belief that with sufficient information the geographical differences will be explained. What geographically weighted regression, for example, reminds us is that the nature of the relationship may be complex because it is changing in form across the study region as the socio-spatial causes also vary from place to place. Such spatial complexity may be challenging to accommodate within the parameters of conventional aspatial or spatially naïve statistical thinking but ignoring the geographical variation is not a solution. What spatial thinking brings to the table is the recognition of geographical context and the knowledge that geographically rooted social outcomes are not independent of where the processes giving rise to those outcomes are taking place."
  },
  {
    "objectID": "spregress.html#further-reading",
    "href": "spregress.html#further-reading",
    "title": "Spatial Regression",
    "section": "Further Reading",
    "text": "Further Reading\n\nSpatial Regression Models (2nd edition) by MD Ward & KS Gleditsch (2018)\nComber et al. (2022) A Route Map for Successful Applications of Geographically Weighted Regression. https://doi.org/10.1111/gean.12316"
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming",
    "section": "",
    "text": "So far in this course we have been cutting and pasting from these webpages into the Console of R Studio. Working in the Console is useful if you want to work with code on a line-by-line basis – sometimes it is helpful to see if something will work; to try things out. However, in practice, it is better to write and work with more reproducible code, either for your own benefit so you can modify something without having entirely to start-over, or for the benefit of others who would like to reproduce your work. Reproducibility is an important component of open research and is to be encouraged wherever possible."
  },
  {
    "objectID": "programming.html#scripts",
    "href": "programming.html#scripts",
    "title": "Programming",
    "section": "Scripts",
    "text": "Scripts\nA script is a text file containing a sequence of commands that can be run together, one after the other, without entering them separately in the Console. Let’s download an example of a script:\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/scripts/script1.R\",\n              \"script1.R\", mode = \"wb\", quiet = TRUE)\n\n\nYou can now use file.edit(\"script1.R\") to view its contents. It should look like this:\n\n The script is based on the opening parts of Geographic Data in ipumsr, where ipumsr provides “an easy way to import census, survey and geographic data provided by ‘IPUMS’ into R”, and IPUMS “provides census and survey data from around the world integrated across time and space.” I have modified their code for greater consistency with other parts of this course but what it does is largely the same:\n\nIt loads some example data and manipulates it to calculate the percentages of people using solid fuels for cooking in regions of Colombia at three time periods. These are the attribute data\nIt loads a geographic boundary file of those regions. This is the map.\nIt simplifies (reduces the detail of) the map to make it faster for plotting.\nIt joins the attribute data to the map by means of their common geography.\nIt produces a choropleth map (thematic map) of the geographic variation in the percentages using solid fuel.\nIt saves the spatially joined data as a shapefile.\n\nIf you now click within the window of the script and use command-A (Mac) or ctrl-A (Windows) to select all the code, followed by command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) – or use the Run button towards the top-right of the script window – then the script will run in its entirety and should, at its conclusion, produce the following maps.\n\nBe patient whilst the code takes a few moments to run.\n\n\n\n\n\nTry also typing source(\"script1.R\", echo = TRUE) into the R Console. Again, the script should run in its entirety.\nAs suggested earlier, an advantage of using a script is that it is easy to make changes to and then re-run it either in part or in full. In the script, find the type = \"cartolight\" argument in the line that says annotation_map_tile(type = \"cartolight\", progress = \"none\") + and change the type to any other of the following, such as cartodark.\n\n\nCode\nrosm::osm.types()\n\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"hikebike\"              \n [7] \"hillshade\"              \"osmgrayscale\"           \"stamenbw\"              \n[10] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[13] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\nIf you wish to view what the different types look like, you can do so here.\nOnce you have made the change, select the parts of the script you wish to rerun (everything under Map the data should be sufficient) and press the Run button or hit command-Enter/Return (Mac) or ctrl-Enter/Return (Windows) to execute the selected code.\n\nA note about the :: notation\nThe use of the :: notation in the code rosm::osm.types() allows a function to be run from a package that has not been loaded. In the example, osm.types() is a function in the rosm package. That package provides access to Open Steet Map and other maps tiles. We could also require(rosm) and then use the function directly as we have in other cases, i.e. using osm.types() instead of rosm::osm.types(). However, sometimes, if a function only is to be used once then there is no need to require the whole package. Also, sometimes we load multiple packages that have functions within them that share the same name. In such circumstances, the package::function format may be required to make sure the correct function (from the correct package) is being called."
  },
  {
    "objectID": "programming.html#r-markdown",
    "href": "programming.html#r-markdown",
    "title": "Programming",
    "section": "R markdown",
    "text": "R markdown\nScripts are useful but sometimes we wish to author documents that combine written text such as this with executable R code and its outputs and to publish them as html, pdf or Word documents. This is where R Markdown is useful.\nFrom the dropdown menus, select File -> New File -> R Markdown. Create the document in html format and give it any title you like.\n\nAfter R Studio has created the document, Knit it. The first time you do this, you will be asked to save the document - call it markdown1.Rmd or any other name you prefer.\n\nIt is self-evident what knitting the document does – it produce an html file which includes the text and formatting, the R code (unless suppressed with echo = FALSE) and output from that code. It also includes the option to publish the document on RPubs (although I suggest you don’t do this now).\n\n This whole course is written based on R Markdown. You can download the markdown file for this session\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/markdown/programming.Rmd\", \"markdown_example.Rmd\", mode = \"wb\", quiet = TRUE)\n\n\nand view it using file.edit(\"markdown_example.Rmd\"). You may note that it begins with a YAML header, to which various arguments can be added or changed – see here for an introduction.\n---\ntitle: \"Programming\"\nauthor: \"Rich Harris\"\ndate: '2022-07-11'\noutput: html_document\n---\nIt then consists of a mixture of text and code chunks. Those code chunks can be executed within the document using the Run drop-down menus and buttons.\n\n The document also includes various syntax, including ##header for a header, **bold** for bold, ![](image.png) to insert an existing image file, and so forth. To learn more, see the R Markdown cheatsheet.\n\nThe source code for this document\nThis page has actually been authored in a variant of R markdown, using quarto. You can view the source code for this and other pages using View Source from the drop-down Code options at the top of the page."
  },
  {
    "objectID": "programming.html#summary",
    "href": "programming.html#summary",
    "title": "Programming",
    "section": "Summary",
    "text": "Summary\nAlthough a lot of what we will do in this course will involve cutting and pasting into the Console, keep in mind that there are better ways of programming that are more reproducible than entering commands one at a time into the Console. These include scripting and using markdown. Note also that as commands are entered into the Console, they are saved in the History to the top right of the screen. All or part of that history can be selected and moved to a source file (a new R Script) as the following shows. The history can also be saved – see ?save.history()."
  },
  {
    "objectID": "programming.html#further-reading",
    "href": "programming.html#further-reading",
    "title": "Programming",
    "section": "Further reading",
    "text": "Further reading\n\nThe book, Efficient R programming by Colin Gillespie and Robin Lovelace has an online version here.\nMore about R Markdown can be learned from https://rmarkdown.rstudio.com/."
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Getting Started",
    "section": "",
    "text": "For this course we will be using R. R is a free software environment for statistical computing and graphics. To run the code blocks for this course on your own computer you will need to have installed R. This is available for Linux, MacOS and Windows."
  },
  {
    "objectID": "start.html#install-r-studio",
    "href": "start.html#install-r-studio",
    "title": "Getting Started",
    "section": "Install R Studio",
    "text": "Install R Studio\nRStudio is an integrated development environment (IDE) that can make programming and other tasks easier in R. An open source edition is available to download and install.\n\nYou need to install R before you install R Studio."
  },
  {
    "objectID": "start.html#open-r-studio",
    "href": "start.html#open-r-studio",
    "title": "Getting Started",
    "section": "Open R Studio",
    "text": "Open R Studio\nOnce R and R Studio are installed, open R Studio on your computer and type the following in the Console to the left or bottom left of the screen, alongside the prompt, >.\n\n\nCode\n1 + 1\n\n\nthen hit Enter/Return. You should, of course, obtain the answer 2, as below.\n\n\n[1] 2\n\n\nYou will also find that if you move your mouse to over the code block above, an option appears to copy the code to the clipboard."
  },
  {
    "objectID": "start.html#install-additional-librariespackages",
    "href": "start.html#install-additional-librariespackages",
    "title": "Getting Started",
    "section": "Install additional libraries/packages",
    "text": "Install additional libraries/packages\nThe base functions of R are greatly extended by the very many packages/libraries that have been developed for it. At the time of writing, there are 19052 of these on CRAN, which is the main repository for them. Many of these have been grouped into ‘tasks’ and topic areas, which can be viewed here.\nMost of the packages that will be needed for this course will be installed as they are needed. However, some will be used so regularly that we should install them now. Cut and paste the following code chunk into the Console and hit Enter/Return.\n\n\nCode\ninstall.packages(\"sf\", dependencies = TRUE)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n\n\n\nRun the code above even if you have the packages installed already so that you also have available all the packages that these depend upon and link to."
  },
  {
    "objectID": "start.html#changing-the-working-directory",
    "href": "start.html#changing-the-working-directory",
    "title": "Getting Started",
    "section": "Changing the working directory",
    "text": "Changing the working directory\nIf you type getwd() into the R Console you will obtain your current working directory – the default location to look for files and to save content to. Mine is,\n\n\nCode\ngetwd()\n\n\n[1] \"/Users/ggrjh/Dropbox/github/MandM\"\n\n\nYou may wish to change this to something else each time you start R. You can do this using the drop-down menus. There is also the function setwd(dir) – type ?setwd in the R Console to learn more.\n\n\nOrganising your files in a project\nYou could also create a new project in R by using File –> New Project… from the dropdown menus and create it either in a new directory (probably most sensible) or an existing one. There is nothing especially magical about a project in R. As stated here, “a project is simply a working directory designated with a .RProj file. When you open a project (using File/Open Project in RStudio or by double–clicking on the .Rproj file outside of R), the working directory will automatically be set to the directory that the .RProj file is located in.” However, it is that which makes it useful: when you open a project you know that you are going to be working in a specific folder on your computer which them becomes the default ‘container’ to save files to or to download them from.”\n\nIt would be a good idea to create a new project now which can then be the folder and working directory for this course and its contents."
  },
  {
    "objectID": "start.html#changing-the-appearance-of-r-studio",
    "href": "start.html#changing-the-appearance-of-r-studio",
    "title": "Getting Started",
    "section": "Changing the appearance of R Studio",
    "text": "Changing the appearance of R Studio\nYou may notice that I prefer a blue to a white screen when working in R. To change it to this, from the drop-down menus use Tools –> Global Options… -> Appearance and select Solarized Dark as the Editor theme. You may, of course, have your own preference."
  },
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "Measuring spatial autocorrelation",
    "section": "",
    "text": "We begin by recreating one of the maps from the previous session. You may wish to change your working directory to be the same as previously if it is not already.\n\n\nCode\ninstalled <- installed.packages()[,1]\nrequired <- c(\"tidyverse\", \"sf\", \"RColorBrewer\", \"classInt\", \"ggplot2\")\ninstall <- required[!(required %in% installed)]\nif(length(install)) install.packages(install, dependencies = TRUE)\n\nrequire(tidyverse)\nrequire(sf)\nrequire(RColorBrewer)\nrequire(classInt)\nrequire(ggplot2)\n\nif(!file.exists(\"municipal.RData\")) download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/municipal.RData?raw=true\", \"municipal.RData\", mode = \"wb\")\nload(\"municipal.RData\")\n\nbrks <- classIntervals(municipal$No_schooling, n = 7, style = \"jenks\")$brks\nmunicipal$No_schooling_gp <- cut(municipal$No_schooling, brks, include.lowest = TRUE)\n\nggplot(data = municipal, aes(fill = No_schooling_gp)) +\n  geom_sf() +\n  scale_fill_brewer(\"%\", palette = \"RdYlBu\", direction = -1) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Percentage of Population with No Schooling\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  ) \n\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population with no schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities, creating clusters of red and of blue values (and of yellow too). However, simply ‘eye-balling’ the map to look for patterns isn’t very scientific and it can be very deceptive. You can probably see patterns in the following map, too, but they arise from an entirely random permutation of the previous map’s data."
  },
  {
    "objectID": "autocorrelation.html#morans-test",
    "href": "autocorrelation.html#morans-test",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran’s test",
    "text": "Moran’s test\nThe classic way of quantifying how similar places are to their neighbours is to calculate the Moran’s statistic, which is a measure of spatial autocorrelation – of how much the values of a variable exhibit spatial clustering of alike values (positive spatial autocorrelation) or of ‘opposite’ values (negative spatial autocorrelation). We can calculate this statistic in R using the spdep package. We will slice out area 128 from the analysis as this is the one with an invalid geometry (see previous session and try which(!st_is_valid(municipal)) to confirm).\n\n\nCode\nif(!(\"spdep\" %in% installed)) install.packages(\"spdep\", dependencies = TRUE)\nrequire(spdep)\n\nmunicipal %>%\n  slice(-128) ->\n  municipal\n\n\n\nCreating a neighbours list\nThe first step is to define neighbours. In the following example, these are places that share a border (which are contiguous). Presently it is sufficient for them to meet at a single point. If the requirement is that they share an edge not merely a corner then change the default argument from queen = TRUE to queen = FALSE (see ?poly2nb for details).\n\n\nCode\nneighbours <- poly2nb(municipal)\n\n\nThe summary of neighbours reveals that, on average, each South African municipality has 5.2 neighbours but it can range from 1 (the 182nd region in the municipal data) to 10 (region 69). The most frequent number is 6.\n\n\nCode\nsummary(neighbours)\n\n\nNeighbour list object:\nNumber of regions: 233 \nNumber of nonzero links: 1210 \nPercentage nonzero weights: 2.228812 \nAverage number of links: 5.193133 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  8 22 43 51 70 30  3  4  1 \n1 least connected region:\n182 with 1 link\n1 most connected region:\n69 with 10 links\n\n\nThe neighbours of region 69 are,\n\n\nCode\nneighbours[[69]]\n\n\n [1]  60  64  68 138 152 153 156 157 163 164\n\n\nIt is instructive to write the list of neighbours to an external neighbours file,\n\n\nCode\nwrite.nb.gal(neighbours, \"neighbours.gal\")\n\n\n… which can then be viewed by using file.edit(\"neighbours.gal\"). The file will look like the below and has a very simple format. The top line say there are 233 regions. Going down, the first of these has 4 neighbours, which are regions 13, 14, 15 and 16. The second has 6 neighbours, which are 3, 4, 8, 18, 188, 233, and so forth. The same information could be encoded in a \\(233\\times233\\) matrix, where cell \\((i, j)\\) is given a value of one if \\(i\\) and \\(j\\) are considered neighbours, else zero. The problem with that approach is most of the matrix is sparse because most regions are not neighbours. It is quicker to state which regions are neighbours. The rest, by definition, are not.\n\nThese neighbourhood relationships can be viewed as a graph by extracting the coordinate points (st_coordinates()), of the centroids (st_centroid()), of the polygons that represent each municipality, and by using the plot functions for sf (simple features) and nb (neighbours list) objects. The argument of_largest_polygon = TRUE returns the centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the whole MULTIPOLYGON, a multipolygon being when one place is represented by multiple polygons (e.g. a mainland and an offshore island). Setting this argument to true means that the centroid will like within the boundary of the place, which isn’t otherwise guarenteed (e.g. it could be in the sea between the mainland and an island).\n\n\nCode\ncoords <- st_centroid(municipal, of_largest_polygon = TRUE)\npts <- st_coordinates(coords)\n\npar(mai = c(0, 0, 0, 0))  # Remove the margins and white space around the plot\nplot(st_geometry(municipal), border = \"grey\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\n\n\nCreating spatial weights\nThe neighbourhood list simply defines which places are neighbours. The spatial weights give a weight to each neighbourhood link. One motivation for doing this is to stop any statistic based on a sum across neighbourhood links to be dominated by those neighbourhoods with most neighbours. Moran is one such statistic. Hence, if a region has six neighbours, each of those is given a weight of \\(1/6\\). If it has four, \\(1/4\\), and so forth. This is called row-standardisation and is the default style in the conversion of a neighbourhood to a spatial weights list with the function nb2listw(). See ?nb2listw for alternative specifications.\n\n\nCode\nspweight <- nb2listw(neighbours)\n# Here are the neighbours of and weights for the first region:\nspweight$neighbours[[1]]\n\n\n[1] 13 14 15 16\n\n\nCode\nspweight$weights[[1]]\n\n\n[1] 0.25 0.25 0.25 0.25\n\n\n\n\nCalculating the Moran’s value\nNow we have the spatial weights, we can run a Moran’s test to measure the strength of spatial autocorrelation in the municipal$No_schooling variable.\n\n\nCode\nmoran <- moran.test(municipal$No_schooling, spweight)\nmoran\n\n\n\n    Moran I test under randomisation\n\ndata:  municipal$No_schooling  \nweights: spweight    \n\nMoran I statistic standard deviate = 13.968, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.576519069      -0.004310345       0.001729177 \n\n\nThe Moran statistic is 0.577 and the 95% confidence interval is,\n\n\nCode\nz <- c(-1.96, 1.96)\nround(moran$estimate[1]  + z * sqrt(moran$estimate[3]), 3)\n\n\n[1] 0.495 0.658\n\n\nSince the confidence interval does not include the expected value of -0.004, we can conclude that there is statistically significant positive autocorrelation in the variables – municipalities with higher percentages of no schooling tend to be surrounded by other municipalities with the same, and similarly, low values tends to be surrounded by other ones that are low.\n\nThe expected value is very close to zero so what we are almost saying is that because the confidence interval does not span zero so there is evidence of positive spatial autocorrelation. Although this is quite close to being true, to actually be correct would require that the expected value of the statistic is zero with no spatial autocorrelation. It isn’t. It is presently -0.004 and approaches zero as the number of observations increases: \\(E(I) = -1 / (n - 1).\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "autocorrelation.html#moran-plot-and-local-moran-values",
    "href": "autocorrelation.html#moran-plot-and-local-moran-values",
    "title": "Measuring spatial autocorrelation",
    "section": "Moran plot and local Moran values",
    "text": "Moran plot and local Moran values\nWhilst there is positive spatial autocorrelation in the values overall, not everywhere is surrounded by similar values. The following plot has 4 quadrants marked upon it. The top right indicates places where both they and their average neighbour have above average values of municipal$No_schooling. We can describe these as high-high clusters on the map. The bottom left indicates places where they and their average neighbour have below average values. These are low-low clusters. Both the high-high and the low-low contribute to positive spatial autocorrelation because, for these, the places and their neighbours display similar values. The two other quadrants do not. In the top left are low-high clusters. In the bottom right are high-low. There reveal clusters of dissimilar values (negative spatial autocorrelation). In the chart, the high-high and low-low places are more plentiful than the low-high and high-low ones, hence the upwards sloping line of best fit and the positive Moran statistic.\n\n\nCode\nmoran.plot(municipal$No_schooling, spweight)\n\n\n\n\n\nIt is straightforward to map which quadrant each place belongs to. First, we calculate the local Moran statistics proposed by Anselin (1995). A local statistic is one that applies to a subspace of the map, whereas a global statistic is a summary measure for the whole map. Anselin showed that the (global) Moran statistic can be decomposed into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours. There are 233 municipalities in the data so there will be 233 local Moran values too.\n\n\nCode\nlocalm <- localmoran(municipal$No_schooling, spweight)\n\n\nUsefully, if we look at the attributes of localm, we discover an attribute named quadr which contains what we want and which can be mapped. In fact, it includes three different versions of what we might want, the differences being due to which average low and high are defined by (see below the section Value in ?localmoran).\n\n\nCode\nnames(attributes(localm))\n\n\n[1] \"dim\"      \"dimnames\" \"call\"     \"class\"    \"quadr\"   \n\n\nCode\nquadr <- attr(localm, \"quadr\")\nhead(quadr)\n\n\n      mean   median    pysal\n1 Low-High Low-High Low-High\n2  Low-Low  Low-Low  Low-Low\n3  Low-Low  Low-Low  Low-Low\n4 High-Low High-Low High-Low\n5  Low-Low  Low-Low  Low-Low\n6  Low-Low  Low-Low  Low-Low\n\n\nCode\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"orange\", \"purple\", \"red\")) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nUnfortunately, the resulting map is somewhat misleading because not all of the local Moran values are statistically significant and some of the various high-high, low-low, etc. pairings my be only trivially alike or dissimilar. Looking at top of the local Moran data suggests a way of isolating those that are statistically significant and is adopted in the following map.\n\n\nCode\nhead(localm, n = 3)   # The p-values are in column 5\n\n\n          Ii          E.Ii     Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.1271124 -0.0055224972 0.31575429 -0.2163830      0.8286892\n2  0.1047729 -0.0019928929 0.07556466  0.3883944      0.6977242\n3  0.1385483 -0.0003126832 0.01187599  1.2742226      0.2025845\n\n\nCode\nquadr[localm[,5] > 0.05, ] <- NA\n\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"orange\", \"purple\", \"red\"), na.value = \"grey80\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )\n\n\n\n\n\nArguably, this new map may still not apply a strict enough definition of statistical significance because the issue of repeat testing has not been tackled. Remember, there are 233 places, 233 local Moran values and therefore 233 tests of significance. The p-values can be adjusted for this using R’s p.adjust() function. The following example uses a false discovery rate method (method = fdr) but other alternatives include method = bonferroni. A question is whether this is now too strict given that there are not 233 independent tests. Rather, the data have overlapping geographies (places share neighbours) as well as spatial dependencies.\n\n\nCode\nquadr[p.adjust(localm[,5], method = \"fdr\") > 0.05, ] <- NA\n\nggplot(data = municipal, aes(fill = quadr$pysal)) +\n  geom_sf() +\n  scale_fill_discrete(\"\", type = c(\"blue\", \"red\"), na.value = \"grey80\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Statistically significant (p-adjusted) local Moran value groups\",\n    subtitle = \"Percentage of Population with No Schooling\"\n  )"
  },
  {
    "objectID": "autocorrelation.html#issues-with-the-moran-statistic",
    "href": "autocorrelation.html#issues-with-the-moran-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Issues with the Moran statistic",
    "text": "Issues with the Moran statistic\n\nHow to define neighbours?\nAny statistic that includes spatial weights is dependent upon how those weights are defined: how, then, to decide which places are neighbours and also how much weight each neighbourhood connection is given in the calculation? The calculations above use first order contiguity (places that share a boundary) but we could extend that definition to include neighbours of neighbours (or more):\n\n\nCode\nneighbours <- nblag(neighbours, maxlag = 2)\n\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))   # Plot graphics in a 1 row by 2 column grid\nplot(st_geometry(municipal), border = \"grey\", main = \"First order contiguity\")\nplot(neighbours[[1]], pts, add = T)\nplot(st_geometry(municipal), border = \"grey\", main = \"Second order contiguity\")\nplot(neighbours[[2]], pts, add = T)\n\n\n\n\n\nChanging the definition of neighbours does, of course, change the Moran statistic and it will change the local Moran values too.\n\n\nCode\nsapply(neighbours, \\(x) {\n  nb2listw(x) %>%\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\n\nMoran I statistic Moran I statistic \n        0.5765191         0.3883236 \n\n\nThere is no particular reason to stick with a contiguity-based definition. We could, for example, look for the \\(k\\) nearest neighbours (or, more precisely, the \\(k\\) nearest centroids to the centroid of each region), as in the two examples below.\n\nThe function knearneigh() contains the default argument, longlat = NULL. It should be ok not to change this here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. If you suspect it isn’t or if coords is simply a matrix of point coordinates, not an explicitly spatial object, change the default argument to longlat = TRUE.\n\n\nCode\npar(mai = c(0, 0, 1, 0))\npar(mfrow = c(1,2))\n\nneighbours <- knn2nb(knearneigh(coords, k = 5))\nplot(st_geometry(municipal), border = \"grey\", main = \"Five nearest neighbours\")\nplot(neighbours, pts, add = T)\n\nneighbours <- knn2nb(knearneigh(coords, k = 10))\nplot(st_geometry(municipal), border = \"grey\", main = \"Ten nearest neighbours\")\nplot(neighbours, pts, add = T)\n\n\n\n\n\nIn fact, we can run through all the possible values of \\(k\\) (from \\(1\\) to \\(k_{max} = (n - 1)\\) where \\(n\\) is the number of municipalities) and consider the Moran statistics that they generate. The resulting chart shows that the statistic is highly dependent on the scale of the analysis: as \\(k\\) increases, neighbourhood relationships extend over an increasing portion of the map, and the statistic tends to decline. It declines because nearby places tend to have similar values whereas those that are further away are more varied.\n\nThe code will generate a lot of warning messages. You can ignore them. They are warning you that \\(k\\) has become a large subset of all \\(n\\).\n\n\nCode\nn <- nrow(municipal)\ny <- sapply(1: (n-1), \\(k) {\n  knearneigh(coords, k) %>%\n    knn2nb %>%\n    nb2listw %>%\n    moran.test(municipal$No_schooling, .) ->\n    moran\n  moran$estimate[1]\n})\n\nggplot(data.frame(k = 1:(n-1), y = y), aes(x = k, y = y)) +\n  geom_line() +\n  ylab(\"Moran statistic\")\n\n\n\n\n\nIn principle the chart could be used to identify an ‘optimal’ value of \\(k\\). Unfortunately, the present case offers few clues as to what that optimal value should be. The p-value for these statistics is not shown but is least when \\(k = 27\\). That might be a justification, of sorts, for choosing \\(k = 27\\) but it is splitting hairs somewhat when all the p-values are tiny and well below \\(p = 0.001\\).\nOther ways of defining neighbours include whether they lie within a lower or upper distance bound of each other: see ?dnearneigh and this vignette on Creating Neighbours.\n\n\nHow to interpet the I statistic\nConceptually, the Moran statistic is easy to understand: it is a (global) measure of the correlation between the values of a variable at a set of locations, and the value of the same variable for those locations’ neighbours. More simply, it is the correlation between locations and their neighbours.\nExcept it isn’t. Or, rather, it is a measure of correlation, just not the more commonly used Pearson correlation. The difference is evident in the following comparison where the Moran value is almost half the Pearson correlation between locations and their average neighbour (where the value for the average neighbour is calculated using last.listw()).\n\n\nCode\nspweight <- nb2listw(knn2nb(knearneigh(coords, 30)))\npearson <- cor(lag.listw(spweight, municipal$No_schooling), municipal$No_schooling)\nmoran <- moran.test(municipal$No_schooling, spweight)$estimate[1]\ndata.frame(person = pearson, moran = moran, row.names = \"correlation\")\n\n\n              person     moran\ncorrelation 0.636569 0.3346272\n\n\nAs Brunsdon and Comber note, the value of Moran’s I is not constrained to be in the range from -1 to +1 but changes with the spatial weights matrix. Following, Jong et al. (1984, p. 20), the extremes for the current spatial weights are,\n\n\nCode\nlistw2mat(spweight) %>%\n  range(eigen((. + t(.)) / 2))\n\n\n[1] -0.3985514  1.0317251\n\n\nNot only is this not in the range -1 to 1, it is not symmetric around the expected (null) value of -0.004.\nPersonally, I am not persuaded that the global Moran’s value is preferable to the more readily interpreted Pearson correlation between locations and their average neighbour. That correlation with \\(k = 30\\) nearest neighbours was calculated above and is 0.637. A confidence interval for this can be calculated using a permutation approach, for example. Given the geography of the South African municipalities, their neighbourhood relations and given the data values, then permuting those values randomly 1000 times around the municipalities generates a 95% confidence interval that the correlation is a long way out of, indicating the positive autocorrelation to be significant. (A permutation approach is also available for the global and local Moran statistics: see ?moran.mc and ?localmoran_perm.)\n\n\nCode\nn <- length(municipal$No_schooling)\nsapply(1:1000, \\(x) {\n  sample(municipal$No_schooling, n) %>% \n    cor(lag.listw(spweight, .))\n}) %>%\n  quantile(prob = c(0.025, 0.975))\n\n\n      2.5%      97.5% \n-0.2337942  0.1416411"
  },
  {
    "objectID": "autocorrelation.html#getis-and-ord-g-statistic",
    "href": "autocorrelation.html#getis-and-ord-g-statistic",
    "title": "Measuring spatial autocorrelation",
    "section": "Getis and Ord G-Statistic",
    "text": "Getis and Ord G-Statistic\nA different method for identifying ‘hot’ or ‘cold spots’ of a variable is provided by the G-statistic. As Brunsdon and Comber observe, the statistic – which is a local statistic, calculated for each location in turn – is based on the proportion of the total sum of standardised attribute values that are within a threshold distance, \\(d\\), of each area centroid. Looking at the map below, which is of South African wards (simplified slightly from this source), we may suspect there are clusters of places with greater percentages of their populations having relatively high incomes and that these are spatial anomalies. Because the percentages are extremely skewed, they have been plotted with a square root transformation applied to the scale of the map (trans = \"sqrt\") – notice how the values 0 to 4% (which are the original values, not the square roots) occupy more of the legend than the values 4 to 8 do, and so forth.\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/workspaces/wards.RData?raw=true\", \"wards.RData\", mode = \"wb\", quiet = TRUE)\nload(\"wards.RData\")\n\nggplot(data = wards, aes(fill = High_income)) +\n  geom_sf(colour = \"transparent\") +\n  scale_fill_distiller(\"%\", guide = \"colourbar\", direction = 1, trans = \"sqrt\",\n                       palette = \"Reds\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  labs(\n    title = \"Percentage of Population with income R614401 or greater\",\n    subtitle = \"2011 South African Census Data\",\n    caption = \"Source: Statistics South Africa\"\n  )\n\n\n\n\n\nThe G-statistic requires a binary spatial weights (style = \"B\", below), whereby locations either are or are not within the threshold distance of each other. The following has a threshold distance of 20km. Not all of the ward centroids will be within 20km of another which creates situations of zero neighbours, which is tolerated by setting zero.policy = TRUE in the conversion from a neighbours to spatial list.\n\nAs with the function knearneigh(), dnearneigh() has the default argument, longlat = NULL, which should not need changing here to longlat = TRUE because it ought to pick this up from coords’s coordinate reference system. However, don’t take this for granted.\n\n\nCode\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nneighbours <- dnearneigh(coords, 0, 20)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\n\nHaving calculated the local G values, we can map them, using a manually generated fill palette. The argument na.translate = F removes the NA values from the legend but not from the map, wherein they are shaded white (na.value = \"white\"). The G values are also standardised z-values, hence values of 1.96 or greater are relatively rate if the assumption that the statistic is Normally distributed is warranted. (If it isn’t, consider using localG_perm.)\n\n\nCode\nbrks <- c(min(wards$localG, na.rm = TRUE), -1.96, 1.96, 2.58, 3.29, max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\npal <- c(\"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 20km threshold\"\n  )\n\n\n\n\n\nLet’s now add a refinement to the map and, based on what we learned from the previous session, label the cities that appear to contain a hot spot of higher percentages of higher earners. Note the use of st_join() which is a spatial join: it identifies, from geography, which ward the cities are located in and appends the ward data, including the G statistics, to the cities, retaining only those that are in the most significant hot spots. Note that this definition of ‘most significant’ doesn’t address the problem of repeat testing which we also saw with the local Moran statistics and, of course, the results are dependent on the distance threshold defining which places are or are not neighbours.\n\n\nCode\nif(!(\"remotes\" %in% installed)) install.packages(\"remotes\", dependencies = TRUE)\nif(!(\"ggsflabel\" %in% installed)) remotes::install_github(\"yutannihilation/ggsflabel\")\nrequire(ggsflabel)\n\nif(!file.exists(\"hotosm_zaf_populated_places_points.shp\")) {\n  download.file(\"https://github.com/profrichharris/profrichharris.github.io/blob/main/MandM/boundary%20files/hotosm_zaf_populated_places_points_shp.zip?raw=true\",\n              \"cities.zip\", mode = \"wb\", quiet = TRUE)\n  unzip(\"cities.zip\")\n}\n  \nread_sf(\"hotosm_zaf_populated_places_points.shp\") %>%\n  filter(place == \"city\") %>%\n  st_join(wards) %>%\n  filter(localG > 3.29) ->\n  cities\n\nlast_plot() +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)\n\n\n\n\n\nHere are the results with a distance threshold of 100km.\n\n\nCode\nneighbours <- dnearneigh(coords, 0, 100)\nspweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\nwards$localG <- localG(wards$High_income, spweight)\n\nbrks <- c(min(wards$localG, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(wards$localG, na.rm = TRUE))\nwards$localG_gp <- cut(wards$localG, brks, include.lowest = TRUE)\n\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot() +\n  geom_sf(data = wards, aes(fill = localG_gp), colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(\n    title = \"Local G statistic\",\n    subtitle = \"Percentage of Population with income R614401 or greater\",\n    caption = \"With a 100km threshold\"\n  ) +\n  geom_sf_label_repel(data = cities,\n                      aes(label = name), alpha = 0.7, size = 3,\n                      max.overlaps = 20,\n                      force = 2)"
  },
  {
    "objectID": "autocorrelation.html#summary",
    "href": "autocorrelation.html#summary",
    "title": "Measuring spatial autocorrelation",
    "section": "Summary",
    "text": "Summary\nThis session has been about identifying and quantifying spatial clustering in data, using a combination of ‘global’ (whole map) and local statistics; specifically, Moran’s I, its local equivalent and the Getis-Ord G statistic. These statistics are dependent on the spatial weights matrix used in their calculation, which does, unfortunately, create something of a vicious circle: ideally that matrix would be calibrated to the spatial patterns evident in the data but they are only evident once the weights matrix has been specified. One option is to take the view that looking at the patterns at a range of scales is itself revealing so choose lots of bandwidths, not just one, and treat it as a multiscale analysis, as in the following.\n\n\nCode\n# Sets the threshold distance from 20 to 180km in intervals of 20\n# I chose 9 distances because they can be shown in a 3 x 3 grid of maps\nd <- seq(from = 20, by = 20, length.out = 9)\n\n# Calculate the G statistics for each distance and save them\n# as a list of sf features (maps)\ncoords <- st_centroid(wards, of_largest_polygon = TRUE)\nmaps <- lapply(d, \\(x) {\n  neighbours <- dnearneigh(coords, 0, x)\n  spweight <- nb2listw(neighbours, style = \"B\", zero.policy = TRUE)\n  wards$localG <- localG(wards$High_income, spweight)\n  wards$distance <- x\n  return(wards)\n})\n\n# Bind the maps together into a single object to be used\n# for the faceting in ggplot2\nmaps <- do.call(rbind, maps)\n\nbrks <- c(min(maps$localG, na.rm = TRUE), -3.29, -2.58, -1.96, 1.96, 2.58, 3.29,\n          max(maps$localG, na.rm = TRUE))\nmaps$localG_gp <- cut(maps$localG, brks, include.lowest = TRUE)\n\npal <- c(\"purple\", \"dark blue\", \"light blue\", \"light grey\", \"yellow\", \"orange\", \"red\")\n\nggplot(data = maps, aes(fill = localG_gp)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(\"G\", values = pal, na.value = \"white\",\n                    na.translate = F) +\n  facet_wrap(~ distance) +\n  theme_light() +\n  theme(axis.title.x = element_blank(), axis.title.y = element_blank(),\n        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),\n        axis.text.x = element_blank(), axis.text.y = element_blank(),\n        legend.position = \"bottom\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "autocorrelation.html#further-reading",
    "href": "autocorrelation.html#further-reading",
    "title": "Measuring spatial autocorrelation",
    "section": "Further Reading",
    "text": "Further Reading\n\nChapter 8 on Localised Spatial Analysis in An Introduction to R for Spatial Analysis & Mapping by Chris Brunsdon and Lex Comber."
  },
  {
    "objectID": "tidyverse.html#introduction",
    "href": "tidyverse.html#introduction",
    "title": "Tidyverse",
    "section": "Introduction",
    "text": "Introduction\nIf base R is R Classic then tidyverse is a new flavour of R, designed for data science. It consists of a collection of R packages that “share an underlying design philosophy, grammar, and data structures”.\nTidyverse is easier to demonstrate then to pin-down to some basics so let’s work through an example using both base R and tidyverse to illustrate some differences."
  },
  {
    "objectID": "tidyverse.html#to-start",
    "href": "tidyverse.html#to-start",
    "title": "Tidyverse",
    "section": "To Start",
    "text": "To Start\nWe will begin by downloading a data file to use. It is an extract of the Covid Data Dashboard for England in December 2021. Some prior manipulation and adjustments to these data have been undertaken for another project so treat them as indicative only (the actual numbers may have been changed slightly from their originals although only marginally so).\n\n\nCode\ndownload.file(\"https://github.com/profrichharris/profrichharris.github.io/raw/main/MandM/data/covid_extract.csv\", \"covid.csv\", mode = \"wb\", quiet = TRUE) \n\n\nWe also need to require(tidyverse) ready for use.\n\n\nCode\nrequire(tidyverse)\n\n\n\nIf you get a warning message saying there is no package called tidyverse then you need to install it: install.packages(\"tidyverse\", dependencies = TRUE)."
  },
  {
    "objectID": "tidyverse.html#reading-in-the-data",
    "href": "tidyverse.html#reading-in-the-data",
    "title": "Tidyverse",
    "section": "Reading-in the data",
    "text": "Reading-in the data\nLet’s read-in and take a look at the data. First in base R.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\nhead(df1)\n\n\n   MSOA11CD regionName X2021.12.04 X2021.12.11 X2021.12.18 X2021.12.25 All.Ages\n1 E02000002     London          25          48         148         176     7726\n2 E02000003     London          46          58         165         215    11246\n3 E02000004     London          24          44         100         141     6646\n4 E02000005     London          58          97         185         231    10540\n5 E02000007     London          38          94         153         205    10076\n6 E02000008     London          54         101         232         245    12777\n\n\nNow using tidyverse,\n\n\nCode\ndf2 <- read_csv(\"covid.csv\")\nprint(df2, n = 6)\n\n\n# A tibble: 6,789 × 7\n  MSOA11CD  regionName `2021-12-04` `2021-12-11` `2021-12-18` `2021-12-25`\n  <chr>     <chr>             <dbl>        <dbl>        <dbl>        <dbl>\n1 E02000002 London               25           48          148          176\n2 E02000003 London               46           58          165          215\n3 E02000004 London               24           44          100          141\n4 E02000005 London               58           97          185          231\n5 E02000007 London               38           94          153          205\n6 E02000008 London               54          101          232          245\n# … with 6,783 more rows, and 1 more variable: `All Ages` <dbl>\n\n\nThere are already some differences. First, tidyverse has, in this case, handled the names of the variables better. It has also created what is described as a tibble which is “a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” You can find out more about them and how they differ from traditional data frames here."
  },
  {
    "objectID": "tidyverse.html#selecting-and-renaming-variables",
    "href": "tidyverse.html#selecting-and-renaming-variables",
    "title": "Tidyverse",
    "section": "Selecting and renaming variables",
    "text": "Selecting and renaming variables\nWe will now select the regionName, 2021-12-04 and All Ages variables, rename the second of these as cases and the third as population, and look at the data again to check that it has worked.\nIn base R,\n\n\nCode\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[2:3] <- c(\"cases\", \"population\")\nhead(df1)\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\nIn tidyverse,\n\n\nCode\ndf2 <- select(df2, regionName, `2021-12-04`, `All Ages`)\ndf2 <- rename(df2, cases = `2021-12-04`, population = `All Ages`)\nprint(df2, n = 6)\n\n\n# A tibble: 6,789 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n# … with 6,783 more rows\n\n\nComparing the two, the tidyverse code may be more intuitive to understand because of its use of verbs as functions: select(), rename() and so forth."
  },
  {
    "objectID": "tidyverse.html#piping",
    "href": "tidyverse.html#piping",
    "title": "Tidyverse",
    "section": "Piping",
    "text": "Piping\nNow we shall bring the two previous stages together, using what is referred to as a pipe. Without worrying about the detail, which will be returned to presently, here is an example of a pipe, |>, being used in base R:\n\n\nCode\nread.csv(\"covid.csv\") |>\n  (\\(x) x[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")])() -> df1\nnames(df1)[2:3] <- c(\"cases\", \"population\")\ndf1 |>\n  head()\n\n\n  regionName cases population\n1     London    25       7726\n2     London    46      11246\n3     London    24       6646\n4     London    58      10540\n5     London    38      10076\n6     London    54      12777\n\n\n\nThe above will only work if you are using R version 4.1.0 or above. You can check which version you are running by using R.Version()$version.\nNow using tidyverse and a different pipe, %>%,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  print(n = 6)\n\n\n# A tibble: 6,789 × 3\n  regionName cases population\n  <chr>      <dbl>      <dbl>\n1 London        25       7726\n2 London        46      11246\n3 London        24       6646\n4 London        58      10540\n5 London        38      10076\n6 London        54      12777\n# … with 6,783 more rows\n\n\nThe obvious difference here is that the tidyverse code is more elegant. But what is the pipe and what is the difference between |> in the base R code and %>% in the tidyverse example?\nA pipe is really just a way of sending (’piping`) something from one line of code to the next, to create a chain of commands (forgive the mixed metaphors). For example,\n\n\nCode\nx <- 0:10\nmean(x)\n\n\n[1] 5\n\n\nCould be calculated as\n\n\nCode\n0:10 |>\n  mean()\n\n\n[1] 5\n\n\nor as\n\n\nCode\n0:10 %>%\n  mean\n\n\n[1] 5\n\n\nA more complicated example is below. It employs the function sapply(), a variant of the function lapply(X, FUN) that takes a list X and applies the function FUN to each part of it. In the example, it is the function mean.\n\n\nCode\nx <- list(0:10, 10:20)  # Creates a list with two parts: the numbers 0 to 10, and 10 to 20\ny <- sapply(x, mean)    # Calculates the mean for each part of the list, which are 5 and 15\nsum(y)                  # Sums together the two means, giving 20\n\n\n[1] 20\n\n\nThe above could instead be written as\n\n\nCode\nlist(0:10, 10:20) |>\n  sapply(mean) |>\n  sum()\n\n\n[1] 20\n\n\nor as\n\n\nCode\nlist(0:10, 10:20) %>%\n  sapply(mean) %>%\n  sum\n\n\n[1] 20\n\n\nAll three arrive at the same answer, which is 20.\nSo far, so good but what is the difference between |> and %>%? The answer is that %>% was developed before |> in the magrittr package, whereas |> is R’s new native pipe. They are often interchangeable but not always.\nAt the moment, the |> pipe is less flexible to use than %>%. Consider the following example. The final two lines of code work fine using %>% to pipe the data frame into the regression model (the function lm() fits a linear model).\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)   # Adds some random noise to the relationship between y and x\ndata.frame(x, y) %>%\n  lm(y ~ x, data = .)\n\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n   -0.03212      1.99702  \n\n\nHowever, it does not work with the pipe, |> because it does not recognise the place holder ., which receives the data frame from the line above and contains the variables for the model.\n\n\nCode\n# The following code does not work\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  lm(y ~ x, data = .)\n\n\nTo solve the problem, the above code can be modified by wrapping the regression part in another function but the end result is rather ‘clunky’.\n\n\nCode\nx <- 1:100\ny <- 2*x + rnorm(100)\ndata.frame(x, y) |>\n  (\\(z) lm(y ~ x, data = z))() \n\n\n\nCall:\nlm(formula = y ~ x, data = z)\n\nCoefficients:\n(Intercept)            x  \n     0.3335       1.9951  \n\n\nOver time, expect |> to be developed and to supersede %>%. For now, however, you are unlikely to encounter errors using %>% as a substitute for |> but you might using |> instead of %>%. In other words, %>% is the safer choice and the one which will be used for these tutorials."
  },
  {
    "objectID": "tidyverse.html#back-to-the-example",
    "href": "tidyverse.html#back-to-the-example",
    "title": "Tidyverse",
    "section": "Back to the example",
    "text": "Back to the example\nAfter that digression into piping, let’s return to our example that is comparing base R and tidyverse to read-in a table of data, select variables and rename one, and, in the following, to calculate the number of COVID-19 cases per English region as a percentage of their estimated populations in the week ending 2021-12-04.\nFirst, in base R:\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)  # Total cases per region\ncases    # This step isn't necessary but is included to show the result of the line above\n\n\n           East Midlands          East of England                   London \n                   25472                    35785                    43060 \n              North East               North West               South East \n                   10796                    31185                    62807 \n              South West            West Midlands Yorkshire and The Humber \n                   33846                    26554                    21079 \n\n\nCode\npopulation <- tapply(df1$population, df1$regionName, sum)   # Total population per region\nrate <- round(cases / population * 100, 3)\nrate\n\n\n           East Midlands          East of England                   London \n                   0.524                    0.571                    0.479 \n              North East               North West               South East \n                   0.403                    0.423                    0.681 \n              South West            West Midlands Yorkshire and The Humber \n                   0.598                    0.445                    0.381 \n\n\nNow using tidyverse,\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  group_by(regionName) %>%\n  summarise(across(where(is.numeric), sum)) %>%\n  mutate(rate = round(cases / population * 100, 3)) %>%\n  print(n = Inf)\n\n\n# A tibble: 9 × 4\n  regionName               cases population  rate\n  <chr>                    <dbl>      <dbl> <dbl>\n1 East Midlands            25472    4865583 0.524\n2 East of England          35785    6269161 0.571\n3 London                   43060    8991550 0.479\n4 North East               10796    2680763 0.403\n5 North West               31185    7367456 0.423\n6 South East               62807    9217265 0.681\n7 South West               33846    5656917 0.598\n8 West Midlands            26554    5961929 0.445\n9 Yorkshire and The Humber 21079    5526350 0.381\n\n\nEither way produces the same answers but, again, there is an elegance and consistency to the tidyverse way of doing it that is missing from base R."
  },
  {
    "objectID": "tidyverse.html#plotting",
    "href": "tidyverse.html#plotting",
    "title": "Tidyverse",
    "section": "Plotting",
    "text": "Plotting\nAs a final step for the comparison, we will extend the code to visualise the regional COVID-19 rates in a histogram, with a rug plot included. A rug plot is a way of preserving the individual data values that would otherwise be ‘lost’ within the bins of a histogram.\nAs previously, we begin with base R,\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ncases <- tapply(df1$cases, df1$regionName, sum)\npopulation <- tapply(df1$population, df1$regionName, sum)\nrate <- round(cases / population * 100, 3)\nhist(rate, xlab = \"rate (cases as % of population)\",\n     main = \"Regional COVID-19 rates: week ending 2021-12-04\")\nrug(rate, lwd = 2)\n\n\n\n\n\n…and continue with tidyverse, creating the output in such a way that it mimics the previous plot.\n\n\nCode\nrequire(ggplot2)\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  group_by(regionName) %>%\n  summarise(across(where(is.numeric), sum)) %>%\n  mutate(rate = round(cases / population * 100, 3)) ->\n  df2\n\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05, center = -0.025) +\n    geom_rug(size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nIn this instance, it is the tidyverse code that is the more elaborate. This is partly because there is more customisation of it to mimic the previous plot, which has added the final two lines of code. However, it is also because it is using the package ggplot2 to produce the histogram. We return to ggplot2 more in later sessions. For now it is sufficient to scan the code and observe how it is ‘layering up’ the various components of the graphic, which those components separated by the + in the lines of code.\nI prefer the ggplot2 to the hist() graphics plot but that may be a matter of personal taste. However, ggplot2 can do ‘clever things’ with the visualisation, a hint of which is shown below.\n\n\nCode\nrequire(ggplot2)\ndf2 %>%\n  ggplot(aes(x = rate)) +\n    geom_histogram(colour = \"black\", fill = \"grey\", binwidth = 0.05, center = -0.025) +\n    geom_rug(aes(colour = regionName), size = 2) +\n    labs(x = \"rate (cases as % of population)\", y = \"Frequency\",\n         title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n    scale_colour_discrete(name = \"Region\") +\n    theme_minimal() +\n    theme(panel.grid.major.y = element_blank()) \n\n\n\n\n\n Please don’t form that impression that ggplot2 is hard-wired to tidverse, and base R to the base graphics. In practice, they are interchangeable.\nHere is an example of using ggplot2 after a sequence of base R commands.\n\n\nCode\ndf1 <- read.csv(\"covid.csv\")\ndf1 <- df1[, c(\"regionName\", \"X2021.12.04\", \"All.Ages\")]\nnames(df1)[c(2,3)] <- c(\"cases\", \"population\")\ndf1$rate <- round(df1$cases / df1$population * 100, 3)\nggplot(df1, aes(x = rate, y = regionName)) +\n  geom_boxplot() +\n  labs(x = \"rate (cases as % of population)\",\n       y = \"region\",\n       title = \"Regional COVID-19 rates: week ending 2021-12-04\") +\n  theme_minimal()\n\n\n\n\n\nAnd here is an example of using the base R graphic boxplot() after a chain of tidyverse commands.\n\n\nCode\nread_csv(\"covid.csv\") %>%\n  select(regionName, `2021-12-04`, `All Ages`) %>%\n  rename(cases = `2021-12-04`, population = `All Ages`) %>%\n  mutate(rate = round(cases / population * 100, 3)) -> df2\npar(mai=c(0.8,2,0.5,0.5), bty = \"n\", pch = 20)  # See text below\nboxplot(df2$rate ~ df2$regionName, horizontal = TRUE,\n        whisklty = \"solid\", staplelty = 0,\n        col = \"white\", las = 1, cex = 0.9, cex.axis = 0.75,\n        xlab = \"rate (cases as % of population)\", ylab=\"\",\n        main = \"Regional COVID-19 rates: week ending 2021-12-04\")\ntitle(ylab = \"region\", line = 6)\n\n\n\n\n\nI would argue that, in this instance, the base R graphic is as nice as the ggplot2 one but it took more customisation to get it that way and I had to go digging around in the help files, ?boxplot, ?bxp and ?par to find what I needed, which included changing the graphic’s margins (par(mai=...))), moving and changing the size of the text on the vertical axis (the argument cex.axis and the use of the title() function), changing the appearance of the ‘whiskers’ (whisklty = \"solid\" and staplelty = 0), and so forth. Still, it does demonstrate that you can have a lot of control over what is produced, if you have the patience and tenacity to do so."
  },
  {
    "objectID": "tidyverse.html#which-is-better",
    "href": "tidyverse.html#which-is-better",
    "title": "Tidyverse",
    "section": "Which is better?",
    "text": "Which is better?\nHaving provided a very small taste of tidyverse and how it differs from base R, we might be tempted to ask, “which is better?” However, the question is misguided: it is a little like deciding to go to South America and asking whether Spanish or Portuguese is the better language. It depends, of course, on what you intend to do and where you intend to travel.\nI use both base R and tidyverse packages in my work, sometimes drifting between the two in rather haphazard ways. If I can get what I want to work then I am happy. Outcomes worry me more than means so, although I use tidyverse a lot, I am not always as tidy as it would want me to be!"
  },
  {
    "objectID": "tidyverse.html#futher-reading",
    "href": "tidyverse.html#futher-reading",
    "title": "Tidyverse",
    "section": "Futher reading",
    "text": "Futher reading\n\nThere is much more to tidyverse than has been covered here. See here for further information about it and its core packages.\nA full introduction to using tidyverse for Data Science is provided by the book R for Data Science by Hadley Wickham and Garrett Grolemund. There is a free online version of it available."
  },
  {
    "objectID": "themap.html",
    "href": "themap.html",
    "title": "The Spatial Variable",
    "section": "",
    "text": "When we look at a map such as the following, which is a choropleth (or thematic) map showing the percentage of the population with no experience of schooling in each of the South African municipalities in 2011, one thing should be immediately obvious: the areas are shaded in a range of colours; they are not all the same. This is because the values that the colours represent vary across the country with some places having a greater percentage of their population without schooling than others. In this way, the map reveals and also visually represents the spatial (geographic) variation in the variable of interest. The map portrays the geographic pattern.\n\nIt is not surprising to find variation. It is improbable that all the values would be the same. It is nearly always possible to find that some places have lower or higher values than others and to colour the map accordingly. Nevertheless, two characteristics of the spatial variation appear evident in the map.\n\nSpatial heterogeneity. This is the idea that the values typical in one part of the map are not typical in another. To put it simply, some parts of the map are shaded in blue whereas others are in red and those parts seem neither randomly nor regularly distributed because of…\nSpatial clustering. This is the idea that values found in one part of the map tend to be surrounded by similar values in neighbouring parts of the map. In other words, there are patches of blue and patches of red coloured areas on the map – blue tends be near blue and red tends to be near red Another name for this is positive spatial autocorrelation: values tend to be more similar to nearby other values than they are to distant ones.\n\nEvidence of spatial clustering supports Waldo Tobler’s much cited ‘first law’ of geography: “everything is related to everything else, but near things are more related than distant things.” It isn’t really a law because it is by no means always true: sometimes neighbouring places can have very different characteristics (there can be sharp changes across borders; spatial discontinuities). However, it does suggest that places tend to be situated within broader spatial contexts such that the processes that both generate and are generated by those contexts have a spatial expression and root. They map may reveal evidence of historic and on-going processes of socio-spatial inequalities, for example.\nTaken together, these characteristics of spatial variation indicate spatial dependencies, whereby the measured attributes of one place are not independent of other places. This dependence has statistical consequences if assumptions of independence are violated. Of more substantive geographic interest is how they have arisen – which processes are they caused by or associated with? Why are places not all the same? Why is there a geographical pattern?"
  },
  {
    "objectID": "themap.html#from-the-map-towards-models",
    "href": "themap.html#from-the-map-towards-models",
    "title": "The Spatial Variable",
    "section": "From the map towards models",
    "text": "From the map towards models\nWith the above questions in mind, we might imagine the map as a first stage in a process of geographical enquiry where what we do is look for and then quantify some of the geographical patterns in the data before beginning to model them. Here the map is not simply a tool for visualising and communicating data, it is also a tool for exploring data and thinking geographically about them.\n\n\n\n\n\n\n In practice, the process of analysis is likely to involve greater cycling between the various stages. Nevertheless, there is a good argument for starting with the map."
  },
  {
    "objectID": "themap.html#further-reading",
    "href": "themap.html#further-reading",
    "title": "The Spatial Variable",
    "section": "Further Reading",
    "text": "Further Reading\nThe Spatial Variable was the name of a lecture given by Ron Johnson, one of the most influential geographers of recent times, on his appointment as Professor at the University of Sheffield. A transcript and brief commentary on that lecture is available here and is highly recommended reading."
  },
  {
    "objectID": "base.html#introduction",
    "href": "base.html#introduction",
    "title": "Base R",
    "section": "Introduction",
    "text": "Introduction\nBase R is what you download from CRAN. You might think of it as classic R. A short introduction of ‘the basics’ is provided below."
  },
  {
    "objectID": "base.html#functions",
    "href": "base.html#functions",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\nR is a functional programming language where functions ‘do things’ to objects. What they do is dependent upon the class/type and attributes of the objects that go into the function, and also on the arguments of the function.\nFor example, try typing the following into the R Console, which is the bottom left panel of R Studio. Type it alongside the prompt symbol, > then hit Enter/Return.\n\n\nCode\nround(10.32, digits = 0)\n\n\n[1] 10\n\n\nThis calls the function round(), which is operating on the numeric object, 10.32. The argument digits specifies the number of digits to round to. It is set to zero in the example above. Since digits = 0 is the default value for the function, we could just write\n\n\nCode\nround(10.32)\n\n\n[1] 10\n\n\nand obtain the same answer. I know that digits = 0 is the default value because, as I type the name of the function into the R Console, I see the arguments of the function and any default values appear.\n\nWe can also find out more about the function, including some examples of its use, by opening its help file.\n\n\nCode\n?round\n\n\n Because digits = 0 is the default value, if we want to round to one digit, we need to specify the argument explicitly.\n\n\nCode\nround(10.32, digits = 1)\n\n\n[1] 10.3\n\n\nThe following also works because it preserves the order of the arguments in the function.\n\n\nCode\nround(10.32, 1)\n\n\n[1] 10.3\n\n\nIn other words, if we do not specifically state that x = 10.32 and digits = 1 then they will be taken to be the first and second arguments of the function.\n In these examples, both the input to and output from the function are a numeric vector of type double. The input is:\n\n\nCode\nclass(10.32)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(10.32)\n\n\n[1] \"double\"\n\n\nThe output is:\n\n\nCode\nclass(round(10.32, digits = 1))\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(round(10.32, digits = 1))\n\n\n[1] \"double\"\n\n\nNote how a function can be wrapped within a function, as in the example above: class(round(...)).\nWhereas 10.32 is a numeric vector of length 1,\n\n\nCode\nlength(10.32)\n\n\n[1] 1\n\n\nthe round() function can operate on numeric vectors of other lengths too.\n\n\nCode\nround(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7))\n\n\n[1] 1 2 3 4 6 7 8\n\n\nHere the combine function, c() is used to create a vector of length 7, which is the input into round(). The output is of length 7 too.\n\n\nCode\nlength(round(c(1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7)))\n\n\n[1] 7\n\n\n\nThere are lots of functions for R and I often forget what I need. Fortunately, there is a large user community too and so a quick web search often helps me quickly to find what I need.\n\nWriting a new function\nWe can write our own functions. The following will take a number and report whether it is a prime number or not.\n\n\nCode\nis.prime <- function(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}\n\n\nLet’s try it.\n\n\nCode\nis.prime(2)\n\n\n[1] TRUE\n\n\nCode\nis.prime(10)\n\n\n[1] FALSE\n\n\nCode\nis.prime(13)\n\n\n[1] TRUE\n\n\nCode\nis.prime(3.3)\n\n\n[1] NA\n\n\nThere is quite a lot to unpack about the function. It is not all immediately relevant but it is instructive to have an overview of what it is doing. First of all the function takes the form\nf <- function(x) {\n  ...\n}\nwhere x is the input into the function in much the same way that x is the number to be rounded in round(x = ...). It a ‘place holder’ for the input into the function.\nStatements such as if(x == 2) are logical statements: if(...) is true then do whatever follows. Where what is to be done spans over multiple lines, they are enclosed (like the function itself) by ‘curly brackets’, {...}.\nThe statement if(x < 2 | x %% floor(x) != 0) in the function is also a logical statement with the inclusion of an or statement, denoted by |. What it is checking is whether x < 2 or if x is a fraction. Had we needed to have both conditions to be met, then an and statement would be used, denoted by & instead of |. Note that ! means not, so != tests for not equal to and is the opposite of ==, which tests for equality.\nWhere it says, 2:(x-1), this is equivalent to the function, seq(from = 2, to = (x-1), by = 1). It generates a sequence of integer numbers from 2 to (x-1).\nifelse is another logical statement. It takes the form, ifelse(condition, a, b): if the condition is met then do a, else do b. In the prime number function it is checking whether dividing \\(x\\) by any of the numbers from 2 to \\(x-1\\) generates a whole number.\nFinally, the function return() returns an output from the function; specifically, a logical vector of length 1 that is TRUE, FALSE or NA dependent upon whether \\(x\\) is or is not a prime number, or if it is not a whole number above 1.\nNote that in newer versions of R, functions can also take the form,\nf <- \\(x) {\n  ...\n}\nTherefore the following is exactly equivalent to before.\n\n\nCode\nis.prime <- \\(x) {\n  if(x == 2) return(TRUE)\n  if(x < 2 | x %% floor(x) != 0) {\n    warning(\"Please enter an integer number above 1\")\n    return(NA)\n  }\n  y <- 2:(x-1)\n  ifelse(all(x%%y > 0), return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "base.html#objects-and-classes",
    "href": "base.html#objects-and-classes",
    "title": "Base R",
    "section": "Objects and Classes",
    "text": "Objects and Classes\nOur function that checks for a prime number is stored in the object is.prime.\n\n\nCode\nclass(is.prime)\n\n\n[1] \"function\"\n\n\nThere are other classes of object in R. Some of the most common are listed below.\n\nLogical\nThe output from the is.prime() function is an example of an object of class logical because the answer is TRUE or FALSE (or NA, not applicable).\n\n\nCode\nx <- is.prime(10)\nprint(x)\n\n\n[1] FALSE\n\n\nCode\nclass(x)\n\n\n[1] \"logical\"\n\n\nSome other examples:\n\n\nCode\ny <- 10 > 5\nprint(y)\n\n\n[1] TRUE\n\n\nCode\nclass(y)\n\n\n[1] \"logical\"\n\n\nCode\nz <- 2 == 5   # is 2 equal to 5?\nprint(z)\n\n\n[1] FALSE\n\n\n\nNote that # indicates a comment in the code. If you cut and paste the line y <- 2 == 5 # is 2 equal to 5? into the Console then the command y <- 2 == 5 will be run, whereas the comment # is 2 equal to 5? will be ignored as it is just there for information.\n\n\nNumeric\nWe have already seen that some objects are numeric.\n\n\nCode\nx <- mean(0:100)\nprint(x)\n\n\n[1] 50\n\n\nCode\nclass(x)\n\n\n[1] \"numeric\"\n\n\nThis presently is of type double (i.e. it allows for decimal places even where they are not required)\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nbut could be converted to class integer.\n\n\nCode\nx <- as.integer(x)\nclass(x)\n\n\n[1] \"integer\"\n\n\n\n\nCharacter\nOther classes include character. Note the difference between the length() of a character vector and the number of characters, nchar(), that any element of that vector contains.\n\n\nCode\nx <- \"Mapping and Modelling in R\"\nprint(x)\n\n\n[1] \"Mapping and Modelling in R\"\n\n\nCode\nlength(x)   # There is only one element in this vector\n\n\n[1] 1\n\n\nCode\nnchar(x)    # And that element contains 26 letters\n\n\n[1] 26\n\n\nCode\nclass(x)\n\n\n[1] \"character\"\n\n\nCode\ny <- paste(x, \"with Richard Harris\")\nprint(y)\n\n\n[1] \"Mapping and Modelling in R with Richard Harris\"\n\n\nCode\nlength(y)   # There is still only one element\n\n\n[1] 1\n\n\nCode\nnchar(y)    # But now it contains more letters\n\n\n[1] 46\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nCode\nz <- unlist(strsplit(x, \" \"))\nprint(z)\n\n\n[1] \"Mapping\"   \"and\"       \"Modelling\" \"in\"        \"R\"        \n\n\nCode\nlength(z)   # The initial vectors has been split into 5 parts\n\n\n[1] 5\n\n\nCode\nnchar(z)\n\n\n[1] 7 3 9 2 1\n\n\nCode\nclass(z)\n\n\n[1] \"character\"\n\n\n\nAs the name suggests, print() is a function that prints its contents to screen. Often it can be omitted in favour of referencing the object directly. For instance, in the example above, rather than typing print(z) it would be sufficient just to type z. Just occasionally though you will find that an object does not print as you intended when the function is omitted. If this happens, try putting print() back in.\n\n\nMatrix\nAn example of a matrix is\n\n\nCode\nx <- matrix(1:9, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nCode\nncol(x)   # Number of columns\n\n\n[1] 3\n\n\nCode\nnrow(x)   # Number of rows\n\n\n[1] 3\n\n\nCode\nclass(x)\n\n\n[1] \"matrix\" \"array\" \n\n\nHere the argument byrow is changed from its default value of FALSE to be TRUE:\n\n\nCode\ny <- matrix(1:9, ncol = 3, byrow = TRUE)\n\n\nThis result is equivalent to the transpose of the original matrix.\n\n\nCode\nt(x)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\nData frame\nA data.frame is a table of data, such as,\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nncol(df)\n\n\n[1] 4\n\n\nCode\nnrow(df)\n\n\n[1] 7\n\n\nCode\nlength(df)  # The length is the number of columns\n\n\n[1] 4\n\n\nCode\nnames(df)   # The names of the variables in the data frame\n\n\n[1] \"Day\"   \"Date\"  \"Month\" \"Year\" \n\n\nNote that the length of each column should be equal. The following will generate an error because the Date column is now too short. You might wonder why the Month and Year columns were fine previously. It is because R recycled them the requisite number of times (i.e. it gave all the rows the same value for Month and Year).\n# This will generate an error\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:25,\n                 Month = \"June\",\n                 Year = 2022)\n\n\nFactors\nEarlier versions of R would, by default, convert character fields in a data frame into factors, as in,\n\n\nCode\ndf2 <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022, stringsAsFactors = TRUE)\n\n\nThe difference is not immediately obvious,\n\n\nCode\nhead(df, n= 2)    # with stringsAsFactors = FALSE (the default)\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nCode\nhead(df2, n = 2)  # with stringsAsFactors = TRUE\n\n\n   Day Date Month Year\n1  Mon   20  June 2022\n2 Tues   21  June 2022\n\n\nbut begins to be apparent in the following:\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf2$Day\n\n\n[1] Mon   Tues  Wed   Thurs Fri   Sat   Sun  \nLevels: Fri Mon Sat Sun Thurs Tues Wed\n\n\nCode\ndf$Month\n\n\n[1] \"June\" \"June\" \"June\" \"June\" \"June\" \"June\" \"June\"\n\n\nCode\ndf2$Month\n\n\n[1] June June June June June June June\nLevels: June\n\n\nBasically, a factor is a categorical variable: it encodes which groups/categories (which levels) are to be found in the variable. Knowing this, it is possible to count the number of each group, as in,\n\n\nCode\nsummary(df2)\n\n\n    Day         Date       Month        Year     \n Fri  :1   Min.   :20.0   June:7   Min.   :2022  \n Mon  :1   1st Qu.:21.5            1st Qu.:2022  \n Sat  :1   Median :23.0            Median :2022  \n Sun  :1   Mean   :23.0            Mean   :2022  \n Thurs:1   3rd Qu.:24.5            3rd Qu.:2022  \n Tues :1   Max.   :26.0            Max.   :2022  \n Wed  :1                                         \n\n\nbut not\n\n\nCode\nsummary(df)\n\n\n     Day                 Date         Month                Year     \n Length:7           Min.   :20.0   Length:7           Min.   :2022  \n Class :character   1st Qu.:21.5   Class :character   1st Qu.:2022  \n Mode  :character   Median :23.0   Mode  :character   Median :2022  \n                    Mean   :23.0                      Mean   :2022  \n                    3rd Qu.:24.5                      3rd Qu.:2022  \n                    Max.   :26.0                      Max.   :2022  \n\n\nFactors can be useful but do not always behave as you might anticipate. For example,\n\n\nCode\nx <- factor(c(\"2021\", \"2022\"))\nas.numeric(x)\n\n\n[1] 1 2\n\n\nis different from,\n\n\nCode\nx <- c(\"2021\", \"2022\")\nas.numeric(x)\n\n\n[1] 2021 2022\n\n\nNow the deafult is stringsAsFactors = FALSE, which is better when using functions such as read.csv() to read a .csv file into a data.frame in R.\n\n\nLists\nA list is a more flexible class that can hold together other types of object. Without a list, the following only works because the 1:3 are coerced from numbers in x to characters in y – note the \" \" around them, which shows they are now text.\n\n\nCode\nx <- as.integer(1:3)\nclass(x)\n\n\n[1] \"integer\"\n\n\nCode\ny <- c(\"a\", x)\ny\n\n\n[1] \"a\" \"1\" \"2\" \"3\"\n\n\nCode\nclass(y)\n\n\n[1] \"character\"\n\n\nOn the other hand,\n\n\nCode\ny <- list(\"a\", x)\n\n\ncreates a ragged list of two parts:\n\n\nCode\nclass(y)\n\n\n[1] \"list\"\n\n\nCode\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n\nThe first part has the character \"a\" in it.\n\n\nCode\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\nclass(y[[1]])\n\n\n[1] \"character\"\n\n\nThe second has the numbers 1 to 3 in it.\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\nclass(y[[2]])\n\n\n[1] \"integer\"\n\n\nNote that the length of the list is the length of its parts, of which the following example has three.\n\n\nCode\ny <- list(\"a\", x, df)\ny\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nCode\nlength(y)\n\n\n[1] 3\n\n\nThis should not be confused with the length of any one part.\n\n\nCode\nlength(y[[1]])\n\n\n[1] 1\n\n\nCode\nlength(y[[2]])\n\n\n[1] 3\n\n\nCode\nlength(y[[3]])\n\n\n[1] 4"
  },
  {
    "objectID": "base.html#assignments",
    "href": "base.html#assignments",
    "title": "Base R",
    "section": "Assignments",
    "text": "Assignments\nThroughout this document I have used the assignment term <- to store the output of a function, as in x <- as.integer(1:3) and y <- list(\"a\", x, df), and so forth. The <- is used to assign the result of a function to an object. You can, if you prefer use =. For example, all the following make the same assignment, which is to give x the value of 1.\n\n\nCode\nx <- 1\nx = 1\n1 -> x\n\n\nPersonally, I tend to avoid using = so not to confuse assignments with arguments,\n\n\nCode\nx <- round(10.32, digits = 1)   # I think this is a bit clearer\nx = round(10.32, digits = 1)    # and this a bit less so\n\n\nalso not assignments with logical statements,\n\n\nCode\nx <- 1\ny <- 2\nz <- x == y   # Again, this is a bit clearer\nz = x == y    # and this not so much\n\n\nand, albeit pedantic, to avoid the following sort of situation which makes no sense mathematically…\n\n\nCode\nx = 1\ny = 2\nx = y\n\n\n… but does in terms of what it really means:\n\n\nCode\nx <- 1\ny <- 2\nx <- y # Assign the value of y to x, overwriting its previous value\n\n\nWhich you use is a matter of personal preference and, of course, = has one less character than <- to worry about. However, this course is written with,\n<- (or ->) is as assignment, as in x <- 1;\n= is the value of an argument, as in round(x, digits = 1); and\n== is a logical test for equality, as in x == y.\n\nIt is important to remember that R is case sensitive. An object called x is different from one called X; y is not the same as Y and so forth."
  },
  {
    "objectID": "base.html#manipulating-objects",
    "href": "base.html#manipulating-objects",
    "title": "Base R",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nIn addition to passing objects to functions such as,\n\n\nCode\nx <- 0:100\nmean(x)\n\n\n[1] 50\n\n\nCode\nsum(x)\n\n\n[1] 5050\n\n\nCode\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      25      50      50      75     100 \n\n\nCode\nmedian(x)\n\n\n[1] 50\n\n\nCode\nquantile(x, probs = c(0, 0.25, 0.5, 0.75, 1))\n\n\n  0%  25%  50%  75% 100% \n   0   25   50   75  100 \n\n\nCode\nhead(sqrt(x)) # The square roots of the first of x\n\n\n[1] 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nCode\ntail(x^2)     # The square roots of the last of x\n\n\n[1]  9025  9216  9409  9604  9801 10000\n\n\nCode\nsd(x)         # The standard deviation of x\n\n\n[1] 29.30017\n\n\nthere are other ways we may wish to interact with objects.\n\nMathematical operations\nMathematical operations generally operate on a pairwise basis between corresponding elements in a vector. For example,\n\n\nCode\nx <- 1\ny <- 3\nx + y\n\n\n[1] 4\n\n\nCode\nx <- 1:5\ny <- 6:10\nx + y\n\n\n[1]  7  9 11 13 15\n\n\nCode\nx * y   # Multiplication\n\n\n[1]  6 14 24 36 50\n\n\nCode\nx / y   # Divisions\n\n\n[1] 0.1666667 0.2857143 0.3750000 0.4444444 0.5000000\n\n\nIf one vector is shorter that the other, values will be recycled. In the following example the results are \\(1\\times6\\), \\(2\\times7\\), \\(3\\times8\\), \\(4\\times9\\) and then \\(5\\times6\\) as y is recycled.\n\n\nCode\nx <- 1:5\ny <- 6:9\nx * y\n\n\n[1]  6 14 24 36 30\n\n\n\n\nSubsets of objects\n\nVectors\nIf x is a vector then x[n] is the nth element (the nth position, the nth item) in the vector. To illustrate,\n\n\nCode\nx <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nx[1]\n\n\n[1] \"a\"\n\n\nCode\nx[3]\n\n\n[1] \"c\"\n\n\nCode\nx[c(1, 3, 5)]\n\n\n[1] \"a\" \"c\" \"e\"\n\n\nCode\nx[length(x)]\n\n\n[1] \"f\"\n\n\nThe notation -n can be used to exclude elements.\n\n\nCode\nx[-3]   # All of x except the 3rd element\n\n\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n\nCode\nx[c(-1, -3, -5)]    # x without the 1st, 3rd and 5th elements\n\n\n[1] \"b\" \"d\" \"f\"\n\n\n\n\nMatrices\nIf x is a matrix then x[i, j] is the value of the ith row of the jth column:\n\n\nCode\nx <- matrix(1:10, ncol = 2)\nx\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[1, 1]     # row 1, column 1\n\n\n[1] 1\n\n\nCode\nx[2, 1]     # row 2, column 1\n\n\n[1] 2\n\n\nCode\nx[c(3, 5), 2]   # rows 3 and 5 of column 2\n\n\n[1]  8 10\n\n\nCode\nx[nrow(x), ncol(x)]   # the final entry in the matrix\n\n\n[1] 10\n\n\nAll of the values in the ith row can be selected using the form x[i, ]\n\n\nCode\nx[1, ]    # row 1\n\n\n[1] 1 6\n\n\nCode\nx[3, ]    # row 3\n\n\n[1] 3 8\n\n\nCode\nx[c(1, 5), ]  # rows 1 and 5\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    5   10\n\n\nCode\nx[c(-1, -3), ]  # All except the 1st and 3rd rows\n\n\n     [,1] [,2]\n[1,]    2    7\n[2,]    4    9\n[3,]    5   10\n\n\nand all of the values in the jth column can be selected using the form x[, j]\n\n\nCode\nx[ ,1]    # column 1\n\n\n[1] 1 2 3 4 5\n\n\nCode\nx[ ,2]    # column 2\n\n\n[1]  6  7  8  9 10\n\n\nCode\nx[ , 1:2]   # columns 1 and 2\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nCode\nx[-3 , 1:2]   # columns 1 and 2 except row 3\n\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    4    9\n[4,]    5   10\n\n\n\n\nData frames\nData frames are not unlike a matrix.\n\n\nCode\ndf <- data.frame(Day = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"),\n                 Date = 20:26,\n                 Month = \"June\",\n                 Year = 2022)\ndf[, 1]   # The first column\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, 1]  # The first row of the first column (Day)\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, 2]  # The second row of the second column (Date)\n\n\n[1] 21\n\n\nHowever, you can also use reference the variable name directly, through the x$variable style notation,\n\n\nCode\ndf$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ndf$Date[2]\n\n\n[1] 21\n\n\nor, if you wish, with the square brackets, using the [, \"variable\"] format.\n\n\nCode\ndf[, \"Day\"]\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ndf[1, \"Day\"]\n\n\n[1] \"Mon\"\n\n\nCode\ndf[2, \"Date\"]\n\n\n[1] 21\n\n\n\n\nLists\nWe have already seen the use of double square brackets, [[...]] to refer to a part of a list:\n\n\nCode\nx <- 1:3\ny <- list(\"a\", x, df)\ny[[1]]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]]\n\n\n[1] 1 2 3\n\n\nCode\ny[[3]]\n\n\n    Day Date Month Year\n1   Mon   20  June 2022\n2  Tues   21  June 2022\n3   Wed   22  June 2022\n4 Thurs   23  June 2022\n5   Fri   24  June 2022\n6   Sat   25  June 2022\n7   Sun   26  June 2022\n\n\nThe extension to this is to be able to refer to a specific element within a part of the list by combining it with the other notation. Some examples are:\n\n\nCode\ny[[1]][1]\n\n\n[1] \"a\"\n\n\nCode\ny[[2]][3]\n\n\n[1] 3\n\n\nCode\ny[[3]]$Day\n\n\n[1] \"Mon\"   \"Tues\"  \"Wed\"   \"Thurs\" \"Fri\"   \"Sat\"   \"Sun\"  \n\n\nCode\ny[[3]]$Day[1]\n\n\n[1] \"Mon\"\n\n\nCode\ny[[3]][2, \"Date\"]\n\n\n[1] 21"
  },
  {
    "objectID": "base.html#deleting-objects-and-saving-the-workspace",
    "href": "base.html#deleting-objects-and-saving-the-workspace",
    "title": "Base R",
    "section": "Deleting objects and saving the workspace",
    "text": "Deleting objects and saving the workspace\nMy current workspace is\n\n\nCode\ngetwd()\n\n\n[1] \"/Users/ggrjh/Dropbox/github/MandM\"\n\n\nand it contains the following objects:\n\n\nCode\nls()\n\n\n[1] \"df\"       \"df2\"      \"is.prime\" \"x\"        \"y\"        \"z\"       \n\n\nTo delete a specific object, use rm(),\n\n\nCode\nrm(z)\n\n\nOr, more than one,\n\n\nCode\nrm(df, df2, is.prime)\n\n\nTo save the workspace and all the objects it now contains use the save.image() function.\n\n\nCode\nsave.image(\"workspace1.RData\")\n\n\nTo delete all the objects created in your workspace, use\n\n\nCode\nrm(list=ls())\n\n\n\nIt is a good idea to save a workspace with a new filename before deleting too much from your workspace to allow you to recover it if necessary. Be especially careful if use rm(list=ls()) as there is no undo function.\nTo (re)load a workspace, use load().\n\n\nCode\nload(\"workspace1.RData\")"
  },
  {
    "objectID": "base.html#further-reading",
    "href": "base.html#further-reading",
    "title": "Base R",
    "section": "Further reading",
    "text": "Further reading\nThis short introduction to base R has really only scratched the surface. For a fuller introduction, see the software manual, An Introduction to R.\nDon’t worry if not everything makes sense at this stage. The best way to learn R is to put it into practice and that is what we shall be doing in later sessions."
  }
]