---
format:
  html:
    theme: cosmo
    toc: true
    number-sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```

![](logocolour.jpg){width=300}

<font size = 10>Geographies of COVID-19</font><font size = 6></br>An introduction to quantitative geography</font><font size = 4></br>[Professor Richard Harris](https://profrichharris.github.io/), (updated: `r format(Sys.Date(), "%b %d %Y")`)</font>

## Introduction

Welcome to the [School of Geographical Sciences](http://www.bristol.ac.uk/geography/), [University of Bristol](https://www.bristol.ac.uk/). The School has a long and distinguished history of teaching quantitative methods, spatial modelling and geographic data science to all of our students. Historically, some of our pioneering staff in this field have included:

[Peter Haggett](https://en.wikipedia.org/wiki/Peter_Haggett), one of the most influential geographers of his generation;

![](haggett.jpg){width=100}

[Ron Johnston](https://en.wikipedia.org/wiki/Ron_Johnston_(geographer)), one of the most prolific authors ever in human geography;

![](johnston.jpg){width=100}

[Les Hepple](https://doi.org/10.1068/a4097), an exceptional scholar, polymath and dedicated teacher;

![](hepple.jpg){width=100}

and [Kelvyn Jones](https://en.wikipedia.org/wiki/Kelvyn_Jones), a leading pioneer of quantitative methods in human geography.

![](jones.jpg){width=100}


We continue to honour this tradition through our [Quantitative Spatial Science Research Group](https://www.bristol.ac.uk/geography/research/quantitative-spatial-science/), which leads the teaching of quantitative methods to students in all years of our undergraduate (and Masters) programmes. These include [BSc Geography](http://www.bristol.ac.uk/study/undergraduate/2023/geography/bsc-geography/), [BSc Geography with Quantitative Research Methods](http://www.bristol.ac.uk/study/undergraduate/2023/quantitative-research-methods/bsc-geography-quantitative-research/) and our new [MSc in Geographic Data Science and Spatial Analytics](https://www.bristol.ac.uk/study/postgraduate/2022/sci/msc-geographic-data-science-and-spatial-analytics/).


## Why Teaching Computing, Quantitative Methods and Data-handling Skills?

Programming languages such as [R](https://cran.r-project.org/) and [Python](https://www.python.org/) provide an integrated environment that can be used to input, analyse and communicate the results of data-based research, including by visual methods such as maps. This webpage was written in [R Studio](https://www.rstudio.com/), allowing me to bring together the computer code, the results of running that code and other text such as that which you are reading, all into one document. More standard off-the-shelf software such as Excel do not allow this. Presently, we teach R to **all our students**. We do this to support their training in research methods -- the sorts of skills that all geographers need [and are expected to have](https://www.qaa.ac.uk/quality-code/subject-benchmark-statements/geography) -- and also to enhance their employability because data science skills are well regarded by employers.


## Today's Exercise

All you need to do for today's session is follow along with the instructor, copying and pasting code from this document into the R Console as they do. **Please do not rush ahead as this is more likely to generate errors that you will then need to go back and correct**. Just take it one step at a time, in company with the instructor. The purpose of the session is not to teach you coding so there is no need to worry about what the code fully means. It is simply to give you a flavour of the sort of skills we teach our students.

If you wish, you could run this practical again from a personal computer when you get home. If you do, take a note of the web adress, which is [https://profrichharris.github.io/openday](https://profrichharris.github.io/openday) and note that you will first need to install [R](https://cran.r-project.org/) and then [RStudio Desktop](https://www.rstudio.com/products/rstudio/download/) on the computer for the following code to work. These software are free and operate on Windows, Mac OS and Linux.


### To Begin

When the instructor invites you to do so, please open RStudio on your computer.


### Step 1: Install/load any required libraries

The great advantage of software such as R is that it has available a large number of additional libraries that extend its functionality (for mapping, for example). The ones we need today are listed below. The code chunk checks to see if these library have been installed already, if not then installs them, and then instructs that they are required for the current session in R.

```{r}

if(!"tidyverse" %in% installed.packages()[,1]) install.packages("tidyverse")
if(!"lme4" %in% installed.packages()[,1]) install.packages("lme4")
if(!"ggplot2" %in% installed.packages()[,1]) install.packages("ggplot2")
if(!"proxy" %in% installed.packages()[,1]) install.packages("proxy")
if(!"sf" %in% installed.packages()[,1]) install.packages("sf")

require(tidyverse)
require(lme4)
require(ggplot2)
require(sf)

```


Next, we will download and load into R some data which give the reported number of positive COVID-19 cases in English neighbourhoods for the period of pandemic until free testing ended. These data are based on those from the [Coranvirus Data Dashboard](https://coronavirus.data.gov.uk/) for England, with some minor adjustments to ensure the regional totals match those reported regionally. The English neighbourhoods are what are known as [Middle Super Output Areas](https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography) (MSOAs). As the data are read-in, some variables are recalculated to create percentages (e.g. the percentage of the population aged 5 to 11 years) and the number of care home beds per 1000 of the Adult population. 

```{r}

df <- read_csv("https://www.dropbox.com/s/baqkwvsb0ah3ahh/covid_data.csv?dl=1") %>%
  mutate(across(starts_with("age"), ~ 100 * . / `All Ages`)) %>%
  mutate(carebeds = round(1000 * carebeds / `Adults`, 1)) %>%
  mutate(`age22-34` = `age22-24` + `age25-29` + `age30-34`) %>%
  pivot_longer(., cols = c(starts_with("2020-"), starts_with("2021-"), starts_with("2022-")),
               names_to = "week",
               values_to = "cases") %>%
  filter(week > "2020-03-13")
```

Here is the top of the data:

```{r}

head(df)

```



### Step 2: Fit a model

Next we will each fit a statistical model to a randomly sampled week of the pandemic. **Please note that unless your randomly selected week happens to be the same as mine, here, then our results and what appears on screen will differ from now on. The code remains the same, however.**

The model is not really very different to fitting a line of best fit to a scatter plot of data. The model basically says that the number of cases of COVID-19 in a neighbourhood is a function of the number of people who live there, their age profile and whether there is a carehome/A&E facility there. Remember that early in the pandemic those living in carehomes were particularly at risk. The model allows that in some places the number of COVID-19 cases is higher than expected, which, from a geographical point of view raises the question, *where?*

It may take a minute or two to fit the model so please be patient.

```{r, include = FALSE}
set.seed(06062022)
```

```{r}

sample.week <- sample(df$week, 1)

mlm <-  df %>%
  filter(week == sample.week) %>%
  mutate(across(starts_with("age") | matches("carebeds") | matches("AandE"),
                ~ as.numeric(scale(.)))) %>%
  glmer(cases ~  `age5-11` + `age12-17` + `age18-21` + `age22-34` +
                  carebeds + AandE +
                 (1|MSOA11CD) + (1|regionName) + offset(log(`All Ages`)),
                family=poisson(), data = .,
                control = glmerControl(calc.derivs = FALSE))

```


When the model has been fitted, take a note of the week it has been fitted for.

```{r}

print(sample.week)

```


### Step 3: Identify which region had the highest COVID rate that week?

If we 'dig' into the results of the model a little we find what are, in effect, the regional rates of COVID-19, allowing for ages, carehomes and A&E facilities. They are reported on a stanardised and also relative scale: a value greater than zero represents a region with more COVID-19 cases relative to the average that week; a value greater than zero represents a region with fewer. For the week I am looking at, which is in April 2021, Yorkshire and The Humber has most cases and the South West the least **but, remember, it could be different for the week you are looking at**.

```{r}

regional.rates <- ranef(mlm, whichel = "regionName") %>%
  as_tibble() %>%
  select(-grpvar, -term) %>%
  rename(region = grp, relative.rate = condval) %>%
  arrange(desc(region))

regional.rates %>%
  select(region, relative.rate) %>%
  mutate(relative.rate = round(relative.rate, 3)) %>%
  print(n = Inf)

```


### Step 4: Ask how different the regions really are from one another

An important statistical idea is that differences can arise 'by chance' because of uncertainties in the way data are collected. Looked at another way, it would be extremely surprising if all the regions had *exactly* the same COVID-19 rate in the selected week. Given that, it is helpful to have some idea of our confidence/(un-/)certainty in the data and if the regional rates could really overlap or are substantially different from each other. In this week, Yorkshire and The Humber does seem to be different from the rest, as the chart below shows, but there is little difference between, say, London, the East of England and the West Midlands. 

```{r}

regional.rates %>%
  mutate(ymin = relative.rate - 1.39 * condsd, ymax = relative.rate + 1.39 * condsd) %>%
  ggplot(., aes(x = region, y = relative.rate, ymin = ymin, ymax = ymax)) +
  geom_errorbar() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  xlab("Region") +
  ylab("Relative rate of COVID-19") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, size = 8, vjust = 0.5))

```


### Step 5: Map the regional differences

Now that we have calculated the regional rates we can do the digital equivalent of taking a blank map of the study region and colouring it in to produce a choropleth map of those rates.

To achieve this, first we need a boundary file, which is 'the blank map'.

```{r}

if(!file.exists("england_gor_2011_gen_clipped.shp")) {
  download.file("https://www.dropbox.com/s/gb5161kl17zra7v/England_gor_2011_gen_clipped.zip?dl=1",
              "regions.zip", mode = "wb")
  unzip("regions.zip")
}
  
map.outline <- read_sf("england_gor_2011_gen_clipped.shp") %>%
  rename(region = name)

```


Next, the map is joined to the regional rates and coloured. In my map, although there are some differences between how the regions are shaded, they are very slight. The reason for this is that the regional geography of the disease is not especially strong for the week, possibly because the important differences are to be found at a sub-regional scale.

```{r}

inner_join(map.outline, regional.rates, by = "region") %>%
  mutate(relative.rate = ifelse(relative.rate > 3.5, 3.5, relative.rate)) %>%
  mutate(relative.rate = ifelse(relative.rate < -3.5, -3.5, relative.rate)) %>%
  ggplot(.) +
    geom_sf(aes(fill = relative.rate)) +
    scale_fill_distiller(palette = "RdYlBu", direction = -1,
       limits = c(-3.5, 3.5)) +
    ggtitle(paste0("Highest rate: ", regional.rates$region[1]), sample.week)

```


### Step 6: Look at the sub-regional differences

Another important statistical idea, which also is geographical, is that results are scale dependent. For example, what we see in a map is dependent upon on the size (and shape and position) of the areas that we use for the map. So, too, are the results of a statistical model that uses geographical data.

Let's refit the model but at a sub-regional scale for the same week. This will take a little longer to fit because of the more detailed geography, which is a mixture of urban and local authority geographies.

```{r}

mlm2 <-  df %>%
  filter(week == sample.week) %>%
  mutate(across(starts_with("age") | matches("carebeds") | matches("AandE"),
                ~ as.numeric(scale(.)))) %>%
  glmer(cases ~  `age5-11` + `age12-17` + `age18-21` + `age22-34` +
                  carebeds + AandE +
                 (1|MSOA11CD) + (1|PLACE) + offset(log(`All Ages`)),
                family=poisson(), data = .,
                control = glmerControl(calc.derivs = FALSE))

```


The places with the highest relative rates in the week are (for me):

```{r}

sub.regional.rates <- ranef(mlm2, whichel = "PLACE") %>%
  as_tibble() %>%
  select(-grpvar, -term) %>%
  rename(PLACE = grp, relative.rate = condval) %>%
  arrange(desc(PLACE))

sub.regional.rates %>%
  mutate(relative.rate = round(relative.rate, 3)) %>%
  print(n = 10)

```


If I now map the rates, I obtain the following map. As it turns out, the differences between the places are not especially great for this week even at a sub-regional scale.

```{r}

if(!file.exists("places.shp")) {
  download.file("https://www.dropbox.com/s/7cd8ef8nrkbild5/places.zip?dl=1",
              "places.zip", mode = "wb")
  unzip("places.zip")
}
  
read_sf("places.shp") %>%
  inner_join(., sub.regional.rates, by = "PLACE") %>%
  mutate(relative.rate = ifelse(relative.rate > 3.5, 3.5, relative.rate)) %>%
  mutate(relative.rate = ifelse(relative.rate < -3.5, -3.5, relative.rate)) %>%
  ggplot(.) +
    geom_sf(aes(fill = relative.rate), colour = "transparent") +
    scale_fill_distiller(palette = "RdYlBu", direction = -1,
       limits = c(-3.5, 3.5)) +
    geom_sf(data = map.outline, fill = "transparent") +
    ggtitle(paste0("Highest rate: ",sub.regional.rates$PLACE[1]), sample.week)

```


### Step 7: Record the location with the highest rate in the padlet

Go to the padlet, [https://en-gb.padlet.com/profrichharris/openday](https://en-gb.padlet.com/profrichharris/openday), click on the red circle with a white + sign, search for the place with the highest COVID-19 rate in your map, and enter the date of that rate in the text before hitting Publish.


### Step 8: See what happens in a different week

Finally, if there is time, and if instructed, fo back to Step 2 and repeat everything from that step onward 


## To Consider

At the end of this exercise, when everyone has finished, if you go back to the padlet, [https://en-gb.padlet.com/profrichharris/openday](https://en-gb.padlet.com/profrichharris/openday) and interact with it then you should find that:

(a) there is a geography to COVID-19 in England; and
(b) that geography changes throughout the pandemic.

So, **here are some important geographical questions for you to consider: why? Why are the rates higher in some places than in others? What might that tells us about those places or the populations that live in them? What are the causes of the differences? Is it a 'standalone' geography or is the geography of COVID-19 related to other geographical patterns of living within England? Can you think of some possible explanations?**


## Further Reading

Department for Levelling Up, Housing and Communities (2022). Levelling Up the United Kingdom: executive summary. Dpt. for Levelling Up, Housing and Communities, London. Retrieved June 6, 2022, from [https://www.gov.uk/government/publications/levelling-up-the-united-kingdom](https://www.gov.uk/government/publications/levelling-up-the-united-kingdom).

Horton, R. (2020). Comment: COVID-19 is not a pandemic. *The Lancet*, 396 (10255), p. 874. [https://doi.org/10.1016/S0140-6736(20)32000-6](https://doi.org/10.1016/S0140-6736(20)32000-6). 

Lawence, D. (2020). *An Avoidable Crisis: The disproportionate impact of COVID-19 on Black, Asian and minority ethnic communities.* The Labour Party, London. Retrieved May 24, 2021, from [https://www.lawrencereview.co.uk/](https://www.lawrencereview.co.uk/).




  
  
  
